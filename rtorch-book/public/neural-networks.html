<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Neural Networks | A Minimal rTorch Tutorial</title>
  <meta name="description" content="This is a minimal tutorial of using the rTorch package to have fun while doing machine learning. This book was produced with bookdown." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Neural Networks | A Minimal rTorch Tutorial" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal tutorial of using the rTorch package to have fun while doing machine learning. This book was produced with bookdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Neural Networks | A Minimal rTorch Tutorial" />
  
  <meta name="twitter:description" content="This is a minimal tutorial of using the rTorch package to have fun while doing machine learning. This book was produced with bookdown." />
  

<meta name="author" content="Alfonso R. Reyes" />


<meta name="date" content="2019-08-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="logistic-regression.html">
<link rel="next" href="datasets-in-pytorch.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal rTorch Tutorial</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#installation"><i class="fa fa-check"></i><b>1.1</b> Installation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#how-do-we-start-using-rtorch"><i class="fa fa-check"></i><b>2.2</b> How do we start using <code>rTorch</code></a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#what-can-you-do-with-rtorch"><i class="fa fa-check"></i><b>2.3</b> What can you do with <code>rTorch</code></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="lessons-learned.html"><a href="lessons-learned.html"><i class="fa fa-check"></i><b>3</b> Lessons Learned</a><ul>
<li class="chapter" data-level="3.1" data-path="lessons-learned.html"><a href="lessons-learned.html#enumeration"><i class="fa fa-check"></i><b>3.1</b> Enumeration</a></li>
<li class="chapter" data-level="3.2" data-path="lessons-learned.html"><a href="lessons-learned.html#how-to-iterate-a-generator"><i class="fa fa-check"></i><b>3.2</b> How to iterate a generator</a><ul>
<li class="chapter" data-level="3.2.1" data-path="lessons-learned.html"><a href="lessons-learned.html#using-enumerate-and-iterate"><i class="fa fa-check"></i><b>3.2.1</b> Using <code>enumerate</code> and <code>iterate</code></a></li>
<li class="chapter" data-level="3.2.2" data-path="lessons-learned.html"><a href="lessons-learned.html#using-a-for-loop"><i class="fa fa-check"></i><b>3.2.2</b> Using a <code>for-loop</code></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="lessons-learned.html"><a href="lessons-learned.html#zero-gradient"><i class="fa fa-check"></i><b>3.3</b> Zero gradient</a></li>
<li class="chapter" data-level="3.4" data-path="lessons-learned.html"><a href="lessons-learned.html#transform-a-tensor"><i class="fa fa-check"></i><b>3.4</b> Transform a tensor</a></li>
<li class="chapter" data-level="3.5" data-path="lessons-learned.html"><a href="lessons-learned.html#build-a-model-class"><i class="fa fa-check"></i><b>3.5</b> Build a model class</a><ul>
<li class="chapter" data-level="3.5.1" data-path="lessons-learned.html"><a href="lessons-learned.html#examples"><i class="fa fa-check"></i><b>3.5.1</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="lessons-learned.html"><a href="lessons-learned.html#convert-a-tensor-to-numpy-object"><i class="fa fa-check"></i><b>3.6</b> Convert a tensor to <code>numpy</code> object</a></li>
<li class="chapter" data-level="3.7" data-path="lessons-learned.html"><a href="lessons-learned.html#convert-a-numpy-object-to-an-r-object"><i class="fa fa-check"></i><b>3.7</b> Convert a <code>numpy</code> object to an <code>R</code> object</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tensors.html"><a href="tensors.html"><i class="fa fa-check"></i><b>4</b> Tensors</a><ul>
<li class="chapter" data-level="4.1" data-path="tensors.html"><a href="tensors.html#r-code"><i class="fa fa-check"></i><b>4.1</b> R code</a><ul>
<li class="chapter" data-level="4.1.1" data-path="tensors.html"><a href="tensors.html#load-the-libraries"><i class="fa fa-check"></i><b>4.1.1</b> Load the libraries</a></li>
<li class="chapter" data-level="4.1.2" data-path="tensors.html"><a href="tensors.html#datasets"><i class="fa fa-check"></i><b>4.1.2</b> Datasets</a></li>
<li class="chapter" data-level="4.1.3" data-path="tensors.html"><a href="tensors.html#run-the-model"><i class="fa fa-check"></i><b>4.1.3</b> Run the model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>5</b> Linear Algebra with Torch</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-algebra.html"><a href="linear-algebra.html#scalars"><i class="fa fa-check"></i><b>5.1</b> Scalars</a></li>
<li class="chapter" data-level="5.2" data-path="linear-algebra.html"><a href="linear-algebra.html#vectors"><i class="fa fa-check"></i><b>5.2</b> Vectors</a></li>
<li class="chapter" data-level="5.3" data-path="linear-algebra.html"><a href="linear-algebra.html#matrices"><i class="fa fa-check"></i><b>5.3</b> Matrices</a></li>
<li class="chapter" data-level="5.4" data-path="linear-algebra.html"><a href="linear-algebra.html#d-tensors"><i class="fa fa-check"></i><b>5.4</b> 3D+ tensors</a></li>
<li class="chapter" data-level="5.5" data-path="linear-algebra.html"><a href="linear-algebra.html#transpose-of-a-matrix"><i class="fa fa-check"></i><b>5.5</b> Transpose of a matrix</a></li>
<li class="chapter" data-level="5.6" data-path="linear-algebra.html"><a href="linear-algebra.html#vectors-special-case-of-a-matrix"><i class="fa fa-check"></i><b>5.6</b> Vectors, special case of a matrix</a></li>
<li class="chapter" data-level="5.7" data-path="linear-algebra.html"><a href="linear-algebra.html#tensor-addition"><i class="fa fa-check"></i><b>5.7</b> Tensor addition</a></li>
<li class="chapter" data-level="5.8" data-path="linear-algebra.html"><a href="linear-algebra.html#add-a-scalar-to-a-tensor"><i class="fa fa-check"></i><b>5.8</b> Add a scalar to a tensor</a></li>
<li class="chapter" data-level="5.9" data-path="linear-algebra.html"><a href="linear-algebra.html#multiplying-tensors"><i class="fa fa-check"></i><b>5.9</b> Multiplying tensors</a></li>
<li class="chapter" data-level="5.10" data-path="linear-algebra.html"><a href="linear-algebra.html#dot-product"><i class="fa fa-check"></i><b>5.10</b> Dot product</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>6</b> Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="linear-regression.html"><a href="linear-regression.html#case-1-simple-linear-regression"><i class="fa fa-check"></i><b>6.1</b> Case 1: simple linear regression</a></li>
<li class="chapter" data-level="6.2" data-path="linear-regression.html"><a href="linear-regression.html#creating-the-network-model"><i class="fa fa-check"></i><b>6.2</b> Creating the network model</a></li>
<li class="chapter" data-level="6.3" data-path="linear-regression.html"><a href="linear-regression.html#datasets-1"><i class="fa fa-check"></i><b>6.3</b> Datasets</a><ul>
<li class="chapter" data-level="6.3.1" data-path="linear-regression.html"><a href="linear-regression.html#converting-from-numpy-to-tensor"><i class="fa fa-check"></i><b>6.3.1</b> Converting from numpy to tensor</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="linear-regression.html"><a href="linear-regression.html#optimizer-and-loss"><i class="fa fa-check"></i><b>6.4</b> Optimizer and Loss</a></li>
<li class="chapter" data-level="6.5" data-path="linear-regression.html"><a href="linear-regression.html#training"><i class="fa fa-check"></i><b>6.5</b> Training</a></li>
<li class="chapter" data-level="6.6" data-path="linear-regression.html"><a href="linear-regression.html#result"><i class="fa fa-check"></i><b>6.6</b> Result</a></li>
<li class="chapter" data-level="6.7" data-path="linear-regression.html"><a href="linear-regression.html#case-2-rainfall"><i class="fa fa-check"></i><b>6.7</b> Case 2: Rainfall</a></li>
<li class="chapter" data-level="6.8" data-path="linear-regression.html"><a href="linear-regression.html#select-device"><i class="fa fa-check"></i><b>6.8</b> Select device</a></li>
<li class="chapter" data-level="6.9" data-path="linear-regression.html"><a href="linear-regression.html#training-data"><i class="fa fa-check"></i><b>6.9</b> Training data</a></li>
<li class="chapter" data-level="6.10" data-path="linear-regression.html"><a href="linear-regression.html#convert-to-tensors"><i class="fa fa-check"></i><b>6.10</b> Convert to tensors</a></li>
<li class="chapter" data-level="6.11" data-path="linear-regression.html"><a href="linear-regression.html#build-the-model"><i class="fa fa-check"></i><b>6.11</b> Build the model</a></li>
<li class="chapter" data-level="6.12" data-path="linear-regression.html"><a href="linear-regression.html#generate-predictions"><i class="fa fa-check"></i><b>6.12</b> Generate predictions</a></li>
<li class="chapter" data-level="6.13" data-path="linear-regression.html"><a href="linear-regression.html#loss-function"><i class="fa fa-check"></i><b>6.13</b> Loss Function</a></li>
<li class="chapter" data-level="6.14" data-path="linear-regression.html"><a href="linear-regression.html#compute-gradients"><i class="fa fa-check"></i><b>6.14</b> Compute Gradients</a></li>
<li class="chapter" data-level="6.15" data-path="linear-regression.html"><a href="linear-regression.html#adjust-weights-and-biases-using-gradient-descent"><i class="fa fa-check"></i><b>6.15</b> Adjust weights and biases using gradient descent</a></li>
<li class="chapter" data-level="6.16" data-path="linear-regression.html"><a href="linear-regression.html#train-for-multiple-epochs"><i class="fa fa-check"></i><b>6.16</b> Train for multiple epochs</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>7</b> Logistic Regression</a><ul>
<li class="chapter" data-level="7.0.1" data-path="logistic-regression.html"><a href="logistic-regression.html#hyperparameters"><i class="fa fa-check"></i><b>7.0.1</b> Hyperparameters</a></li>
<li class="chapter" data-level="7.0.2" data-path="logistic-regression.html"><a href="logistic-regression.html#read-datasets"><i class="fa fa-check"></i><b>7.0.2</b> Read datasets</a></li>
<li class="chapter" data-level="7.0.3" data-path="logistic-regression.html"><a href="logistic-regression.html#define-the-model"><i class="fa fa-check"></i><b>7.0.3</b> Define the model</a></li>
<li class="chapter" data-level="7.0.4" data-path="logistic-regression.html"><a href="logistic-regression.html#training-1"><i class="fa fa-check"></i><b>7.0.4</b> Training</a></li>
<li class="chapter" data-level="7.0.5" data-path="logistic-regression.html"><a href="logistic-regression.html#prediction"><i class="fa fa-check"></i><b>7.0.5</b> Prediction</a></li>
<li class="chapter" data-level="7.0.6" data-path="logistic-regression.html"><a href="logistic-regression.html#save-the-model"><i class="fa fa-check"></i><b>7.0.6</b> Save the model</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>8</b> Neural Networks</a><ul>
<li class="chapter" data-level="8.1" data-path="neural-networks.html"><a href="neural-networks.html#in-r"><i class="fa fa-check"></i><b>8.1</b> In R</a><ul>
<li class="chapter" data-level="8.1.1" data-path="neural-networks.html"><a href="neural-networks.html#select-device-1"><i class="fa fa-check"></i><b>8.1.1</b> Select device</a></li>
<li class="chapter" data-level="8.1.2" data-path="neural-networks.html"><a href="neural-networks.html#create-datasets"><i class="fa fa-check"></i><b>8.1.2</b> Create datasets</a></li>
<li class="chapter" data-level="8.1.3" data-path="neural-networks.html"><a href="neural-networks.html#define-the-model-1"><i class="fa fa-check"></i><b>8.1.3</b> Define the model</a></li>
<li class="chapter" data-level="8.1.4" data-path="neural-networks.html"><a href="neural-networks.html#loss-function-1"><i class="fa fa-check"></i><b>8.1.4</b> Loss function</a></li>
<li class="chapter" data-level="8.1.5" data-path="neural-networks.html"><a href="neural-networks.html#iterate-through-batches"><i class="fa fa-check"></i><b>8.1.5</b> Iterate through batches</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="datasets-in-pytorch.html"><a href="datasets-in-pytorch.html"><i class="fa fa-check"></i><b>9</b> Datasets in PyTorch</a></li>
<li class="chapter" data-level="10" data-path="bookdown-tips.html"><a href="bookdown-tips.html"><i class="fa fa-check"></i><b>10</b> Bookdown tips</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Minimal rTorch Tutorial</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="neural-networks" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Neural Networks</h1>
<p>Source: <a href="https://github.com/jcjohnson/pytorch-examples#pytorch-nn" class="uri">https://github.com/jcjohnson/pytorch-examples#pytorch-nn</a></p>
<p>In this example we use the torch <code>nn</code> package to implement our two-layer network:</p>
<div id="in-r" class="section level2">
<h2><span class="header-section-number">8.1</span> In R</h2>
<div id="select-device-1" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Select device</h3>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb260-1" data-line-number="1"><span class="kw">library</span>(rTorch)</a>
<a class="sourceLine" id="cb260-2" data-line-number="2"></a>
<a class="sourceLine" id="cb260-3" data-line-number="3">device =<span class="st"> </span>torch<span class="op">$</span><span class="kw">device</span>(<span class="st">&#39;cpu&#39;</span>)</a>
<a class="sourceLine" id="cb260-4" data-line-number="4"></a>
<a class="sourceLine" id="cb260-5" data-line-number="5"><span class="co"># device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU</span></a></code></pre></div>
<pre><code> N is batch size; 
 D_in is input dimension;
 H is hidden dimension; 
 D_out is output dimension.</code></pre>
</div>
<div id="create-datasets" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Create datasets</h3>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb262-1" data-line-number="1">torch<span class="op">$</span><span class="kw">manual_seed</span>(<span class="dv">0</span>)</a></code></pre></div>
<pre><code>## &lt;torch._C.Generator&gt;</code></pre>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb264-1" data-line-number="1">N &lt;-<span class="st"> </span>64L; D_in &lt;-<span class="st"> </span>1000L; H &lt;-<span class="st"> </span>100L; D_out &lt;-<span class="st"> </span>10L</a>
<a class="sourceLine" id="cb264-2" data-line-number="2"></a>
<a class="sourceLine" id="cb264-3" data-line-number="3"><span class="co"># Create random Tensors to hold inputs and outputs</span></a>
<a class="sourceLine" id="cb264-4" data-line-number="4">x =<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(N, D_in, <span class="dt">device=</span>device)</a>
<a class="sourceLine" id="cb264-5" data-line-number="5">y =<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(N, D_out, <span class="dt">device=</span>device)</a></code></pre></div>
</div>
<div id="define-the-model-1" class="section level3">
<h3><span class="header-section-number">8.1.3</span> Define the model</h3>
<p>Use the <code>nn</code> package to define our model as a sequence of layers. <code>nn.Sequential</code>
is a Module which contains other Modules, and applies them in sequence to
produce its output. Each Linear Module computes output from input using a
linear function, and holds internal Tensors for its weight and bias.
After constructing the model we use the <code>.to()</code> method to move it to the
desired device.</p>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb265-1" data-line-number="1">model =<span class="st"> </span>torch<span class="op">$</span>nn<span class="op">$</span><span class="kw">Sequential</span>(</a>
<a class="sourceLine" id="cb265-2" data-line-number="2">          torch<span class="op">$</span>nn<span class="op">$</span><span class="kw">Linear</span>(D_in, H),</a>
<a class="sourceLine" id="cb265-3" data-line-number="3">          torch<span class="op">$</span>nn<span class="op">$</span><span class="kw">ReLU</span>(),</a>
<a class="sourceLine" id="cb265-4" data-line-number="4">          torch<span class="op">$</span>nn<span class="op">$</span><span class="kw">Linear</span>(H, D_out))<span class="op">$</span><span class="kw">to</span>(device)</a></code></pre></div>
</div>
<div id="loss-function-1" class="section level3">
<h3><span class="header-section-number">8.1.4</span> Loss function</h3>
<p>The <code>nn</code> package also contains definitions of popular loss functions; in this case we will use Mean Squared Error (MSE) as our loss function. Setting <code>reduction='sum'</code> means that we are computing the <em>sum</em> of squared errors rather than the mean; this is for consistency with the examples above where we manually compute the loss, but in practice it is more common to use mean squared error as a loss by setting <code>reduction='elementwise_mean'</code>.</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb266-1" data-line-number="1">loss_fn =<span class="st"> </span>torch<span class="op">$</span>nn<span class="op">$</span><span class="kw">MSELoss</span>(<span class="dt">reduction =</span> <span class="st">&#39;sum&#39;</span>)</a></code></pre></div>
</div>
<div id="iterate-through-batches" class="section level3">
<h3><span class="header-section-number">8.1.5</span> Iterate through batches</h3>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb267-1" data-line-number="1">learning_rate =<span class="st"> </span><span class="fl">1e-4</span></a>
<a class="sourceLine" id="cb267-2" data-line-number="2"></a>
<a class="sourceLine" id="cb267-3" data-line-number="3"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">500</span>) {</a>
<a class="sourceLine" id="cb267-4" data-line-number="4">  <span class="co"># Forward pass: compute predicted y by passing x to the model. Module objects</span></a>
<a class="sourceLine" id="cb267-5" data-line-number="5">  <span class="co"># override the __call__ operator so you can call them like functions. When</span></a>
<a class="sourceLine" id="cb267-6" data-line-number="6">  <span class="co"># doing so you pass a Tensor of input data to the Module and it produces</span></a>
<a class="sourceLine" id="cb267-7" data-line-number="7">  <span class="co"># a Tensor of output data.</span></a>
<a class="sourceLine" id="cb267-8" data-line-number="8">  y_pred =<span class="st"> </span><span class="kw">model</span>(x)</a>
<a class="sourceLine" id="cb267-9" data-line-number="9"></a>
<a class="sourceLine" id="cb267-10" data-line-number="10">  <span class="co"># Compute and print loss. We pass Tensors containing the predicted and true</span></a>
<a class="sourceLine" id="cb267-11" data-line-number="11">  <span class="co"># values of y, and the loss function returns a Tensor containing the loss.</span></a>
<a class="sourceLine" id="cb267-12" data-line-number="12">  loss =<span class="st"> </span><span class="kw">loss_fn</span>(y_pred, y)</a>
<a class="sourceLine" id="cb267-13" data-line-number="13">  </a>
<a class="sourceLine" id="cb267-14" data-line-number="14">  <span class="kw">cat</span>(t, <span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>)</a>
<a class="sourceLine" id="cb267-15" data-line-number="15">  <span class="kw">cat</span>(loss<span class="op">$</span><span class="kw">item</span>(), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</a>
<a class="sourceLine" id="cb267-16" data-line-number="16">  </a>
<a class="sourceLine" id="cb267-17" data-line-number="17">  <span class="co"># Zero the gradients before running the backward pass.</span></a>
<a class="sourceLine" id="cb267-18" data-line-number="18">  model<span class="op">$</span><span class="kw">zero_grad</span>()</a>
<a class="sourceLine" id="cb267-19" data-line-number="19"></a>
<a class="sourceLine" id="cb267-20" data-line-number="20">  <span class="co"># Backward pass: compute gradient of the loss with respect to all the learnable</span></a>
<a class="sourceLine" id="cb267-21" data-line-number="21">  <span class="co"># parameters of the model. Internally, the parameters of each Module are stored</span></a>
<a class="sourceLine" id="cb267-22" data-line-number="22">  <span class="co"># in Tensors with requires_grad=True, so this call will compute gradients for</span></a>
<a class="sourceLine" id="cb267-23" data-line-number="23">  <span class="co"># all learnable parameters in the model.</span></a>
<a class="sourceLine" id="cb267-24" data-line-number="24">  loss<span class="op">$</span><span class="kw">backward</span>()</a>
<a class="sourceLine" id="cb267-25" data-line-number="25"></a>
<a class="sourceLine" id="cb267-26" data-line-number="26">  <span class="co"># Update the weights using gradient descent. Each parameter is a Tensor, so</span></a>
<a class="sourceLine" id="cb267-27" data-line-number="27">  <span class="co"># we can access its data and gradients like we did before.</span></a>
<a class="sourceLine" id="cb267-28" data-line-number="28">  <span class="kw">with</span>(torch<span class="op">$</span><span class="kw">no_grad</span>(), {</a>
<a class="sourceLine" id="cb267-29" data-line-number="29">      <span class="cf">for</span> (param <span class="cf">in</span> <span class="kw">iterate</span>(model<span class="op">$</span><span class="kw">parameters</span>())) {</a>
<a class="sourceLine" id="cb267-30" data-line-number="30">        <span class="co"># in Python this code is much simpler. In R we have to do some conversions</span></a>
<a class="sourceLine" id="cb267-31" data-line-number="31">        </a>
<a class="sourceLine" id="cb267-32" data-line-number="32">        <span class="co"># param$data &lt;- torch$sub(param$data,</span></a>
<a class="sourceLine" id="cb267-33" data-line-number="33">        <span class="co">#                         torch$mul(param$grad$float(),</span></a>
<a class="sourceLine" id="cb267-34" data-line-number="34">        <span class="co">#                           torch$scalar_tensor(learning_rate)))</span></a>
<a class="sourceLine" id="cb267-35" data-line-number="35">        </a>
<a class="sourceLine" id="cb267-36" data-line-number="36">        param<span class="op">$</span>data &lt;-<span class="st"> </span>param<span class="op">$</span>data <span class="op">-</span><span class="st"> </span>param<span class="op">$</span>grad <span class="op">*</span><span class="st"> </span>learning_rate</a>
<a class="sourceLine" id="cb267-37" data-line-number="37">      }</a>
<a class="sourceLine" id="cb267-38" data-line-number="38">   })</a>
<a class="sourceLine" id="cb267-39" data-line-number="39">}  </a></code></pre></div>
<pre><code>## 1    628.2839 
## 2    584.9592 
## 3    546.728 
## 4    512.6885 
## 5    482.1306 
## 6    454.6265 
## 7    429.5551 
## 8    406.399 
## 9    384.6441 
## 10   364.3125 
## 11   345.3866 
## 12   327.6258 
## 13   310.8002 
## 14   294.8379 
## 15   279.7325 
## 16   265.308 
## 17   251.6231 
## 18   238.5361 
## 19   226.0232 
## 20   214.1025 
## 21   202.7003 
## 22   191.8059 
## 23   181.4473 
## 24   171.5948 
## 25   162.2114 
## 26   153.2922 
## 27   144.8526 
## 28   136.86 
## 29   129.2786 
## 30   122.0849 
## 31   115.2736 
## 32   108.8145 
## 33   102.702 
## 34   96.9281 
## 35   91.47466 
## 36   86.3278 
## 37   81.47502 
## 38   76.89146 
## 39   72.56895 
## 40   68.47774 
## 41   64.62608 
## 42   60.99714 
## 43   57.56915 
## 44   54.3427 
## 45   51.30913 
## 46   48.4516 
## 47   45.7645 
## 48   43.2405 
## 49   40.86606 
## 50   38.63006 
## 51   36.52527 
## 52   34.54914 
## 53   32.69345 
## 54   30.9498 
## 55   29.30734 
## 56   27.76275 
## 57   26.31089 
## 58   24.9431 
## 59   23.6538 
## 60   22.43881 
## 61   21.29258 
## 62   20.21278 
## 63   19.19288 
## 64   18.23302 
## 65   17.32939 
## 66   16.47696 
## 67   15.67115 
## 68   14.90951 
## 69   14.19054 
## 70   13.51087 
## 71   12.86819 
## 72   12.26046 
## 73   11.68514 
## 74   11.14105 
## 75   10.62579 
## 76   10.13727 
## 77   9.67443 
## 78   9.235745 
## 79   8.81936 
## 80   8.423415 
## 81   8.047976 
## 82   7.691305 
## 83   7.352424 
## 84   7.030395 
## 85   6.724676 
## 86   6.434433 
## 87   6.158278 
## 88   5.895441 
## 89   5.645772 
## 90   5.408593 
## 91   5.182788 
## 92   4.967783 
## 93   4.762821 
## 94   4.567942 
## 95   4.381676 
## 96   4.203955 
## 97   4.034254 
## 98   3.872373 
## 99   3.717669 
## 100  3.569852 
## 101  3.42879 
## 102  3.293934 
## 103  3.165006 
## 104  3.041733 
## 105  2.923832 
## 106  2.810992 
## 107  2.702966 
## 108  2.599546 
## 109  2.500533 
## 110  2.405609 
## 111  2.314677 
## 112  2.227567 
## 113  2.143984 
## 114  2.063824 
## 115  1.986898 
## 116  1.913161 
## 117  1.842398 
## 118  1.77456 
## 119  1.709334 
## 120  1.64675 
## 121  1.586662 
## 122  1.528932 
## 123  1.473498 
## 124  1.420194 
## 125  1.369045 
## 126  1.319939 
## 127  1.272713 
## 128  1.227352 
## 129  1.183716 
## 130  1.141713 
## 131  1.101289 
## 132  1.062379 
## 133  1.024975 
## 134  0.9889802 
## 135  0.9543175 
## 136  0.9209599 
## 137  0.8888108 
## 138  0.8578573 
## 139  0.828075 
## 140  0.7993811 
## 141  0.7717422 
## 142  0.7451235 
## 143  0.7194628 
## 144  0.6947174 
## 145  0.6708718 
## 146  0.6479479 
## 147  0.6259133 
## 148  0.6046655 
## 149  0.5841969 
## 150  0.5644467 
## 151  0.5453879 
## 152  0.5270226 
## 153  0.509312 
## 154  0.4922202 
## 155  0.4757309 
## 156  0.4598286 
## 157  0.444492 
## 158  0.4296912 
## 159  0.4153878 
## 160  0.4015924 
## 161  0.3882794 
## 162  0.3754345 
## 163  0.3630294 
## 164  0.3510535 
## 165  0.3394946 
## 166  0.3283299 
## 167  0.3175527 
## 168  0.3071378 
## 169  0.2970789 
## 170  0.2873671 
## 171  0.2779845 
## 172  0.2689149 
## 173  0.2601575 
## 174  0.2516952 
## 175  0.2435159 
## 176  0.2356067 
## 177  0.2279707 
## 178  0.2206163 
## 179  0.2135031 
## 180  0.2066278 
## 181  0.199988 
## 182  0.1935741 
## 183  0.1873666 
## 184  0.1813661 
## 185  0.1755629 
## 186  0.1699517 
## 187  0.1645233 
## 188  0.1592781 
## 189  0.1542067 
## 190  0.149301 
## 191  0.1445545 
## 192  0.1399648 
## 193  0.1355246 
## 194  0.1312279 
## 195  0.1270718 
## 196  0.1230509 
## 197  0.1191625 
## 198  0.115399 
## 199  0.1117588 
## 200  0.1082348 
## 201  0.1048254 
## 202  0.1015256 
## 203  0.09833349 
## 204  0.09524446 
## 205  0.0922533 
## 206  0.08935806 
## 207  0.08655693 
## 208  0.08384982 
## 209  0.081232 
## 210  0.07869583 
## 211  0.07624327 
## 212  0.0738695 
## 213  0.07157086 
## 214  0.06934651 
## 215  0.0671912 
## 216  0.06510548 
## 217  0.0630878 
## 218  0.0611335 
## 219  0.05924014 
## 220  0.05740607 
## 221  0.05563042 
## 222  0.05391053 
## 223  0.05224558 
## 224  0.05063397 
## 225  0.04907332 
## 226  0.04756238 
## 227  0.04609917 
## 228  0.0446822 
## 229  0.04330759 
## 230  0.04197593 
## 231  0.04068634 
## 232  0.03943768 
## 233  0.03822812 
## 234  0.0370564 
## 235  0.03592146 
## 236  0.03482136 
## 237  0.03375633 
## 238  0.0327246 
## 239  0.03172487 
## 240  0.03075593 
## 241  0.02981687 
## 242  0.02890735 
## 243  0.02802622 
## 244  0.02717285 
## 245  0.02634558 
## 246  0.02554371 
## 247  0.02476675 
## 248  0.0240143 
## 249  0.02328515 
## 250  0.02257836 
## 251  0.02189306 
## 252  0.02122923 
## 253  0.02058575 
## 254  0.01996238 
## 255  0.01935788 
## 256  0.01877213 
## 257  0.01820429 
## 258  0.01765402 
## 259  0.01712051 
## 260  0.01660351 
## 261  0.01610242 
## 262  0.0156171 
## 263  0.01514632 
## 264  0.01469009 
## 265  0.01424752 
## 266  0.01381863 
## 267  0.01340292 
## 268  0.01299987 
## 269  0.01260905 
## 270  0.01223013 
## 271  0.01186274 
## 272  0.01150682 
## 273  0.0111617 
## 274  0.01082683 
## 275  0.01050223 
## 276  0.01018802 
## 277  0.009882836 
## 278  0.009586995 
## 279  0.009300183 
## 280  0.009021979 
## 281  0.008752229 
## 282  0.008490788 
## 283  0.008237117 
## 284  0.007991165 
## 285  0.007752791 
## 286  0.007521485 
## 287  0.00729729 
## 288  0.0070798 
## 289  0.006868829 
## 290  0.006664461 
## 291  0.006466233 
## 292  0.006273943 
## 293  0.006087383 
## 294  0.005906438 
## 295  0.005730878 
## 296  0.005560733 
## 297  0.005395614 
## 298  0.005235502 
## 299  0.005080266 
## 300  0.004929713 
## 301  0.004783633 
## 302  0.004641939 
## 303  0.004504489 
## 304  0.004371175 
## 305  0.004241788 
## 306  0.004116328 
## 307  0.003994633 
## 308  0.003876575 
## 309  0.003762146 
## 310  0.003650998 
## 311  0.003543299 
## 312  0.003438761 
## 313  0.003337309 
## 314  0.003238867 
## 315  0.0031434 
## 316  0.003050815 
## 317  0.002960989 
## 318  0.002873863 
## 319  0.002789256 
## 320  0.002707202 
## 321  0.002627575 
## 322  0.002550329 
## 323  0.002475383 
## 324  0.002402639 
## 325  0.002332083 
## 326  0.002263665 
## 327  0.002197253 
## 328  0.0021328 
## 329  0.002070281 
## 330  0.002009582 
## 331  0.001950708 
## 332  0.001893577 
## 333  0.001838136 
## 334  0.001784306 
## 335  0.001732112 
## 336  0.001681432 
## 337  0.0016323 
## 338  0.001584614 
## 339  0.001538318 
## 340  0.00149336 
## 341  0.001449755 
## 342  0.001407453 
## 343  0.001366422 
## 344  0.001326558 
## 345  0.001287868 
## 346  0.001250318 
## 347  0.001213876 
## 348  0.00117851 
## 349  0.0011442 
## 350  0.001110891 
## 351  0.001078564 
## 352  0.001047205 
## 353  0.001016748 
## 354  0.000987183 
## 355  0.0009584787 
## 356  0.0009306203 
## 357  0.0009035908 
## 358  0.0008773552 
## 359  0.0008518948 
## 360  0.0008271908 
## 361  0.0008032027 
## 362  0.0007798997 
## 363  0.0007572919 
## 364  0.0007353517 
## 365  0.0007140448 
## 366  0.0006933782 
## 367  0.0006733016 
## 368  0.0006538091 
## 369  0.0006348886 
## 370  0.0006165295 
## 371  0.0005986958 
## 372  0.0005813859 
## 373  0.0005645752 
## 374  0.0005482732 
## 375  0.0005324451 
## 376  0.0005170752 
## 377  0.00050215 
## 378  0.0004876566 
## 379  0.0004735905 
## 380  0.0004599217 
## 381  0.000446661 
## 382  0.0004337932 
## 383  0.000421295 
## 384  0.000409168 
## 385  0.000397383 
## 386  0.0003859476 
## 387  0.0003748359 
## 388  0.000364068 
## 389  0.0003535805 
## 390  0.0003434142 
## 391  0.0003335507 
## 392  0.0003239612 
## 393  0.0003146584 
## 394  0.0003056151 
## 395  0.0002968509 
## 396  0.0002883257 
## 397  0.0002800551 
## 398  0.0002720189 
## 399  0.000264218 
## 400  0.0002566472 
## 401  0.0002492882 
## 402  0.0002421443 
## 403  0.0002352033 
## 404  0.000228468 
## 405  0.000221927 
## 406  0.0002155735 
## 407  0.000209406 
## 408  0.0002034151 
## 409  0.0001976036 
## 410  0.0001919538 
## 411  0.000186465 
## 412  0.000181138 
## 413  0.0001759652 
## 414  0.0001709464 
## 415  0.0001660616 
## 416  0.0001613203 
## 417  0.0001567196 
## 418  0.0001522457 
## 419  0.0001479028 
## 420  0.0001436849 
## 421  0.0001395875 
## 422  0.0001356086 
## 423  0.0001317492 
## 424  0.0001279981 
## 425  0.0001243516 
## 426  0.0001208117 
## 427  0.0001173674 
## 428  0.0001140324 
## 429  0.0001107887 
## 430  0.0001076371 
## 431  0.000104577 
## 432  0.0001016119 
## 433  9.872603e-05 
## 434  9.591727e-05 
## 435  9.320263e-05 
## 436  9.055879e-05 
## 437  8.798708e-05 
## 438  8.549038e-05 
## 439  8.306588e-05 
## 440  8.071403e-05 
## 441  7.842657e-05 
## 442  7.62079e-05 
## 443  7.40515e-05 
## 444  7.195051e-05 
## 445  6.991543e-05 
## 446  6.79373e-05 
## 447  6.601425e-05 
## 448  6.414913e-05 
## 449  6.233487e-05 
## 450  6.057473e-05 
## 451  5.886107e-05 
## 452  5.719856e-05 
## 453  5.558319e-05 
## 454  5.401246e-05 
## 455  5.24875e-05 
## 456  5.100614e-05 
## 457  4.957083e-05 
## 458  4.817137e-05 
## 459  4.681194e-05 
## 460  4.549512e-05 
## 461  4.420892e-05 
## 462  4.296327e-05 
## 463  4.175395e-05 
## 464  4.057565e-05 
## 465  3.943508e-05 
## 466  3.832371e-05 
## 467  3.724535e-05 
## 468  3.619579e-05 
## 469  3.517841e-05 
## 470  3.419127e-05 
## 471  3.322741e-05 
## 472  3.229243e-05 
## 473  3.138653e-05 
## 474  3.050298e-05 
## 475  2.964781e-05 
## 476  2.881304e-05 
## 477  2.800681e-05 
## 478  2.721956e-05 
## 479  2.645602e-05 
## 480  2.57129e-05 
## 481  2.499174e-05 
## 482  2.429051e-05 
## 483  2.361012e-05 
## 484  2.294818e-05 
## 485  2.230642e-05 
## 486  2.167962e-05 
## 487  2.107264e-05 
## 488  2.048336e-05 
## 489  1.991114e-05 
## 490  1.935353e-05 
## 491  1.88103e-05 
## 492  1.82841e-05 
## 493  1.777284e-05 
## 494  1.727619e-05 
## 495  1.67922e-05 
## 496  1.632309e-05 
## 497  1.586889e-05 
## 498  1.54238e-05 
## 499  1.499398e-05 
## 500  1.457493e-05</code></pre>
<p>These two expression are equivalent, with the first being the long version natural way of doing it in PyTorch. The second is using the generics in R for subtraction, multiplication and scalar conversion.</p>
<pre><code>param$data &lt;- torch$sub(param$data,
                        torch$mul(param$grad$float(),
                          torch$scalar_tensor(learning_rate)))
}</code></pre>
<pre><code>param$data &lt;- param$data - param$grad * learning_rate</code></pre>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="logistic-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="datasets-in-pytorch.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["rtorch-book.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
