[
["tensors.html", "Chapter 4 Tensors 4.1 Arithmetic of tensors 4.2 Boolean operations 4.3 Slicing 4.4 Example", " Chapter 4 Tensors We describe the most important PyTorch methods in this chapter. 4.1 Arithmetic of tensors 4.2 Boolean operations 4.3 Slicing 4.4 Example The following example was converted from PyTorch to rTorch to show differences and similarities of both approaches. The original source can be found here: Source: https://github.com/jcjohnson/pytorch-examples#pytorch-tensors 4.4.1 Load the libraries library(rTorch) device = torch$device(&#39;cpu&#39;) # device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU torch$manual_seed(0) #&gt; &lt;torch._C.Generator&gt; N is batch size; D_in is input dimension; H is hidden dimension; D_out is output dimension. 4.4.2 Datasets We will create a random dataset for a two layer neural network. N &lt;- 64L; D_in &lt;- 1000L; H &lt;- 100L; D_out &lt;- 10L # Create random Tensors to hold inputs and outputs x &lt;- torch$randn(N, D_in, device=device) y &lt;- torch$randn(N, D_out, device=device) # Randomly initialize weights w1 &lt;- torch$randn(D_in, H, device=device) # layer 1 w2 &lt;- torch$randn(H, D_out, device=device) # layer 2 4.4.3 Run the model learning_rate = 1e-6 # loop for (t in 1:50) { # Forward pass: compute predicted y h &lt;- x$mm(w1) h_relu &lt;- h$clamp(min=0) y_pred &lt;- h_relu$mm(w2) # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor # of shape (); we can get its value as a Python number with loss.item(). loss &lt;- (torch$sub(y_pred, y))$pow(2)$sum() cat(t, &quot;\\t&quot;) cat(loss$item(), &quot;\\n&quot;) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred &lt;- torch$mul(torch$scalar_tensor(2.0), torch$sub(y_pred, y)) grad_w2 &lt;- h_relu$t()$mm(grad_y_pred) grad_h_relu &lt;- grad_y_pred$mm(w2$t()) grad_h &lt;- grad_h_relu$clone() # grad_h[h &lt; 0] = 0 mask &lt;- grad_h$lt(0) # print(mask) # negatives &lt;- torch$masked_select(grad_h, mask) # print(negatives) # negatives &lt;- 0.0 torch$masked_select(grad_h, mask)$fill_(0.0) # print(grad_h) grad_w1 &lt;- x$t()$mm(grad_h) # Update weights using gradient descent w1 &lt;- torch$sub(w1, torch$mul(learning_rate, grad_w1)) w2 &lt;- torch$sub(w2, torch$mul(learning_rate, grad_w2)) } #&gt; 1 29428666 #&gt; 2 22572578 #&gt; 3 20474034 #&gt; 4 19486618 #&gt; 5 1.8e+07 #&gt; 6 15345387 #&gt; 7 1.2e+07 #&gt; 8 8557820 #&gt; 9 5777508 #&gt; 10 3791835 #&gt; 11 2494379 #&gt; 12 1679618 #&gt; 13 1176170 #&gt; 14 858874 #&gt; 15 654740 #&gt; 16 517359 #&gt; 17 421628 #&gt; 18 351479 #&gt; 19 298321 #&gt; 20 256309 #&gt; 21 222513 #&gt; 22 194530 #&gt; 23 171048 #&gt; 24 151092 #&gt; 25 134001 #&gt; 26 119256 #&gt; 27 106431 #&gt; 28 95220 #&gt; 29 85393 #&gt; 30 76739 #&gt; 31 69099 #&gt; 32 62340 #&gt; 33 56344 #&gt; 34 51009 #&gt; 35 46249 #&gt; 36 41992 #&gt; 37 38182 #&gt; 38 34770 #&gt; 39 31705 #&gt; 40 28946 #&gt; 41 26458 #&gt; 42 24211 #&gt; 43 22179 #&gt; 44 20334 #&gt; 45 18659 #&gt; 46 17138 #&gt; 47 15753 #&gt; 48 14494 #&gt; 49 13347 #&gt; 50 12301 "]
]
