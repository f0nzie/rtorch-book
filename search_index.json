[
["appendixB.html", "B Activation Functions B.1 The Sigmoid function B.2 The ReLU function B.3 The tanh function B.4 The Softmax Activation function B.5 Coding your own activation functions in Python B.6 Softmax in Python", " B Activation Functions library(rTorch) library(ggplot2) B.1 The Sigmoid function Using the PyTorch sigmoid() function: x &lt;- torch$range(-5., 5., 0.1) y &lt;- torch$sigmoid(x) df &lt;- data.frame(x = x$numpy(), sx = y$numpy()) df #&gt; x sx #&gt; 1 -5.0 0.00669 #&gt; 2 -4.9 0.00739 #&gt; 3 -4.8 0.00816 #&gt; 4 -4.7 0.00901 #&gt; 5 -4.6 0.00995 #&gt; 6 -4.5 0.01099 #&gt; 7 -4.4 0.01213 #&gt; 8 -4.3 0.01339 #&gt; 9 -4.2 0.01477 #&gt; 10 -4.1 0.01630 #&gt; 11 -4.0 0.01799 #&gt; 12 -3.9 0.01984 #&gt; 13 -3.8 0.02188 #&gt; 14 -3.7 0.02413 #&gt; 15 -3.6 0.02660 #&gt; 16 -3.5 0.02931 #&gt; 17 -3.4 0.03230 #&gt; 18 -3.3 0.03557 #&gt; 19 -3.2 0.03917 #&gt; 20 -3.1 0.04311 #&gt; 21 -3.0 0.04743 #&gt; 22 -2.9 0.05215 #&gt; 23 -2.8 0.05732 #&gt; 24 -2.7 0.06297 #&gt; 25 -2.6 0.06914 #&gt; 26 -2.5 0.07586 #&gt; 27 -2.4 0.08317 #&gt; 28 -2.3 0.09112 #&gt; 29 -2.2 0.09975 #&gt; 30 -2.1 0.10910 #&gt; 31 -2.0 0.11920 #&gt; 32 -1.9 0.13011 #&gt; 33 -1.8 0.14185 #&gt; 34 -1.7 0.15447 #&gt; 35 -1.6 0.16798 #&gt; 36 -1.5 0.18243 #&gt; 37 -1.4 0.19782 #&gt; 38 -1.3 0.21417 #&gt; 39 -1.2 0.23148 #&gt; 40 -1.1 0.24974 #&gt; 41 -1.0 0.26894 #&gt; 42 -0.9 0.28905 #&gt; 43 -0.8 0.31003 #&gt; 44 -0.7 0.33181 #&gt; 45 -0.6 0.35434 #&gt; 46 -0.5 0.37754 #&gt; 47 -0.4 0.40131 #&gt; 48 -0.3 0.42556 #&gt; 49 -0.2 0.45017 #&gt; 50 -0.1 0.47502 #&gt; 51 0.0 0.50000 #&gt; 52 0.1 0.52498 #&gt; 53 0.2 0.54983 #&gt; 54 0.3 0.57444 #&gt; 55 0.4 0.59869 #&gt; 56 0.5 0.62246 #&gt; 57 0.6 0.64566 #&gt; 58 0.7 0.66819 #&gt; 59 0.8 0.68997 #&gt; 60 0.9 0.71095 #&gt; 61 1.0 0.73106 #&gt; 62 1.1 0.75026 #&gt; 63 1.2 0.76852 #&gt; 64 1.3 0.78584 #&gt; 65 1.4 0.80218 #&gt; 66 1.5 0.81757 #&gt; 67 1.6 0.83202 #&gt; 68 1.7 0.84553 #&gt; 69 1.8 0.85815 #&gt; 70 1.9 0.86989 #&gt; 71 2.0 0.88080 #&gt; 72 2.1 0.89090 #&gt; 73 2.2 0.90025 #&gt; 74 2.3 0.90888 #&gt; 75 2.4 0.91683 #&gt; 76 2.5 0.92414 #&gt; 77 2.6 0.93086 #&gt; 78 2.7 0.93703 #&gt; 79 2.8 0.94268 #&gt; 80 2.9 0.94785 #&gt; 81 3.0 0.95257 #&gt; 82 3.1 0.95689 #&gt; 83 3.2 0.96083 #&gt; 84 3.3 0.96443 #&gt; 85 3.4 0.96770 #&gt; 86 3.5 0.97069 #&gt; 87 3.6 0.97340 #&gt; 88 3.7 0.97587 #&gt; 89 3.8 0.97812 #&gt; 90 3.9 0.98016 #&gt; 91 4.0 0.98201 #&gt; 92 4.1 0.98370 #&gt; 93 4.2 0.98523 #&gt; 94 4.3 0.98661 #&gt; 95 4.4 0.98787 #&gt; 96 4.5 0.98901 #&gt; 97 4.6 0.99005 #&gt; 98 4.7 0.99099 #&gt; 99 4.8 0.99184 #&gt; 100 4.9 0.99261 #&gt; 101 5.0 0.99331 ggplot(df, aes(x = x, y = sx)) + geom_point() + ggtitle(&quot;Sigmoid&quot;) Plot the sigmoid function using an R custom-made function: sigmoid = function(x) { 1 / (1 + exp(-x)) } x &lt;- seq(-5, 5, 0.01) plot(x, sigmoid(x), col = &#39;blue&#39;, cex = 0.5, main = &quot;Sigmoid&quot;) B.2 The ReLU function Using the PyTorch relu() function: x &lt;- torch$range(-5., 5., 0.1) y &lt;- torch$relu(x) df &lt;- data.frame(x = x$numpy(), sx = y$numpy()) df ggplot(df, aes(x = x, y = sx)) + geom_point() + ggtitle(&quot;ReLU&quot;) B.3 The tanh function Using the PyTorch tanh() function: x &lt;- torch$range(-5., 5., 0.1) y &lt;- torch$tanh(x) df &lt;- data.frame(x = x$numpy(), sx = y$numpy()) df ggplot(df, aes(x = x, y = sx)) + geom_point() + ggtitle(&quot;tanh&quot;) B.4 The Softmax Activation function Using the PyTorch tanh() function: x &lt;- torch$range(-5.0, 5.0, 0.1) y &lt;- torch$softmax(x, dim=0L) df &lt;- data.frame(x = x$numpy(), sx = y$numpy()) ggplot(df, aes(x = x, y = sx)) + geom_point() + ggtitle(&quot;Softmax&quot;) B.5 Coding your own activation functions in Python library(rTorch) import numpy as np import matplotlib.pyplot as plt np.random.seed(42) Linear activation def Linear(x, derivative=False): &quot;&quot;&quot; Computes the Linear activation function for array x inputs: x: array derivative: if True, return the derivative else the forward pass &quot;&quot;&quot; if derivative: # Return derivative of the function at x return np.ones_like(x) else: # Return forward pass of the function at x return x Sigmoid activation def Sigmoid(x, derivative=False): &quot;&quot;&quot; Computes the Sigmoid activation function for array x inputs: x: array derivative: if True, return the derivative else the forward pass &quot;&quot;&quot; f = 1/(1+np.exp(-x)) if derivative: # Return derivative of the function at x return f*(1-f) else: # Return forward pass of the function at x return f Hyperbolic Tangent activation def Tanh(x, derivative=False): &quot;&quot;&quot; Computes the Hyperbolic Tangent activation function for array x inputs: x: array derivative: if True, return the derivative else the forward pass &quot;&quot;&quot; f = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) if derivative: # Return derivative of the function at x return 1-f**2 else: # Return the forward pass of the function at x return f Rectifier linear unit (ReLU) def ReLU(x, derivative=False): &quot;&quot;&quot; Computes the Rectifier Linear Unit activation function for array x inputs: x: array derivative: if True, return the derivative else the forward pass &quot;&quot;&quot; if derivative: # Return derivative of the function at x return (x&gt;0).astype(int) else: # Return forward pass of the function at x return np.maximum(x, 0) Visualization Plotting using matplotlib: x = np.linspace(-6, 6, 100) units = { &quot;Linear&quot;: lambda x: Linear(x), &quot;Sigmoid&quot;: lambda x: Sigmoid(x), &quot;ReLU&quot;: lambda x: ReLU(x), &quot;tanh&quot;: lambda x: Tanh(x) } plt.figure(figsize=(5, 5)) [plt.plot(x, unit(x), label=unit_name, lw=2) for unit_name, unit in units.items()] plt.legend(loc=2, fontsize=16) plt.title(&#39;Activation functions&#39;, fontsize=20) plt.ylim([-2, 5]) plt.xlim([-6, 6]) plt.show() B.6 Softmax in Python # Source: https://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/ import numpy as np import matplotlib.pyplot as plt def softmax(inputs): &quot;&quot;&quot; Calculate the softmax for the give inputs (array) :param inputs: :return: &quot;&quot;&quot; return np.exp(inputs) / float(sum(np.exp(inputs))) def line_graph(x, y, x_title, y_title): &quot;&quot;&quot; Draw line graph with x and y values :param x: :param y: :param x_title: :param y_title: :return: &quot;&quot;&quot; plt.plot(x, y) plt.xlabel(x_title) plt.ylabel(y_title) plt.show() graph_x = np.linspace(-6, 6, 100) graph_y = softmax(graph_x) print(&quot;Graph X readings: {}&quot;.format(graph_x)) print(&quot;Graph Y readings: {}&quot;.format(graph_y)) line_graph(graph_x, graph_y, &quot;Inputs&quot;, &quot;Softmax Scores&quot;) "]
]
