[
["tensors.html", "Chapter 3 Tensors 3.1 Tensor data types 3.2 Arithmetic of tensors 3.3 NumPy and PyTorch 3.4 Create tensors 3.5 Tensor resizing 3.6 Reshape tensors 3.7 Special tensors 3.8 Tensor fill 3.9 Access to tensor elements 3.10 Other tensor operations 3.11 Logical operations 3.12 Distributions", " Chapter 3 Tensors We describe the most important PyTorch methods in this chapter. library(rTorch) 3.1 Tensor data types # Default data type torch$tensor(list(1.2, 3))$dtype # default for floating point is torch.float32 #&gt; torch.float32 # change default data type to float64 torch$set_default_dtype(torch$float64) torch$tensor(list(1.2, 3))$dtype # a new floating point tensor #&gt; torch.float64 There are five major type of Tensors in PyTorch library(rTorch) byte &lt;- torch$ByteTensor(3L, 3L) float &lt;- torch$FloatTensor(3L, 3L) double &lt;- torch$DoubleTensor(3L, 3L) long &lt;- torch$LongTensor(3L, 3L) boolean &lt;- torch$BoolTensor(5L, 5L) message(&quot;byte tensor&quot;) #&gt; byte tensor byte #&gt; tensor([[0, 0, 0], #&gt; [0, 0, 0], #&gt; [0, 0, 0]], dtype=torch.uint8) message(&quot;float tensor&quot;) #&gt; float tensor float #&gt; tensor([[0., 0., 0.], #&gt; [0., 0., 0.], #&gt; [0., 0., 0.]], dtype=torch.float32) message(&quot;double&quot;) #&gt; double double #&gt; tensor([[0., 0., 0.], #&gt; [0., 0., 0.], #&gt; [0., 0., 0.]]) message(&quot;long&quot;) #&gt; long long #&gt; tensor([[ 0, 0, 0], #&gt; [94028854591024, 0, 94028851725472], #&gt; [ 0, 0, 0]]) message(&quot;boolean&quot;) #&gt; boolean boolean #&gt; tensor([[ True, True, True, True, True], #&gt; [ True, False, False, True, True], #&gt; [ True, True, True, True, False], #&gt; [False, True, False, False, False], #&gt; [False, False, False, False, True]], dtype=torch.bool) A 4D tensor like in MNIST hand-written digits recognition dataset: mnist_4d &lt;- torch$FloatTensor(60000L, 3L, 28L, 28L) message(&quot;size&quot;) #&gt; size mnist_4d$size() #&gt; torch.Size([60000, 3, 28, 28]) message(&quot;length&quot;) #&gt; length length(mnist_4d) #&gt; [1] 141120000 message(&quot;shape, like in numpy&quot;) #&gt; shape, like in numpy mnist_4d$shape #&gt; torch.Size([60000, 3, 28, 28]) message(&quot;number of elements&quot;) #&gt; number of elements mnist_4d$numel() #&gt; [1] 141120000 A 3D tensor: ft3d &lt;- torch$FloatTensor(4L, 3L, 2L) ft3d #&gt; tensor([[[ 0.0000e+00, 0.0000e+00], #&gt; [ 0.0000e+00, nan], #&gt; [ 1.1444e-28, 1.2583e+00]], #&gt; #&gt; [[ 9.2250e+36, 1.3410e+00], #&gt; [ 1.1444e-28, 1.2583e+00], #&gt; [ 9.2250e+36, 1.3410e+00]], #&gt; #&gt; [[ 1.4660e+13, 1.5417e+00], #&gt; [ 1.7109e-30, 1.6350e+00], #&gt; [ 0.0000e+00, 0.0000e+00]], #&gt; #&gt; [[ 0.0000e+00, 4.5625e+00], #&gt; [ 0.0000e+00, 4.1943e+00], #&gt; [ 0.0000e+00, -4.1943e+00]]], dtype=torch.float32) 3.2 Arithmetic of tensors 3.2.1 Add tensors # add a scalar to a tensor # 3x5 matrix uniformly distributed between 0 and 1 mat0 &lt;- torch$FloatTensor(3L, 5L)$uniform_(0L, 1L) mat0 + 0.1 #&gt; tensor([[0.9229, 0.4725, 0.4518, 0.7611, 0.4710], #&gt; [0.1383, 0.7360, 0.2547, 0.4312, 0.9219], #&gt; [0.8313, 1.0674, 0.8798, 0.5295, 0.7414]], dtype=torch.float32) The expression tensor.index(m) is equivalent to tensor[m]. Add an element of tensor to a tensor: # fill a 3x5 matrix with 0.1 mat1 &lt;- torch$FloatTensor(3L, 5L)$uniform_(0.1, 0.1) # a vector with all ones mat2 &lt;- torch$FloatTensor(5L)$uniform_(1, 1) mat1[1, 1] + mat2 #&gt; tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000], dtype=torch.float32) # add two tensors mat1 + mat0 #&gt; tensor([[0.9229, 0.4725, 0.4518, 0.7611, 0.4710], #&gt; [0.1383, 0.7360, 0.2547, 0.4312, 0.9219], #&gt; [0.8313, 1.0674, 0.8798, 0.5295, 0.7414]], dtype=torch.float32) Add two tensors using the function add(): # PyTorch add two tensors x = torch$rand(5L, 4L) y = torch$rand(5L, 4L) print(x$add(y)) #&gt; tensor([[1.6924, 1.2777, 0.7074, 1.0936], #&gt; [1.6382, 0.7486, 0.7621, 1.0250], #&gt; [1.4642, 0.8942, 1.1989, 1.4702], #&gt; [0.9506, 0.2731, 1.3442, 1.3144], #&gt; [1.4281, 0.7944, 1.4052, 1.1495]]) Add two tensors using the generic +: print(x + y) #&gt; tensor([[1.6924, 1.2777, 0.7074, 1.0936], #&gt; [1.6382, 0.7486, 0.7621, 1.0250], #&gt; [1.4642, 0.8942, 1.1989, 1.4702], #&gt; [0.9506, 0.2731, 1.3442, 1.3144], #&gt; [1.4281, 0.7944, 1.4052, 1.1495]]) 3.2.2 Multiply a tensor by a scalar # Multiply tensor by scalar tensor = torch$ones(4L, dtype=torch$float64) scalar = np$float64(4.321) print(scalar) #&gt; [1] 4.32 print(torch$scalar_tensor(scalar)) #&gt; tensor(4.3210) Multiply two tensors using the function mul: (prod = torch$mul(tensor, torch$scalar_tensor(scalar))) #&gt; tensor([4.3210, 4.3210, 4.3210, 4.3210]) Short version using generics (prod = tensor * scalar) #&gt; tensor([4.3210, 4.3210, 4.3210, 4.3210]) 3.3 NumPy and PyTorch numpy has been made available as a module in rTorch. We can call functions from numpy refrerring to it as np$_a_function. Examples: # a 2D numpy array syn0 &lt;- np$random$rand(3L, 5L) print(syn0) #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 0.722 0.896 0.932 0.766 0.266 #&gt; [2,] 0.588 0.160 0.420 0.167 0.429 #&gt; [3,] 0.801 0.428 0.706 0.126 0.191 # numpy arrays of zeros syn1 &lt;- np$zeros(c(5L, 10L)) print(syn1) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 0 0 0 0 0 0 0 0 0 0 #&gt; [2,] 0 0 0 0 0 0 0 0 0 0 #&gt; [3,] 0 0 0 0 0 0 0 0 0 0 #&gt; [4,] 0 0 0 0 0 0 0 0 0 0 #&gt; [5,] 0 0 0 0 0 0 0 0 0 0 # add a scalar to a numpy array syn1 = syn1 + 0.1 print(syn1) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 #&gt; [2,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 #&gt; [3,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 #&gt; [4,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 #&gt; [5,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 3.3.1 Tuples (Python) and vectors (R) In numpy a multidimensional array needs to be defined with a tuple in R we do it with a vector. In Python, we use a tuple, (5, 5) import numpy as np print(np.ones((5, 5))) #&gt; [[1. 1. 1. 1. 1.] #&gt; [1. 1. 1. 1. 1.] #&gt; [1. 1. 1. 1. 1.] #&gt; [1. 1. 1. 1. 1.] #&gt; [1. 1. 1. 1. 1.]] In R, we use a vector c(5L, 5L). The L indicates an integer. l1 &lt;- np$ones(c(5L, 5L)) print(l1) #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 1 1 1 1 1 #&gt; [2,] 1 1 1 1 1 #&gt; [3,] 1 1 1 1 1 #&gt; [4,] 1 1 1 1 1 #&gt; [5,] 1 1 1 1 1 Vector-matrix multiplication in numpy: np$dot(syn0, syn1) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 0.358 0.358 0.358 0.358 0.358 0.358 0.358 0.358 0.358 0.358 #&gt; [2,] 0.176 0.176 0.176 0.176 0.176 0.176 0.176 0.176 0.176 0.176 #&gt; [3,] 0.225 0.225 0.225 0.225 0.225 0.225 0.225 0.225 0.225 0.225 Build a numpy array from three R vectors: X &lt;- np$array(rbind(c(1,2,3), c(4,5,6), c(7,8,9))) print(X) #&gt; [,1] [,2] [,3] #&gt; [1,] 1 2 3 #&gt; [2,] 4 5 6 #&gt; [3,] 7 8 9 And transpose the array: np$transpose(X) #&gt; [,1] [,2] [,3] #&gt; [1,] 1 4 7 #&gt; [2,] 2 5 8 #&gt; [3,] 3 6 9 3.3.2 Make a numpy array a tensor with as_tensor() a = np$array(list(1, 2, 3)) # a numpy array t = torch$as_tensor(a) # convert it to tensor print(t) #&gt; tensor([1., 2., 3.]) We can create the tensor directly from R using tensor(): torch$tensor(list( 1, 2, 3)) # create a tensor #&gt; tensor([1., 2., 3.]) t[1L]$fill_(-1) # fill element with -1 #&gt; tensor(-1.) print(a) #&gt; [1] -1 2 3 3.3.3 Tensor to array, and viceversa This is a very common operation in machine learning: # convert tensor to a numpy array a = torch$rand(5L, 4L) b = a$numpy() print(b) #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 0.4496 0.406 0.318 0.622 #&gt; [2,] 0.5525 0.963 0.465 0.570 #&gt; [3,] 0.2062 0.442 0.235 0.838 #&gt; [4,] 0.0958 0.739 0.806 0.448 #&gt; [5,] 0.5112 0.458 0.766 0.766 # convert a numpy array to a tensor np_a = np$array(c(c(3, 4), c(3, 6))) t_a = torch$from_numpy(np_a) print(t_a) #&gt; tensor([3., 4., 3., 6.]) 3.4 Create tensors A random 1D tensor: ft1 &lt;- torch$FloatTensor(np$random$rand(5L)) print(ft1) #&gt; tensor([0.5116, 0.6226, 0.7873, 0.2706, 0.7585], dtype=torch.float32) Force a tensor as a float of 64-bits: ft2 &lt;- torch$as_tensor(np$random$rand(5L), dtype= torch$float64) print(ft2) #&gt; tensor([0.0163, 0.6670, 0.9927, 0.3526, 0.4683]) Convert the tensor to float 16-bits: ft2_dbl &lt;- torch$as_tensor(ft2, dtype = torch$float16) ft2_dbl #&gt; tensor([0.0163, 0.6670, 0.9927, 0.3525, 0.4683], dtype=torch.float16) Create a tensor of size (5 x 7) with uninitialized memory: a &lt;- torch$FloatTensor(5L, 7L) print(a) #&gt; tensor([[-1.8898e-37, 4.5565e-41, -1.4140e+07, 3.0677e-41, -6.1620e+02, #&gt; 3.0677e-41, -3.1368e-37], #&gt; [ 4.5565e-41, 7.5670e-44, 0.0000e+00, 0.0000e+00, 0.0000e+00, #&gt; 1.4013e-45, 2.8026e-45], #&gt; [ 4.2039e-45, 5.6052e-45, 7.0065e-45, 8.4078e-45, 9.8091e-45, #&gt; 1.1210e-44, 1.2612e-44], #&gt; [ 1.4013e-44, 1.5414e-44, 1.6816e-44, 1.8217e-44, 1.9618e-44, #&gt; 2.1019e-44, 2.2421e-44], #&gt; [ 2.3822e-44, 2.5223e-44, 2.6625e-44, 2.8026e-44, 2.9427e-44, #&gt; 3.0829e-44, 3.2230e-44]], dtype=torch.float32) Using arange to create a tensor. Start from 0: v = torch$arange(9L) (v = v$view(3L, 3L)) #&gt; tensor([[0, 1, 2], #&gt; [3, 4, 5], #&gt; [6, 7, 8]]) 3.5 Tensor resizing x = torch$randn(2L, 3L) # Size 2x3 y = x$view(6L) # Resize x to size 6 z = x$view(-1L, 2L) # Size 3x2 print(y) #&gt; tensor([ 0.3614, 1.0859, -1.3490, 1.4022, 1.8910, -0.2455]) print(z) #&gt; tensor([[ 0.3614, 1.0859], #&gt; [-1.3490, 1.4022], #&gt; [ 1.8910, -0.2455]]) Reproduce this tensor: 0 1 2 3 4 5 6 7 8 v = torch$arange(9L) (v = v$view(3L, 3L)) #&gt; tensor([[0, 1, 2], #&gt; [3, 4, 5], #&gt; [6, 7, 8]]) 3.5.1 Concatenate tensors x = torch$randn(2L, 3L) print(x) #&gt; tensor([[-0.2743, 0.1562, -2.5493], #&gt; [ 1.1262, 0.3277, 0.0343]]) Concatenate tensors by dim=0: torch$cat(list(x, x, x), 0L) #&gt; tensor([[-0.2743, 0.1562, -2.5493], #&gt; [ 1.1262, 0.3277, 0.0343], #&gt; [-0.2743, 0.1562, -2.5493], #&gt; [ 1.1262, 0.3277, 0.0343], #&gt; [-0.2743, 0.1562, -2.5493], #&gt; [ 1.1262, 0.3277, 0.0343]]) Concatenate tensors by dim=1: torch$cat(list(x, x, x), 1L) #&gt; tensor([[-0.2743, 0.1562, -2.5493, -0.2743, 0.1562, -2.5493, -0.2743, 0.1562, #&gt; -2.5493], #&gt; [ 1.1262, 0.3277, 0.0343, 1.1262, 0.3277, 0.0343, 1.1262, 0.3277, #&gt; 0.0343]]) 3.6 Reshape tensors 3.6.1 With function chunk(): Let’s say this is an image tensor with the 3-channels and 28x28 pixels # ----- Reshape tensors ----- img &lt;- torch$ones(3L, 28L, 28L) # Create the tensor of ones print(img$size()) #&gt; torch.Size([3, 28, 28]) On the first dimension dim = 0L, reshape the tensor: img_chunks &lt;- torch$chunk(img, chunks = 3L, dim = 0L) print(length(img_chunks)) #&gt; [1] 3 The first chunk member: # 1st chunk member img_chunk &lt;- img_chunks[[1]] print(img_chunk$size()) #&gt; torch.Size([1, 28, 28]) print(img_chunk$sum()) # if the tensor had all ones, what is the sum? #&gt; tensor(784.) The second chunk member: # 2nd chunk member img_chunk &lt;- img_chunks[[2]] print(img_chunk$size()) #&gt; torch.Size([1, 28, 28]) print(img_chunk$sum()) # if the tensor had all ones, what is the sum? #&gt; tensor(784.) # 3rd chunk member img_chunk &lt;- img_chunks[[3]] print(img_chunk$size()) #&gt; torch.Size([1, 28, 28]) print(img_chunk$sum()) # if the tensor had all ones, what is the sum? #&gt; tensor(784.) 3.6.2 With index_select(): img &lt;- torch$ones(3L, 28L, 28L) # Create the tensor of ones This is the layer 1: # index_select. get layer 1 indices = torch$tensor(c(0L)) img_layer &lt;- torch$index_select(img, dim = 0L, index = indices) The size of the layer: print(img_layer$size()) #&gt; torch.Size([1, 28, 28]) The sum of all elements in that layer: print(img_layer$sum()) #&gt; tensor(784.) This is the layer 2: # index_select. get layer 2 indices = torch$tensor(c(1L)) img_layer &lt;- torch$index_select(img, dim = 0L, index = indices) print(img_layer$size()) #&gt; torch.Size([1, 28, 28]) print(img_layer$sum()) #&gt; tensor(784.) This is the layer 3: # index_select. get layer 3 indices = torch$tensor(c(2L)) img_layer &lt;- torch$index_select(img, dim = 0L, index = indices) print(img_layer$size()) #&gt; torch.Size([1, 28, 28]) print(img_layer$sum()) #&gt; tensor(784.) 3.7 Special tensors 3.7.1 Identity matrix # identity matrix eye = torch$eye(3L) # Create an identity 3x3 tensor print(eye) #&gt; tensor([[1., 0., 0.], #&gt; [0., 1., 0.], #&gt; [0., 0., 1.]]) 3.7.2 Ones (v = torch$ones(10L)) # A tensor of size 10 containing all ones #&gt; tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) (v = torch$ones(2L, 1L, 2L, 1L)) # Size 2x1x2x1 #&gt; tensor([[[[1.], #&gt; [1.]]], #&gt; #&gt; #&gt; [[[1.], #&gt; [1.]]]]) v = torch$ones_like(eye) # A tensor with same shape as eye. Fill it with 1. v #&gt; tensor([[1., 1., 1.], #&gt; [1., 1., 1.], #&gt; [1., 1., 1.]]) 3.7.3 Zeros (z = torch$zeros(10L)) # A tensor of size 10 containing all zeros #&gt; tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) 3.8 Tensor fill On this tensor: (v = torch$ones(3L, 3L)) #&gt; tensor([[1., 1., 1.], #&gt; [1., 1., 1.], #&gt; [1., 1., 1.]]) Fill row 1 with 2s: v[1L, ]$fill_(2L) #&gt; tensor([2., 2., 2.]) print(v) #&gt; tensor([[2., 2., 2.], #&gt; [1., 1., 1.], #&gt; [1., 1., 1.]]) Fill row 2 with 3s: v[2L, ]$fill_(3L) #&gt; tensor([3., 3., 3.]) print(v) #&gt; tensor([[2., 2., 2.], #&gt; [3., 3., 3.], #&gt; [1., 1., 1.]]) # Initialize Tensor with a range of value v = torch$arange(10L) # similar to range(5) but creating a Tensor (v = torch$arange(0L, 10L, step = 1L)) # Size 5. Similar to range(0, 5, 1) #&gt; tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 3.8.1 Initialize a linear or log scale Tensor Create a tensor with 10 linear points for (1, 10) inclusive: (v = torch$linspace(1L, 10L, steps = 10L)) #&gt; tensor([ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.]) Create a tensor with 10 logarithmic points for (1, 10) inclusive: (v = torch$logspace(start=-10L, end = 10L, steps = 5L)) #&gt; tensor([1.0000e-10, 1.0000e-05, 1.0000e+00, 1.0000e+05, 1.0000e+10]) 3.8.2 Inplace / Out-of-place On this uninitialized tensor: (a &lt;- torch$FloatTensor(5L, 7L)) #&gt; tensor([[-1.8898e-37, 4.5565e-41, -2.1946e+07, 3.0677e-41, 0.0000e+00, #&gt; 0.0000e+00, 0.0000e+00], #&gt; [ 0.0000e+00, -9.6936e+00, 3.0677e-41, 3.8255e-43, 0.0000e+00, #&gt; -1.7251e+05, 3.0677e-41], #&gt; [-1.8898e-37, 4.5565e-41, -2.9606e+00, 3.0677e-41, -5.9568e+00, #&gt; 3.0677e-41, -2.0789e+00], #&gt; [ 3.0677e-41, 3.1529e-43, 0.0000e+00, -1.8898e-37, 4.5565e-41, #&gt; -6.7922e+07, 3.0677e-41], #&gt; [ 0.0000e+00, 0.0000e+00, -1.6589e+07, 3.0677e-41, 0.0000e+00, #&gt; 0.0000e+00, 0.0000e+00]], dtype=torch.float32) Fill the tensor with the value 3.5: a$fill_(3.5) #&gt; tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]], #&gt; dtype=torch.float32) Add a scalar to the tensor: b &lt;- a$add(4.0) The tensor a is still filled with 3.5. A new tensor b is returned with values 3.5 + 4.0 = 7.5 print(a) #&gt; tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]], #&gt; dtype=torch.float32) print(b) #&gt; tensor([[7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000], #&gt; [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000], #&gt; [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000], #&gt; [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000], #&gt; [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000]], #&gt; dtype=torch.float32) 3.9 Access to tensor elements # replace an element at position 0, 0 (new_tensor = torch$Tensor(list(list(1, 2), list(3, 4)))) #&gt; tensor([[1., 2.], #&gt; [3., 4.]]) Print element at position 1,1: print(new_tensor[1L, 1L]) #&gt; tensor(1.) Fill element at position 1,1 with 5: new_tensor[1L, 1L]$fill_(5) #&gt; tensor(5.) Show the modified tensor: print(new_tensor) # tensor([[ 5., 2.],[ 3., 4.]]) #&gt; tensor([[5., 2.], #&gt; [3., 4.]]) Access an element at position 1, 0: print(new_tensor[2L, 1L]) # tensor([ 3.]) #&gt; tensor(3.) print(new_tensor[2L, 1L]$item()) # 3. #&gt; [1] 3 3.9.1 Using indices to access elements On this tensor: x = torch$randn(3L, 4L) print(x) #&gt; tensor([[-2.6779, 0.7904, -0.6174, -0.5065], #&gt; [-1.0275, 0.4956, -0.2455, -1.6991], #&gt; [ 1.1835, -1.2575, 0.5088, 0.3303]]) Select indices, dim=0: indices = torch$tensor(list(0L, 2L)) torch$index_select(x, 0L, indices) #&gt; tensor([[-2.6779, 0.7904, -0.6174, -0.5065], #&gt; [ 1.1835, -1.2575, 0.5088, 0.3303]]) Select indices, dim=1: torch$index_select(x, 1L, indices) #&gt; tensor([[-2.6779, -0.6174], #&gt; [-1.0275, -0.2455], #&gt; [ 1.1835, 0.5088]]) 3.9.2 Using the take function # Take by indices src = torch$tensor(list(list(4, 3, 5), list(6, 7, 8)) ) print(src) #&gt; tensor([[4., 3., 5.], #&gt; [6., 7., 8.]]) print( torch$take(src, torch$tensor(list(0L, 2L, 5L))) ) #&gt; tensor([4., 5., 8.]) 3.10 Other tensor operations 3.10.1 cross product m1 = torch$ones(3L, 5L) m2 = torch$ones(3L, 5L) v1 = torch$ones(3L) # Cross product # Size 3x5 (r = torch$cross(m1, m2)) #&gt; tensor([[0., 0., 0., 0., 0.], #&gt; [0., 0., 0., 0., 0.], #&gt; [0., 0., 0., 0., 0.]]) 3.10.2 Dot product # Dot product of 2 tensors # Dot product of 2 tensors p &lt;- torch$Tensor(list(4L, 2L)) q &lt;- torch$Tensor(list(3L, 1L)) (r = torch$dot(p, q)) # 14 #&gt; tensor(14.) (r &lt;- p %.*% q) #&gt; tensor(14.) 3.11 Logical operations m0 = torch$zeros(3L, 5L) m1 = torch$ones(3L, 5L) m2 = torch$eye(3L, 5L) print(m1 == m0) #&gt; tensor([[False, False, False, False, False], #&gt; [False, False, False, False, False], #&gt; [False, False, False, False, False]], dtype=torch.bool) print(m1 != m1) #&gt; tensor([[False, False, False, False, False], #&gt; [False, False, False, False, False], #&gt; [False, False, False, False, False]], dtype=torch.bool) print(m2 == m2) #&gt; tensor([[True, True, True, True, True], #&gt; [True, True, True, True, True], #&gt; [True, True, True, True, True]], dtype=torch.bool) # AND m1 &amp; m1 #&gt; tensor([[True, True, True, True, True], #&gt; [True, True, True, True, True], #&gt; [True, True, True, True, True]], dtype=torch.bool) # OR m0 | m2 #&gt; tensor([[ True, False, False, False, False], #&gt; [False, True, False, False, False], #&gt; [False, False, True, False, False]], dtype=torch.bool) # OR m1 | m2 #&gt; tensor([[True, True, True, True, True], #&gt; [True, True, True, True, True], #&gt; [True, True, True, True, True]], dtype=torch.bool) # all_boolean &lt;- function(x) { # # convert tensor of 1s and 0s to a unique boolean # as.logical(torch$all(x)$numpy()) # } # tensor is less than A &lt;- torch$ones(60000L, 1L, 28L, 28L) C &lt;- A * 0.5 # is C &lt; A all(torch$lt(C, A)) #&gt; tensor(1, dtype=torch.uint8) all(C &lt; A) #&gt; tensor(1, dtype=torch.uint8) # is A &lt; C all(A &lt; C) #&gt; tensor(0, dtype=torch.uint8) # tensor is greater than A &lt;- torch$ones(60000L, 1L, 28L, 28L) D &lt;- A * 2.0 all(torch$gt(D, A)) #&gt; tensor(1, dtype=torch.uint8) all(torch$gt(A, D)) #&gt; tensor(0, dtype=torch.uint8) # tensor is less than or equal A1 &lt;- torch$ones(60000L, 1L, 28L, 28L) all(torch$le(A1, A1)) #&gt; tensor(1, dtype=torch.uint8) all(A1 &lt;= A1) #&gt; tensor(1, dtype=torch.uint8) # tensor is greater than or equal A0 &lt;- torch$zeros(60000L, 1L, 28L, 28L) all(torch$ge(A0, A0)) #&gt; tensor(1, dtype=torch.uint8) all(A0 &gt;= A0) #&gt; tensor(1, dtype=torch.uint8) all(A1 &gt;= A0) #&gt; tensor(1, dtype=torch.uint8) all(A1 &lt;= A0) #&gt; tensor(0, dtype=torch.uint8) 3.11.1 Logical NOT all_true &lt;- torch$BoolTensor(list(TRUE, TRUE, TRUE, TRUE)) all_true #&gt; tensor([True, True, True, True], dtype=torch.bool) # logical NOT not_all_true &lt;- !all_true not_all_true #&gt; tensor([False, False, False, False], dtype=torch.bool) diag &lt;- torch$eye(5L) diag #&gt; tensor([[1., 0., 0., 0., 0.], #&gt; [0., 1., 0., 0., 0.], #&gt; [0., 0., 1., 0., 0.], #&gt; [0., 0., 0., 1., 0.], #&gt; [0., 0., 0., 0., 1.]]) # logical NOT not_diag &lt;- !diag # convert to integer not_diag$to(dtype=torch$uint8) #&gt; tensor([[0, 1, 1, 1, 1], #&gt; [1, 0, 1, 1, 1], #&gt; [1, 1, 0, 1, 1], #&gt; [1, 1, 1, 0, 1], #&gt; [1, 1, 1, 1, 0]], dtype=torch.uint8) 3.12 Distributions Initialize a tensor randomized with a normal distribution with mean=0, var=1: a &lt;- torch$randn(5L, 7L) print(a) #&gt; tensor([[-0.2440, 0.3863, 0.5554, 1.9857, 0.5566, -0.5568, -0.2788], #&gt; [ 1.8216, -0.4341, -0.8520, -1.2455, -0.6905, -1.1641, -0.0847], #&gt; [ 0.9347, -2.0082, -1.1771, -0.1572, 0.8908, -1.0802, 2.2156], #&gt; [-0.2378, -1.6119, 0.3687, 0.3101, -0.1219, 0.7256, -0.3620], #&gt; [ 1.1319, 0.7339, 0.3024, 0.3040, -0.4364, 0.3858, -0.7121]]) print(a$size()) #&gt; torch.Size([5, 7]) 3.12.1 Uniform matrix library(rTorch) # 3x5 matrix uniformly distributed between 0 and 1 mat0 &lt;- torch$FloatTensor(3L, 5L)$uniform_(0L, 1L) # fill a 3x5 matrix with 0.1 mat1 &lt;- torch$FloatTensor(3L, 5L)$uniform_(0.1, 0.1) # a vector with all ones mat2 &lt;- torch$FloatTensor(5L)$uniform_(1, 1) mat0 #&gt; tensor([[0.6971, 0.2102, 0.7090, 0.5477, 0.4500], #&gt; [0.7571, 0.9322, 0.1615, 0.5475, 0.1401], #&gt; [0.5822, 0.6926, 0.5127, 0.8093, 0.1165]], dtype=torch.float32) mat1 #&gt; tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000], #&gt; [0.1000, 0.1000, 0.1000, 0.1000, 0.1000], #&gt; [0.1000, 0.1000, 0.1000, 0.1000, 0.1000]], dtype=torch.float32) 3.12.2 Binomial distribution Binomial &lt;- torch$distributions$binomial$Binomial m = Binomial(100, torch$tensor(list(0 , .2, .8, 1))) (x = m$sample()) #&gt; tensor([ 0., 21., 78., 100.]) m = Binomial(torch$tensor(list(list(5.), list(10.))), torch$tensor(list(0.5, 0.8))) (x = m$sample()) #&gt; tensor([[2., 3.], #&gt; [5., 9.]]) 3.12.3 Exponential distribution Exponential &lt;- torch$distributions$exponential$Exponential m = Exponential(torch$tensor(list(1.0))) m$sample() # Exponential distributed with rate=1 #&gt; tensor([0.7231]) 3.12.4 Weibull distribution Weibull &lt;- torch$distributions$weibull$Weibull m = Weibull(torch$tensor(list(1.0)), torch$tensor(list(1.0))) m$sample() # sample from a Weibull distribution with scale=1, concentration=1 #&gt; tensor([0.0222]) "]
]
