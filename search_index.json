[
["simple-regression-with-pytorch.html", "Chapter 9 Simple Regression with PyTorch 9.1 Creating the network model 9.2 Datasets 9.3 Optimizer and Loss 9.4 Training 9.5 Result", " Chapter 9 Simple Regression with PyTorch Source: https://www.guru99.com/pytorch-tutorial.html 9.1 Creating the network model Our network model is a simple Linear layer with an input and an output shape of 1. library(rTorch) from __future__ import print_function import torch import torch.nn as nn import torch.nn.functional as F from torch.autograd import Variable torch.manual_seed(123) #&gt; &lt;torch._C.Generator object at 0x7f3f4528da70&gt; class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.layer = torch.nn.Linear(1, 1) def forward(self, x): x = self.layer(x) return x net = Net() print(net) #&gt; Net( #&gt; (layer): Linear(in_features=1, out_features=1, bias=True) #&gt; ) And the network output should be like this Net( (hidden): Linear(in_features=1, out_features=1, bias=True) ) 9.1.1 Code in R This would be the equivalent code in R: library(reticulate) #&gt; #&gt; Attaching package: &#39;reticulate&#39; #&gt; The following objects are masked from &#39;package:rTorch&#39;: #&gt; #&gt; conda_install, conda_python torch &lt;- import(&quot;torch&quot;) nn &lt;- import(&quot;torch.nn&quot;) Variable &lt;- import(&quot;torch.autograd&quot;)$Variable torch$manual_seed(123) #&gt; &lt;torch._C.Generator&gt; main = py_run_string( &quot; import torch.nn as nn class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.layer = torch.nn.Linear(1, 1) def forward(self, x): x = self.layer(x) return x &quot;) # build a Linear Rgression model net &lt;- main$Net() print(net) #&gt; Net( #&gt; (layer): Linear(in_features=1, out_features=1, bias=True) #&gt; ) 9.2 Datasets Before you start the training process, you need to know our data. You make a random function to test our model. \\(Y = x3 sin(x)+ 3x+0.8 rand(100)\\) # Visualize our data import matplotlib.pyplot as plt import numpy as np np.random.seed(123) x = np.random.rand(100) y = np.sin(x) * np.power(x,3) + 3*x + np.random.rand(100)*0.8 plt.scatter(x, y) plt.show() This is the code in R: np &lt;- import(&quot;numpy&quot;) np$random$seed(123L) x = np$random$rand(100L) y = np$sin(x) * np$power(x, 3L) + 3*x + np$random$rand(100L)*0.8 plot(x, y) Before you start the training process, you need to convert the numpy array to Variables that supported by Torch and autograd. # convert numpy array to tensor in shape of input size x = torch.from_numpy(x.reshape(-1,1)).float() y = torch.from_numpy(y.reshape(-1,1)).float() print(x, y) #&gt; tensor([[0.6965], #&gt; [0.2861], #&gt; [0.2269], #&gt; [0.5513], #&gt; [0.7195], #&gt; [0.4231], #&gt; [0.9808], #&gt; [0.6848], #&gt; [0.4809], #&gt; [0.3921], #&gt; [0.3432], #&gt; [0.7290], #&gt; [0.4386], #&gt; [0.0597], #&gt; [0.3980], #&gt; [0.7380], #&gt; [0.1825], #&gt; [0.1755], #&gt; [0.5316], #&gt; [0.5318], #&gt; [0.6344], #&gt; [0.8494], #&gt; [0.7245], #&gt; [0.6110], #&gt; [0.7224], #&gt; [0.3230], #&gt; [0.3618], #&gt; [0.2283], #&gt; [0.2937], #&gt; [0.6310], #&gt; [0.0921], #&gt; [0.4337], #&gt; [0.4309], #&gt; [0.4937], #&gt; [0.4258], #&gt; [0.3123], #&gt; [0.4264], #&gt; [0.8934], #&gt; [0.9442], #&gt; [0.5018], #&gt; [0.6240], #&gt; [0.1156], #&gt; [0.3173], #&gt; [0.4148], #&gt; [0.8663], #&gt; [0.2505], #&gt; [0.4830], #&gt; [0.9856], #&gt; [0.5195], #&gt; [0.6129], #&gt; [0.1206], #&gt; [0.8263], #&gt; [0.6031], #&gt; [0.5451], #&gt; [0.3428], #&gt; [0.3041], #&gt; [0.4170], #&gt; [0.6813], #&gt; [0.8755], #&gt; [0.5104], #&gt; [0.6693], #&gt; [0.5859], #&gt; [0.6249], #&gt; [0.6747], #&gt; [0.8423], #&gt; [0.0832], #&gt; [0.7637], #&gt; [0.2437], #&gt; [0.1942], #&gt; [0.5725], #&gt; [0.0957], #&gt; [0.8853], #&gt; [0.6272], #&gt; [0.7234], #&gt; [0.0161], #&gt; [0.5944], #&gt; [0.5568], #&gt; [0.1590], #&gt; [0.1531], #&gt; [0.6955], #&gt; [0.3188], #&gt; [0.6920], #&gt; [0.5544], #&gt; [0.3890], #&gt; [0.9251], #&gt; [0.8417], #&gt; [0.3574], #&gt; [0.0436], #&gt; [0.3048], #&gt; [0.3982], #&gt; [0.7050], #&gt; [0.9954], #&gt; [0.3559], #&gt; [0.7625], #&gt; [0.5932], #&gt; [0.6917], #&gt; [0.1511], #&gt; [0.3989], #&gt; [0.2409], #&gt; [0.3435]]) tensor([[2.7166], #&gt; [1.3983], #&gt; [0.7679], #&gt; [1.8464], #&gt; [2.6614], #&gt; [1.8297], #&gt; [4.4034], #&gt; [2.7003], #&gt; [2.1778], #&gt; [1.5073], #&gt; [1.2966], #&gt; [2.7287], #&gt; [1.4884], #&gt; [0.8423], #&gt; [1.4895], #&gt; [2.9263], #&gt; [1.0114], #&gt; [0.9445], #&gt; [1.6729], #&gt; [2.4624], #&gt; [2.7788], #&gt; [3.1746], #&gt; [2.6593], #&gt; [2.3800], #&gt; [3.1382], #&gt; [1.7665], #&gt; [1.3082], #&gt; [1.1390], #&gt; [1.5341], #&gt; [2.3566], #&gt; [0.8612], #&gt; [1.4642], #&gt; [1.8066], #&gt; [2.2308], #&gt; [2.0962], #&gt; [1.0096], #&gt; [1.6538], #&gt; [3.3994], #&gt; [3.8747], #&gt; [2.0045], #&gt; [2.0884], #&gt; [0.5845], #&gt; [1.7039], #&gt; [1.7285], #&gt; [3.4602], #&gt; [1.3581], #&gt; [2.0949], #&gt; [3.7935], #&gt; [2.1950], #&gt; [2.6425], #&gt; [0.4948], #&gt; [3.5188], #&gt; [2.1628], #&gt; [1.9643], #&gt; [1.5740], #&gt; [1.0099], #&gt; [1.8123], #&gt; [2.9534], #&gt; [3.6986], #&gt; [1.9485], #&gt; [2.5445], #&gt; [2.4811], #&gt; [2.4700], #&gt; [2.2838], #&gt; [3.4392], #&gt; [0.9015], #&gt; [2.8687], #&gt; [1.4766], #&gt; [1.1847], #&gt; [2.2782], #&gt; [0.8885], #&gt; [3.2565], #&gt; [2.7141], #&gt; [3.0781], #&gt; [0.7763], #&gt; [2.0038], #&gt; [1.8270], #&gt; [0.5882], #&gt; [0.7793], #&gt; [2.6416], #&gt; [1.4162], #&gt; [2.3851], #&gt; [1.9140], #&gt; [1.8385], #&gt; [3.7822], #&gt; [3.6160], #&gt; [1.0941], #&gt; [0.5721], #&gt; [1.6683], #&gt; [1.6848], #&gt; [2.5068], #&gt; [4.3876], #&gt; [1.3866], #&gt; [3.1286], #&gt; [1.9197], #&gt; [2.7949], #&gt; [0.4797], #&gt; [1.8171], #&gt; [1.1042], #&gt; [1.1414]]) 9.2.1 Code in R Notice that before converting to a Torch tensor, we need first to convert the R numeric vector to a numpy array: # convert numpy array to tensor in shape of input size x &lt;- r_to_py(x) y &lt;- r_to_py(y) x = torch$from_numpy(x$reshape(-1L, 1L)) #$float() y = torch$from_numpy(y$reshape(-1L, 1L)) #$float() print(x, y) #&gt; tensor([[0.6965], #&gt; [0.2861], #&gt; [0.2269], #&gt; [0.5513], #&gt; [0.7195], #&gt; [0.4231], #&gt; [0.9808], #&gt; [0.6848], #&gt; [0.4809], #&gt; [0.3921], #&gt; [0.3432], #&gt; [0.7290], #&gt; [0.4386], #&gt; [0.0597], #&gt; [0.3980], #&gt; [0.7380], #&gt; [0.1825], #&gt; [0.1755], #&gt; [0.5316], #&gt; [0.5318], #&gt; [0.6344], #&gt; [0.8494], #&gt; [0.7245], #&gt; [0.6110], #&gt; [0.7224], #&gt; [0.3230], #&gt; [0.3618], #&gt; [0.2283], #&gt; [0.2937], #&gt; [0.6310], #&gt; [0.0921], #&gt; [0.4337], #&gt; [0.4309], #&gt; [0.4937], #&gt; [0.4258], #&gt; [0.3123], #&gt; [0.4264], #&gt; [0.8934], #&gt; [0.9442], #&gt; [0.5018], #&gt; [0.6240], #&gt; [0.1156], #&gt; [0.3173], #&gt; [0.4148], #&gt; [0.8663], #&gt; [0.2505], #&gt; [0.4830], #&gt; [0.9856], #&gt; [0.5195], #&gt; [0.6129], #&gt; [0.1206], #&gt; [0.8263], #&gt; [0.6031], #&gt; [0.5451], #&gt; [0.3428], #&gt; [0.3041], #&gt; [0.4170], #&gt; [0.6813], #&gt; [0.8755], #&gt; [0.5104], #&gt; [0.6693], #&gt; [0.5859], #&gt; [0.6249], #&gt; [0.6747], #&gt; [0.8423], #&gt; [0.0832], #&gt; [0.7637], #&gt; [0.2437], #&gt; [0.1942], #&gt; [0.5725], #&gt; [0.0957], #&gt; [0.8853], #&gt; [0.6272], #&gt; [0.7234], #&gt; [0.0161], #&gt; [0.5944], #&gt; [0.5568], #&gt; [0.1590], #&gt; [0.1531], #&gt; [0.6955], #&gt; [0.3188], #&gt; [0.6920], #&gt; [0.5544], #&gt; [0.3890], #&gt; [0.9251], #&gt; [0.8417], #&gt; [0.3574], #&gt; [0.0436], #&gt; [0.3048], #&gt; [0.3982], #&gt; [0.7050], #&gt; [0.9954], #&gt; [0.3559], #&gt; [0.7625], #&gt; [0.5932], #&gt; [0.6917], #&gt; [0.1511], #&gt; [0.3989], #&gt; [0.2409], #&gt; [0.3435]], dtype=torch.float64) 9.3 Optimizer and Loss Next, you should define the Optimizer and the Loss Function for our training process. # Define Optimizer and Loss Function optimizer = torch.optim.SGD(net.parameters(), lr=0.2) loss_func = torch.nn.MSELoss() print(optimizer) #&gt; SGD ( #&gt; Parameter Group 0 #&gt; dampening: 0 #&gt; lr: 0.2 #&gt; momentum: 0 #&gt; nesterov: False #&gt; weight_decay: 0 #&gt; ) print(loss_func) #&gt; MSELoss() 9.3.1 Equivalent code in R # Define Optimizer and Loss Function optimizer &lt;- torch$optim$SGD(net$parameters(), lr=0.2) loss_func &lt;- torch$nn$MSELoss() print(optimizer) #&gt; SGD ( #&gt; Parameter Group 0 #&gt; dampening: 0 #&gt; lr: 0.2 #&gt; momentum: 0 #&gt; nesterov: False #&gt; weight_decay: 0 #&gt; ) print(loss_func) #&gt; MSELoss() 9.4 Training 9.4.1 Code in Python Now letâ€™s start our training process. With an epoch of 250, you will iterate our data to find the best value for our hyperparameters. inputs = Variable(x) outputs = Variable(y) for i in range(250): prediction = net(inputs) loss = loss_func(prediction, outputs) optimizer.zero_grad() loss.backward() optimizer.step() if i % 10 == 0: # plot and show learning process plt.cla() plt.scatter(x.data.numpy(), y.data.numpy()) plt.plot(x.data.numpy(), prediction.data.numpy(), &#39;r-&#39;, lw=2) plt.text(0.5, 0, &#39;Loss=%.4f&#39; % loss.data.numpy(), fontdict={&#39;size&#39;: 10, &#39;color&#39;: &#39;red&#39;}) plt.pause(0.1) plt.show() 9.4.2 Code in R x = x$type(torch$FloatTensor) # make it a a FloatTensor y = y$type(torch$FloatTensor) inputs = Variable(x) outputs = Variable(y) plot(x$data$numpy(), y$data$numpy(), col = &quot;blue&quot;) for (i in 1:250) { prediction = net(inputs) loss = loss_func(prediction, outputs) optimizer$zero_grad() loss$backward() optimizer$step() if (i %% 10 == 0) { # plot and show learning process # points(x$data$numpy(), y$data$numpy()) points(x$data$numpy(), prediction$data$numpy(), col=&quot;red&quot;) # cat(i, loss$data$numpy(), &quot;\\n&quot;) } } 9.5 Result As you can see below, you successfully performed regression with a neural network. Actually, on every iteration, the red line in the plot will update and change its position to fit the data. But in this picture, you only show you the final result "]
]
