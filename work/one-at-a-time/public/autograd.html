<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Autograd | One at a time</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Autograd | One at a time" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Autograd | One at a time" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Yihui Xie" />


<meta name="date" content="2019-09-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-regression-with-pytorch.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">One at a time</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="part"><span><b>I Image Recognition</b></span></li>
<li class="part"><span><b>II PyTorch and R data structures</b></span></li>
<li class="chapter" data-level="1" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html"><i class="fa fa-check"></i><b>1</b> Working with data.frame</a><ul>
<li class="chapter" data-level="1.1" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#load-pytorch-libraries"><i class="fa fa-check"></i><b>1.1</b> Load PyTorch libraries</a></li>
<li class="chapter" data-level="1.2" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#dataset-iteration-batch-settings"><i class="fa fa-check"></i><b>1.2</b> Dataset iteration batch settings</a></li>
<li class="chapter" data-level="1.3" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#summary-statistics-for-tensors"><i class="fa fa-check"></i><b>1.3</b> Summary statistics for tensors</a></li>
<li class="chapter" data-level="1.4" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#using-data.frame"><i class="fa fa-check"></i><b>1.4</b> using <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="working-with-data-table.html"><a href="working-with-data-table.html"><i class="fa fa-check"></i><b>2</b> Working with data.table</a><ul>
<li class="chapter" data-level="2.1" data-path="working-with-data-table.html"><a href="working-with-data-table.html#load-pytorch-libraries-1"><i class="fa fa-check"></i><b>2.1</b> Load PyTorch libraries</a><ul>
<li class="chapter" data-level="2.1.1" data-path="working-with-data-table.html"><a href="working-with-data-table.html#using-data.table"><i class="fa fa-check"></i><b>2.1.1</b> Using `data.table</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III PyTorch with Rmarkdown</b></span></li>
<li class="chapter" data-level="3" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html"><i class="fa fa-check"></i><b>3</b> Simple Regression with PyTorch</a><ul>
<li class="chapter" data-level="3.1" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#creating-the-network-model"><i class="fa fa-check"></i><b>3.1</b> Creating the network model</a><ul>
<li class="chapter" data-level="3.1.1" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#code-in-r"><i class="fa fa-check"></i><b>3.1.1</b> Code in R</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#datasets"><i class="fa fa-check"></i><b>3.2</b> Datasets</a><ul>
<li class="chapter" data-level="3.2.1" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#code-in-r-1"><i class="fa fa-check"></i><b>3.2.1</b> Code in R</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#optimizer-and-loss"><i class="fa fa-check"></i><b>3.3</b> Optimizer and Loss</a><ul>
<li class="chapter" data-level="3.3.1" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#equivalent-code-in-r"><i class="fa fa-check"></i><b>3.3.1</b> Equivalent code in R</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#training"><i class="fa fa-check"></i><b>3.4</b> Training</a><ul>
<li class="chapter" data-level="3.4.1" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#code-in-python"><i class="fa fa-check"></i><b>3.4.1</b> Code in Python</a></li>
<li class="chapter" data-level="3.4.2" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#code-in-r-2"><i class="fa fa-check"></i><b>3.4.2</b> Code in R</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#result"><i class="fa fa-check"></i><b>3.5</b> Result</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="autograd.html"><a href="autograd.html"><i class="fa fa-check"></i><b>4</b> Autograd</a><ul>
<li class="chapter" data-level="4.1" data-path="autograd.html"><a href="autograd.html#python-code"><i class="fa fa-check"></i><b>4.1</b> Python code</a></li>
<li class="chapter" data-level="4.2" data-path="autograd.html"><a href="autograd.html#r-code"><i class="fa fa-check"></i><b>4.2</b> R code</a></li>
<li class="chapter" data-level="4.3" data-path="autograd.html"><a href="autograd.html#observations"><i class="fa fa-check"></i><b>4.3</b> Observations</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">One at a time</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="autograd" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Autograd</h1>
<p>Source: <a href="https://github.com/jcjohnson/pytorch-examples#pytorch-autograd" class="uri">https://github.com/jcjohnson/pytorch-examples#pytorch-autograd</a></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="kw">library</span>(rTorch)</a></code></pre></div>
<div id="python-code" class="section level2">
<h2><span class="header-section-number">4.1</span> Python code</h2>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb22-1" data-line-number="1"><span class="co"># Do not print from a function. Similar functionality to R invisible()</span></a>
<a class="sourceLine" id="cb22-2" data-line-number="2"><span class="co"># https://stackoverflow.com/a/45669280/5270873</span></a>
<a class="sourceLine" id="cb22-3" data-line-number="3"><span class="im">import</span> os, sys</a>
<a class="sourceLine" id="cb22-4" data-line-number="4"></a>
<a class="sourceLine" id="cb22-5" data-line-number="5"><span class="kw">class</span> HiddenPrints:</a>
<a class="sourceLine" id="cb22-6" data-line-number="6">    <span class="kw">def</span> <span class="fu">__enter__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb22-7" data-line-number="7">        <span class="va">self</span>._original_stdout <span class="op">=</span> sys.stdout</a>
<a class="sourceLine" id="cb22-8" data-line-number="8">        sys.stdout <span class="op">=</span> <span class="bu">open</span>(os.devnull, <span class="st">&#39;w&#39;</span>)</a>
<a class="sourceLine" id="cb22-9" data-line-number="9"></a>
<a class="sourceLine" id="cb22-10" data-line-number="10">    <span class="kw">def</span> <span class="fu">__exit__</span>(<span class="va">self</span>, exc_type, exc_val, exc_tb):</a>
<a class="sourceLine" id="cb22-11" data-line-number="11">        sys.stdout.close()</a>
<a class="sourceLine" id="cb22-12" data-line-number="12">        sys.stdout <span class="op">=</span> <span class="va">self</span>._original_stdout</a></code></pre></div>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb23-1" data-line-number="1"><span class="co"># Code in file autograd/two_layer_net_autograd.py</span></a>
<a class="sourceLine" id="cb23-2" data-line-number="2"><span class="im">import</span> torch</a>
<a class="sourceLine" id="cb23-3" data-line-number="3">device <span class="op">=</span> torch.device(<span class="st">&#39;cpu&#39;</span>)</a>
<a class="sourceLine" id="cb23-4" data-line-number="4"><span class="co"># device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU</span></a>
<a class="sourceLine" id="cb23-5" data-line-number="5"></a>
<a class="sourceLine" id="cb23-6" data-line-number="6">torch.manual_seed(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb23-7" data-line-number="7"></a>
<a class="sourceLine" id="cb23-8" data-line-number="8"><span class="co"># N is batch size; D_in is input dimension;</span></a>
<a class="sourceLine" id="cb23-9" data-line-number="9"><span class="co"># H is hidden dimension; D_out is output dimension.</span></a>
<a class="sourceLine" id="cb23-10" data-line-number="10"><span class="co">#&gt; &lt;torch._C.Generator object at 0x7f4c2787ab90&gt;</span></a>
<a class="sourceLine" id="cb23-11" data-line-number="11">N, D_in, H, D_out <span class="op">=</span> <span class="dv">64</span>, <span class="dv">1000</span>, <span class="dv">100</span>, <span class="dv">10</span></a>
<a class="sourceLine" id="cb23-12" data-line-number="12"></a>
<a class="sourceLine" id="cb23-13" data-line-number="13"><span class="co"># Create random Tensors to hold input and outputs</span></a>
<a class="sourceLine" id="cb23-14" data-line-number="14">x <span class="op">=</span> torch.randn(N, D_in, device<span class="op">=</span>device)</a>
<a class="sourceLine" id="cb23-15" data-line-number="15">y <span class="op">=</span> torch.randn(N, D_out, device<span class="op">=</span>device)</a>
<a class="sourceLine" id="cb23-16" data-line-number="16"></a>
<a class="sourceLine" id="cb23-17" data-line-number="17"><span class="co"># Create random Tensors for weights; setting requires_grad=True means that we</span></a>
<a class="sourceLine" id="cb23-18" data-line-number="18"><span class="co"># want to compute gradients for these Tensors during the backward pass.</span></a>
<a class="sourceLine" id="cb23-19" data-line-number="19">w1 <span class="op">=</span> torch.randn(D_in, H, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb23-20" data-line-number="20">w2 <span class="op">=</span> torch.randn(H, D_out, device<span class="op">=</span>device, requires_grad<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb23-21" data-line-number="21"></a>
<a class="sourceLine" id="cb23-22" data-line-number="22">learning_rate <span class="op">=</span> <span class="fl">1e-6</span></a>
<a class="sourceLine" id="cb23-23" data-line-number="23"></a>
<a class="sourceLine" id="cb23-24" data-line-number="24"><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</a>
<a class="sourceLine" id="cb23-25" data-line-number="25">  <span class="co"># Forward pass: compute predicted y using operations on Tensors. Since w1 and</span></a>
<a class="sourceLine" id="cb23-26" data-line-number="26">  <span class="co"># w2 have requires_grad=True, operations involving these Tensors will cause</span></a>
<a class="sourceLine" id="cb23-27" data-line-number="27">  <span class="co"># PyTorch to build a computational graph, allowing automatic computation of</span></a>
<a class="sourceLine" id="cb23-28" data-line-number="28">  <span class="co"># gradients. Since we are no longer implementing the backward pass by hand we</span></a>
<a class="sourceLine" id="cb23-29" data-line-number="29">  <span class="co"># don&#39;t need to keep references to intermediate values.</span></a>
<a class="sourceLine" id="cb23-30" data-line-number="30">  y_pred <span class="op">=</span> x.mm(w1).clamp(<span class="bu">min</span><span class="op">=</span><span class="dv">0</span>).mm(w2)</a>
<a class="sourceLine" id="cb23-31" data-line-number="31">  </a>
<a class="sourceLine" id="cb23-32" data-line-number="32">  <span class="co"># Compute and print loss. Loss is a Tensor of shape (), and loss.item()</span></a>
<a class="sourceLine" id="cb23-33" data-line-number="33">  <span class="co"># is a Python number giving its value.</span></a>
<a class="sourceLine" id="cb23-34" data-line-number="34">  loss <span class="op">=</span> (y_pred <span class="op">-</span> y).<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>()</a>
<a class="sourceLine" id="cb23-35" data-line-number="35">  <span class="bu">print</span>(t, loss.item())</a>
<a class="sourceLine" id="cb23-36" data-line-number="36"></a>
<a class="sourceLine" id="cb23-37" data-line-number="37">  <span class="co"># Use autograd to compute the backward pass. This call will compute the</span></a>
<a class="sourceLine" id="cb23-38" data-line-number="38">  <span class="co"># gradient of loss with respect to all Tensors with requires_grad=True.</span></a>
<a class="sourceLine" id="cb23-39" data-line-number="39">  <span class="co"># After this call w1.grad and w2.grad will be Tensors holding the gradient</span></a>
<a class="sourceLine" id="cb23-40" data-line-number="40">  <span class="co"># of the loss with respect to w1 and w2 respectively.</span></a>
<a class="sourceLine" id="cb23-41" data-line-number="41">  loss.backward()</a>
<a class="sourceLine" id="cb23-42" data-line-number="42"></a>
<a class="sourceLine" id="cb23-43" data-line-number="43">  <span class="co"># Update weights using gradient descent. For this step we just want to mutate</span></a>
<a class="sourceLine" id="cb23-44" data-line-number="44">  <span class="co"># the values of w1 and w2 in-place; we don&#39;t want to build up a computational</span></a>
<a class="sourceLine" id="cb23-45" data-line-number="45">  <span class="co"># graph for the update steps, so we use the torch.no_grad() context manager</span></a>
<a class="sourceLine" id="cb23-46" data-line-number="46">  <span class="co"># to prevent PyTorch from building a computational graph for the updates</span></a>
<a class="sourceLine" id="cb23-47" data-line-number="47">  <span class="cf">with</span> torch.no_grad():</a>
<a class="sourceLine" id="cb23-48" data-line-number="48">    w1 <span class="op">-=</span> learning_rate <span class="op">*</span> w1.grad</a>
<a class="sourceLine" id="cb23-49" data-line-number="49">    w2 <span class="op">-=</span> learning_rate <span class="op">*</span> w2.grad</a>
<a class="sourceLine" id="cb23-50" data-line-number="50"></a>
<a class="sourceLine" id="cb23-51" data-line-number="51">    <span class="co"># Manually zero the gradients after running the backward pass</span></a>
<a class="sourceLine" id="cb23-52" data-line-number="52">    <span class="cf">with</span> HiddenPrints():   <span class="co"># this would be the equivalent of invisible() in R</span></a>
<a class="sourceLine" id="cb23-53" data-line-number="53">      w1.grad.zero_()</a>
<a class="sourceLine" id="cb23-54" data-line-number="54">      w2.grad.zero_()</a>
<a class="sourceLine" id="cb23-55" data-line-number="55"><span class="co">#&gt; 0 29428666.0</span></a>
<a class="sourceLine" id="cb23-56" data-line-number="56"><span class="co">#&gt; 1 22739450.0</span></a>
<a class="sourceLine" id="cb23-57" data-line-number="57"><span class="co">#&gt; 2 20605262.0</span></a>
<a class="sourceLine" id="cb23-58" data-line-number="58"><span class="co">#&gt; 3 19520376.0</span></a>
<a class="sourceLine" id="cb23-59" data-line-number="59"><span class="co">#&gt; 4 17810228.0</span></a></code></pre></div>
</div>
<div id="r-code" class="section level2">
<h2><span class="header-section-number">4.2</span> R code</h2>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1"><span class="co"># library(reticulate) # originally qwe used reticulate</span></a>
<a class="sourceLine" id="cb24-2" data-line-number="2"><span class="kw">library</span>(rTorch)</a>
<a class="sourceLine" id="cb24-3" data-line-number="3"></a>
<a class="sourceLine" id="cb24-4" data-line-number="4">torch  =<span class="st"> </span><span class="kw">import</span>(<span class="st">&quot;torch&quot;</span>)</a>
<a class="sourceLine" id="cb24-5" data-line-number="5">device =<span class="st"> </span>torch<span class="op">$</span><span class="kw">device</span>(<span class="st">&#39;cpu&#39;</span>)</a>
<a class="sourceLine" id="cb24-6" data-line-number="6"><span class="co"># device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU</span></a>
<a class="sourceLine" id="cb24-7" data-line-number="7"></a>
<a class="sourceLine" id="cb24-8" data-line-number="8">torch<span class="op">$</span><span class="kw">manual_seed</span>(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb24-9" data-line-number="9"><span class="co">#&gt; &lt;torch._C.Generator&gt;</span></a>
<a class="sourceLine" id="cb24-10" data-line-number="10"></a>
<a class="sourceLine" id="cb24-11" data-line-number="11"><span class="co"># N is batch size; D_in is input dimension;</span></a>
<a class="sourceLine" id="cb24-12" data-line-number="12"><span class="co"># H is hidden dimension; D_out is output dimension.</span></a>
<a class="sourceLine" id="cb24-13" data-line-number="13">N &lt;-<span class="st"> </span>64L; D_in &lt;-<span class="st"> </span>1000L; H &lt;-<span class="st"> </span>100L; D_out &lt;-<span class="st"> </span>10L</a>
<a class="sourceLine" id="cb24-14" data-line-number="14"></a>
<a class="sourceLine" id="cb24-15" data-line-number="15"><span class="co"># Create random Tensors to hold inputs and outputs</span></a>
<a class="sourceLine" id="cb24-16" data-line-number="16">x =<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(N, D_in, <span class="dt">device=</span>device)</a>
<a class="sourceLine" id="cb24-17" data-line-number="17">y =<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(N, D_out, <span class="dt">device=</span>device)</a>
<a class="sourceLine" id="cb24-18" data-line-number="18"></a>
<a class="sourceLine" id="cb24-19" data-line-number="19"><span class="co"># Create random Tensors for weights; setting requires_grad=True means that we</span></a>
<a class="sourceLine" id="cb24-20" data-line-number="20"><span class="co"># want to compute gradients for these Tensors during the backward pass.</span></a>
<a class="sourceLine" id="cb24-21" data-line-number="21">w1 =<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(D_in, H, <span class="dt">device=</span>device, <span class="dt">requires_grad=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb24-22" data-line-number="22">w2 =<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(H, D_out, <span class="dt">device=</span>device, <span class="dt">requires_grad=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb24-23" data-line-number="23"></a>
<a class="sourceLine" id="cb24-24" data-line-number="24">learning_rate =<span class="st"> </span>torch<span class="op">$</span><span class="kw">scalar_tensor</span>(<span class="fl">1e-6</span>)</a>
<a class="sourceLine" id="cb24-25" data-line-number="25"></a>
<a class="sourceLine" id="cb24-26" data-line-number="26"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb24-27" data-line-number="27">  <span class="co"># Forward pass: compute predicted y using operations on Tensors. Since w1 and</span></a>
<a class="sourceLine" id="cb24-28" data-line-number="28">  <span class="co"># w2 have requires_grad=True, operations involving these Tensors will cause</span></a>
<a class="sourceLine" id="cb24-29" data-line-number="29">  <span class="co"># PyTorch to build a computational graph, allowing automatic computation of</span></a>
<a class="sourceLine" id="cb24-30" data-line-number="30">  <span class="co"># gradients. Since we are no longer implementing the backward pass by hand we</span></a>
<a class="sourceLine" id="cb24-31" data-line-number="31">  <span class="co"># don&#39;t need to keep references to intermediate values.</span></a>
<a class="sourceLine" id="cb24-32" data-line-number="32">  y_pred =<span class="st"> </span>x<span class="op">$</span><span class="kw">mm</span>(w1)<span class="op">$</span><span class="kw">clamp</span>(<span class="dt">min=</span><span class="dv">0</span>)<span class="op">$</span><span class="kw">mm</span>(w2)</a>
<a class="sourceLine" id="cb24-33" data-line-number="33">  </a>
<a class="sourceLine" id="cb24-34" data-line-number="34">  <span class="co"># Compute and print loss. Loss is a Tensor of shape (), and loss.item()</span></a>
<a class="sourceLine" id="cb24-35" data-line-number="35">  <span class="co"># is a Python number giving its value.</span></a>
<a class="sourceLine" id="cb24-36" data-line-number="36">  loss =<span class="st"> </span>(torch<span class="op">$</span><span class="kw">sub</span>(y_pred, y))<span class="op">$</span><span class="kw">pow</span>(<span class="dv">2</span>)<span class="op">$</span><span class="kw">sum</span>()</a>
<a class="sourceLine" id="cb24-37" data-line-number="37">  <span class="kw">cat</span>(t, <span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>, loss<span class="op">$</span><span class="kw">item</span>(), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</a>
<a class="sourceLine" id="cb24-38" data-line-number="38"></a>
<a class="sourceLine" id="cb24-39" data-line-number="39">  <span class="co"># Use autograd to compute the backward pass. This call will compute the</span></a>
<a class="sourceLine" id="cb24-40" data-line-number="40">  <span class="co"># gradient of loss with respect to all Tensors with requires_grad=True.</span></a>
<a class="sourceLine" id="cb24-41" data-line-number="41">  <span class="co"># After this call w1.grad and w2.grad will be Tensors holding the gradient</span></a>
<a class="sourceLine" id="cb24-42" data-line-number="42">  <span class="co"># of the loss with respect to w1 and w2 respectively.</span></a>
<a class="sourceLine" id="cb24-43" data-line-number="43">  loss<span class="op">$</span><span class="kw">backward</span>()</a>
<a class="sourceLine" id="cb24-44" data-line-number="44"></a>
<a class="sourceLine" id="cb24-45" data-line-number="45">  <span class="co"># Update weights using gradient descent. For this step we just want to mutate</span></a>
<a class="sourceLine" id="cb24-46" data-line-number="46">  <span class="co"># the values of w1 and w2 in-place; we don&#39;t want to build up a computational</span></a>
<a class="sourceLine" id="cb24-47" data-line-number="47">  <span class="co"># graph for the update steps, so we use the torch.no_grad() context manager</span></a>
<a class="sourceLine" id="cb24-48" data-line-number="48">  <span class="co"># to prevent PyTorch from building a computational graph for the updates</span></a>
<a class="sourceLine" id="cb24-49" data-line-number="49">  <span class="kw">with</span>(torch<span class="op">$</span><span class="kw">no_grad</span>(), {</a>
<a class="sourceLine" id="cb24-50" data-line-number="50">    w1<span class="op">$</span>data =<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(w1<span class="op">$</span>data, torch<span class="op">$</span><span class="kw">mul</span>(w1<span class="op">$</span>grad, learning_rate))</a>
<a class="sourceLine" id="cb24-51" data-line-number="51">    w2<span class="op">$</span>data =<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(w2<span class="op">$</span>data, torch<span class="op">$</span><span class="kw">mul</span>(w2<span class="op">$</span>grad, learning_rate))</a>
<a class="sourceLine" id="cb24-52" data-line-number="52"></a>
<a class="sourceLine" id="cb24-53" data-line-number="53">    <span class="co"># Manually zero the gradients after running the backward pass</span></a>
<a class="sourceLine" id="cb24-54" data-line-number="54">    w1<span class="op">$</span>grad<span class="op">$</span><span class="kw">zero_</span>()</a>
<a class="sourceLine" id="cb24-55" data-line-number="55">    w2<span class="op">$</span>grad<span class="op">$</span><span class="kw">zero_</span>()</a>
<a class="sourceLine" id="cb24-56" data-line-number="56">  })</a>
<a class="sourceLine" id="cb24-57" data-line-number="57">}    </a>
<a class="sourceLine" id="cb24-58" data-line-number="58"><span class="co">#&gt; 1     29428666 </span></a>
<a class="sourceLine" id="cb24-59" data-line-number="59"><span class="co">#&gt; 2     22739450 </span></a>
<a class="sourceLine" id="cb24-60" data-line-number="60"><span class="co">#&gt; 3     20605262 </span></a>
<a class="sourceLine" id="cb24-61" data-line-number="61"><span class="co">#&gt; 4     19520376 </span></a>
<a class="sourceLine" id="cb24-62" data-line-number="62"><span class="co">#&gt; 5     17810228</span></a></code></pre></div>
</div>
<div id="observations" class="section level2">
<h2><span class="header-section-number">4.3</span> Observations</h2>
<p>If the seeds worked the same in Python and R, we should see similar results in the output.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-regression-with-pytorch.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["one_at_a_time.pdf", "one_at_a_time.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
