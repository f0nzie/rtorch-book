<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Inference and Validation of MNIST Fashion | One at a time</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Inference and Validation of MNIST Fashion | One at a time" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Inference and Validation of MNIST Fashion | One at a time" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Yihui Xie" />


<meta name="date" content="2019-09-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="autograd.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">One at a time</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a></li>
<li class="part"><span><b>I Image Recognition</b></span></li>
<li class="chapter" data-level="2" data-path="logistic-regression-on-mnist-digits-idx-images-yunjey.html"><a href="logistic-regression-on-mnist-digits-idx-images-yunjey.html"><i class="fa fa-check"></i><b>2</b> Logistic Regression on MNIST digits. IDX images - Yunjey</a><ul>
<li class="chapter" data-level="2.1" data-path="logistic-regression-on-mnist-digits-idx-images-yunjey.html"><a href="logistic-regression-on-mnist-digits-idx-images-yunjey.html#code-in-r"><i class="fa fa-check"></i><b>2.1</b> Code in R</a></li>
<li class="chapter" data-level="2.2" data-path="logistic-regression-on-mnist-digits-idx-images-yunjey.html"><a href="logistic-regression-on-mnist-digits-idx-images-yunjey.html#code-in-python"><i class="fa fa-check"></i><b>2.2</b> Code in Python</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html"><i class="fa fa-check"></i><b>3</b> R: Digits recognition on IDX images - DeepLearningWizard</a><ul>
<li class="chapter" data-level="3.1" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#load-datasets"><i class="fa fa-check"></i><b>3.1</b> Load datasets</a><ul>
<li class="chapter" data-level="3.1.1" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#load-training-dataset"><i class="fa fa-check"></i><b>3.1.1</b> Load training dataset</a></li>
<li class="chapter" data-level="3.1.2" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#introspection"><i class="fa fa-check"></i><b>3.1.2</b> Introspection</a></li>
<li class="chapter" data-level="3.1.3" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#class-and-length-of-train_dataset"><i class="fa fa-check"></i><b>3.1.3</b> Class and length of <code>train_dataset</code></a></li>
<li class="chapter" data-level="3.1.4" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#introspection-training-dataset"><i class="fa fa-check"></i><b>3.1.4</b> Introspection training dataset</a></li>
<li class="chapter" data-level="3.1.5" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#mnist-image-from-training-dataset"><i class="fa fa-check"></i><b>3.1.5</b> MNIST image from training dataset</a></li>
<li class="chapter" data-level="3.1.6" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#plot-a-second-image"><i class="fa fa-check"></i><b>3.1.6</b> Plot a second image</a></li>
<li class="chapter" data-level="3.1.7" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#loading-the-test-dataset"><i class="fa fa-check"></i><b>3.1.7</b> Loading the test dataset</a></li>
<li class="chapter" data-level="3.1.8" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#plot-image-test-dataset"><i class="fa fa-check"></i><b>3.1.8</b> Plot image test dataset</a></li>
<li class="chapter" data-level="3.1.9" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#plot-a-second-test-image"><i class="fa fa-check"></i><b>3.1.9</b> Plot a second test image</a></li>
<li class="chapter" data-level="3.1.10" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#plot-the-last-test-image"><i class="fa fa-check"></i><b>3.1.10</b> Plot the last test image</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#defining-epochs"><i class="fa fa-check"></i><b>3.2</b> Defining epochs</a></li>
<li class="chapter" data-level="3.3" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#create-iterable-objects-training-and-testing-dataset"><i class="fa fa-check"></i><b>3.3</b> Create iterable objects: training and testing dataset</a><ul>
<li class="chapter" data-level="3.3.1" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#check-iteraibility"><i class="fa fa-check"></i><b>3.3.1</b> Check iteraibility</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#building-the-model"><i class="fa fa-check"></i><b>3.4</b> Building the model</a><ul>
<li class="chapter" data-level="3.4.1" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#instantiate-model-class-based-on-input-and-out-dimensions"><i class="fa fa-check"></i><b>3.4.1</b> Instantiate model class based on input and out dimensions</a></li>
<li class="chapter" data-level="3.4.2" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#instantiate-cross-entropy-loss-class"><i class="fa fa-check"></i><b>3.4.2</b> Instantiate Cross Entropy Loss class</a></li>
<li class="chapter" data-level="3.4.3" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#instantiate-optimizer-class"><i class="fa fa-check"></i><b>3.4.3</b> Instantiate Optimizer class</a></li>
<li class="chapter" data-level="3.4.4" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#parameters-introspection"><i class="fa fa-check"></i><b>3.4.4</b> Parameters introspection</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#train-the-model-and-test-per-epoch"><i class="fa fa-check"></i><b>3.5</b> Train the model and test per epoch</a></li>
<li class="chapter" data-level="3.6" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#break-down-accuracy-calculation"><i class="fa fa-check"></i><b>3.6</b> Break down accuracy calculation</a><ul>
<li class="chapter" data-level="3.6.1" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#printing-output-size"><i class="fa fa-check"></i><b>3.6.1</b> Printing output size</a></li>
<li class="chapter" data-level="3.6.2" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#printing-one-output"><i class="fa fa-check"></i><b>3.6.2</b> Printing one output</a></li>
<li class="chapter" data-level="3.6.3" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#printing-prediction-output"><i class="fa fa-check"></i><b>3.6.3</b> Printing prediction output</a></li>
<li class="chapter" data-level="3.6.4" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#print-prediction-value"><i class="fa fa-check"></i><b>3.6.4</b> Print prediction value</a></li>
<li class="chapter" data-level="3.6.5" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#print-prediction-label-and-label-size"><i class="fa fa-check"></i><b>3.6.5</b> Print prediction, label and label size</a></li>
<li class="chapter" data-level="3.6.6" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#print-second-prediction-and-ground-truth"><i class="fa fa-check"></i><b>3.6.6</b> Print second prediction and ground truth</a></li>
<li class="chapter" data-level="3.6.7" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#print-accuracy"><i class="fa fa-check"></i><b>3.6.7</b> Print accuracy</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="r-digits-recognition-on-idx-images-deeplearningwizard.html"><a href="r-digits-recognition-on-idx-images-deeplearningwizard.html#saving-pytorch-model"><i class="fa fa-check"></i><b>3.7</b> Saving PyTorch model</a></li>
</ul></li>
<li class="part"><span><b>II PyTorch and R data structures</b></span></li>
<li class="chapter" data-level="4" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html"><i class="fa fa-check"></i><b>4</b> Working with data.frame</a><ul>
<li class="chapter" data-level="4.1" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#load-pytorch-libraries"><i class="fa fa-check"></i><b>4.1</b> Load PyTorch libraries</a></li>
<li class="chapter" data-level="4.2" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#dataset-iteration-batch-settings"><i class="fa fa-check"></i><b>4.2</b> Dataset iteration batch settings</a></li>
<li class="chapter" data-level="4.3" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#summary-statistics-for-tensors"><i class="fa fa-check"></i><b>4.3</b> Summary statistics for tensors</a></li>
<li class="chapter" data-level="4.4" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#using-data.frame"><i class="fa fa-check"></i><b>4.4</b> using <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="working-with-data-table.html"><a href="working-with-data-table.html"><i class="fa fa-check"></i><b>5</b> Working with data.table</a><ul>
<li class="chapter" data-level="5.1" data-path="working-with-data-table.html"><a href="working-with-data-table.html#load-pytorch-libraries-1"><i class="fa fa-check"></i><b>5.1</b> Load PyTorch libraries</a><ul>
<li class="chapter" data-level="5.1.1" data-path="working-with-data-table.html"><a href="working-with-data-table.html#using-data.table"><i class="fa fa-check"></i><b>5.1.1</b> Using `data.table</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III PyTorch with Rmarkdown</b></span></li>
<li class="chapter" data-level="6" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html"><i class="fa fa-check"></i><b>6</b> Simple Regression with PyTorch</a><ul>
<li class="chapter" data-level="6.1" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#creating-the-network-model"><i class="fa fa-check"></i><b>6.1</b> Creating the network model</a><ul>
<li class="chapter" data-level="6.1.1" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#code-in-r-1"><i class="fa fa-check"></i><b>6.1.1</b> Code in R</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#datasets"><i class="fa fa-check"></i><b>6.2</b> Datasets</a><ul>
<li class="chapter" data-level="6.2.1" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#code-in-r-2"><i class="fa fa-check"></i><b>6.2.1</b> Code in R</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#optimizer-and-loss"><i class="fa fa-check"></i><b>6.3</b> Optimizer and Loss</a><ul>
<li class="chapter" data-level="6.3.1" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#equivalent-code-in-r"><i class="fa fa-check"></i><b>6.3.1</b> Equivalent code in R</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#training"><i class="fa fa-check"></i><b>6.4</b> Training</a><ul>
<li class="chapter" data-level="6.4.1" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#code-in-python-1"><i class="fa fa-check"></i><b>6.4.1</b> Code in Python</a></li>
<li class="chapter" data-level="6.4.2" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#code-in-r-3"><i class="fa fa-check"></i><b>6.4.2</b> Code in R</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="simple-regression-with-pytorch.html"><a href="simple-regression-with-pytorch.html#result"><i class="fa fa-check"></i><b>6.5</b> Result</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="autograd.html"><a href="autograd.html"><i class="fa fa-check"></i><b>7</b> Autograd</a><ul>
<li class="chapter" data-level="7.1" data-path="autograd.html"><a href="autograd.html#python-code"><i class="fa fa-check"></i><b>7.1</b> Python code</a></li>
<li class="chapter" data-level="7.2" data-path="autograd.html"><a href="autograd.html#r-code"><i class="fa fa-check"></i><b>7.2</b> R code</a></li>
<li class="chapter" data-level="7.3" data-path="autograd.html"><a href="autograd.html#observations"><i class="fa fa-check"></i><b>7.3</b> Observations</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="inference-and-validation-of-mnist-fashion.html"><a href="inference-and-validation-of-mnist-fashion.html"><i class="fa fa-check"></i><b>8</b> Inference and Validation of MNIST Fashion</a><ul>
<li class="chapter" data-level="8.1" data-path="inference-and-validation-of-mnist-fashion.html"><a href="inference-and-validation-of-mnist-fashion.html#utility-functions"><i class="fa fa-check"></i><b>8.1</b> Utility functions</a></li>
<li class="chapter" data-level="8.2" data-path="inference-and-validation-of-mnist-fashion.html"><a href="inference-and-validation-of-mnist-fashion.html#load-datasets-1"><i class="fa fa-check"></i><b>8.2</b> Load datasets</a></li>
<li class="chapter" data-level="8.3" data-path="inference-and-validation-of-mnist-fashion.html"><a href="inference-and-validation-of-mnist-fashion.html#model"><i class="fa fa-check"></i><b>8.3</b> Model</a><ul>
<li class="chapter" data-level="8.3.1" data-path="inference-and-validation-of-mnist-fashion.html"><a href="inference-and-validation-of-mnist-fashion.html#instrospection"><i class="fa fa-check"></i><b>8.3.1</b> Instrospection</a></li>
<li class="chapter" data-level="8.3.2" data-path="inference-and-validation-of-mnist-fashion.html"><a href="inference-and-validation-of-mnist-fashion.html#most-likely-class"><i class="fa fa-check"></i><b>8.3.2</b> Most likely class</a></li>
<li class="chapter" data-level="8.3.3" data-path="inference-and-validation-of-mnist-fashion.html"><a href="inference-and-validation-of-mnist-fashion.html#compare-predicted-vs-true-labels"><i class="fa fa-check"></i><b>8.3.3</b> Compare predicted vs true labels</a></li>
<li class="chapter" data-level="8.3.4" data-path="inference-and-validation-of-mnist-fashion.html"><a href="inference-and-validation-of-mnist-fashion.html#untrained-model"><i class="fa fa-check"></i><b>8.3.4</b> Untrained model</a></li>
<li class="chapter" data-level="8.3.5" data-path="inference-and-validation-of-mnist-fashion.html"><a href="inference-and-validation-of-mnist-fashion.html#train-the-model"><i class="fa fa-check"></i><b>8.3.5</b> Train the model</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="inference-and-validation-of-mnist-fashion.html"><a href="inference-and-validation-of-mnist-fashion.html#overfitting"><i class="fa fa-check"></i><b>8.4</b> Overfitting</a><ul>
<li class="chapter" data-level="8.4.1" data-path="inference-and-validation-of-mnist-fashion.html"><a href="inference-and-validation-of-mnist-fashion.html#model-with-dropout"><i class="fa fa-check"></i><b>8.4.1</b> Model with dropout</a></li>
<li class="chapter" data-level="8.4.2" data-path="inference-and-validation-of-mnist-fashion.html"><a href="inference-and-validation-of-mnist-fashion.html#train-the-model-with-dropout"><i class="fa fa-check"></i><b>8.4.2</b> Train the model with dropout</a></li>
<li class="chapter" data-level="8.4.3" data-path="inference-and-validation-of-mnist-fashion.html"><a href="inference-and-validation-of-mnist-fashion.html#inference-on-dropout-model"><i class="fa fa-check"></i><b>8.4.3</b> Inference on dropout model</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="inference-and-validation-of-mnist-fashion.html"><a href="inference-and-validation-of-mnist-fashion.html#whats-next"><i class="fa fa-check"></i><b>8.5</b> What’s next</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">One at a time</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inference-and-validation-of-mnist-fashion" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Inference and Validation of MNIST Fashion</h1>
<p>Source: <a href="https://www.kaggle.com/ysachit/inference-and-validation-ipynb" class="uri">https://www.kaggle.com/ysachit/inference-and-validation-ipynb</a></p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" data-line-number="1"><span class="kw">library</span>(rTorch)</a></code></pre></div>
<p>Now that you have a trained network, you can use it for making predictions. This is typically called inference, a term borrowed from statistics. However, neural networks have a tendency to perform too well on the training data and aren’t able to generalize to data that hasn’t been seen before. This is called overfitting and it impairs inference performance. To test for overfitting while training, we measure the performance on data not in the training set called the validation set. We avoid overfitting through regularization such as dropout while monitoring the validation performance during training.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb88-1" data-line-number="1"><span class="im">import</span> torch</a>
<a class="sourceLine" id="cb88-2" data-line-number="2"><span class="im">import</span> torchvision</a>
<a class="sourceLine" id="cb88-3" data-line-number="3"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb88-4" data-line-number="4"><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</a>
<a class="sourceLine" id="cb88-5" data-line-number="5"><span class="im">import</span> torchvision.datasets <span class="im">as</span> datasets</a>
<a class="sourceLine" id="cb88-6" data-line-number="6"></a>
<a class="sourceLine" id="cb88-7" data-line-number="7"></a>
<a class="sourceLine" id="cb88-8" data-line-number="8">torch.manual_seed(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb88-9" data-line-number="9"><span class="co">#&gt; &lt;torch._C.Generator object at 0x7f7e299e7a50&gt;</span></a></code></pre></div>
<div class="sourceCode" id="cb89"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb89-1" data-line-number="1"><span class="co"># testset = datasets.FashionMNIST(&#39;./mnist_fashion&#39;, download=True, train=False, transform=transforms)</span></a>
<a class="sourceLine" id="cb89-2" data-line-number="2"><span class="cf">pass</span></a></code></pre></div>
<div id="utility-functions" class="section level2">
<h2><span class="header-section-number">8.1</span> Utility functions</h2>
<div class="sourceCode" id="cb90"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb90-1" data-line-number="1"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb90-2" data-line-number="2"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb90-3" data-line-number="3"></a>
<a class="sourceLine" id="cb90-4" data-line-number="4"><span class="kw">def</span> imshow(image, ax<span class="op">=</span><span class="va">None</span>, title<span class="op">=</span><span class="va">None</span>, normalize<span class="op">=</span><span class="va">True</span>):</a>
<a class="sourceLine" id="cb90-5" data-line-number="5">    <span class="co">&quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb90-6" data-line-number="6">    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</a>
<a class="sourceLine" id="cb90-7" data-line-number="7">        fig, ax <span class="op">=</span> plt.subplots()</a>
<a class="sourceLine" id="cb90-8" data-line-number="8">    image <span class="op">=</span> image.numpy().transpose((<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</a>
<a class="sourceLine" id="cb90-9" data-line-number="9"></a>
<a class="sourceLine" id="cb90-10" data-line-number="10">    <span class="cf">if</span> normalize:</a>
<a class="sourceLine" id="cb90-11" data-line-number="11">        mean <span class="op">=</span> np.array([<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>])</a>
<a class="sourceLine" id="cb90-12" data-line-number="12">        std <span class="op">=</span> np.array([<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</a>
<a class="sourceLine" id="cb90-13" data-line-number="13">        image <span class="op">=</span> std <span class="op">*</span> image <span class="op">+</span> mean</a>
<a class="sourceLine" id="cb90-14" data-line-number="14">        image <span class="op">=</span> np.clip(image, <span class="dv">0</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb90-15" data-line-number="15"></a>
<a class="sourceLine" id="cb90-16" data-line-number="16">    ax.imshow(image)</a>
<a class="sourceLine" id="cb90-17" data-line-number="17">    ax.spines[<span class="st">&#39;top&#39;</span>].set_visible(<span class="va">False</span>)</a>
<a class="sourceLine" id="cb90-18" data-line-number="18">    ax.spines[<span class="st">&#39;right&#39;</span>].set_visible(<span class="va">False</span>)</a>
<a class="sourceLine" id="cb90-19" data-line-number="19">    ax.spines[<span class="st">&#39;left&#39;</span>].set_visible(<span class="va">False</span>)</a>
<a class="sourceLine" id="cb90-20" data-line-number="20">    ax.spines[<span class="st">&#39;bottom&#39;</span>].set_visible(<span class="va">False</span>)</a>
<a class="sourceLine" id="cb90-21" data-line-number="21">    ax.tick_params(axis<span class="op">=</span><span class="st">&#39;both&#39;</span>, length<span class="op">=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb90-22" data-line-number="22">    ax.set_xticklabels(<span class="st">&#39;&#39;</span>)</a>
<a class="sourceLine" id="cb90-23" data-line-number="23">    ax.set_yticklabels(<span class="st">&#39;&#39;</span>)</a>
<a class="sourceLine" id="cb90-24" data-line-number="24"></a>
<a class="sourceLine" id="cb90-25" data-line-number="25">    <span class="cf">return</span> ax</a>
<a class="sourceLine" id="cb90-26" data-line-number="26"></a>
<a class="sourceLine" id="cb90-27" data-line-number="27"><span class="kw">def</span> view_classify(img, ps, version<span class="op">=</span><span class="st">&quot;MNIST&quot;</span>):</a>
<a class="sourceLine" id="cb90-28" data-line-number="28">    <span class="co">&#39;&#39;&#39; Function for viewing an image and it&#39;s predicted classes.</span></a>
<a class="sourceLine" id="cb90-29" data-line-number="29"><span class="co">    &#39;&#39;&#39;</span></a>
<a class="sourceLine" id="cb90-30" data-line-number="30">    ps <span class="op">=</span> ps.data.numpy().squeeze()</a>
<a class="sourceLine" id="cb90-31" data-line-number="31"></a>
<a class="sourceLine" id="cb90-32" data-line-number="32">    fig, (ax1, ax2) <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">9</span>), ncols<span class="op">=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb90-33" data-line-number="33">    ax1.imshow(img.resize_(<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>).numpy().squeeze())</a>
<a class="sourceLine" id="cb90-34" data-line-number="34">    ax1.axis(<span class="st">&#39;off&#39;</span>)</a>
<a class="sourceLine" id="cb90-35" data-line-number="35">    ax2.barh(np.arange(<span class="dv">10</span>), ps)</a>
<a class="sourceLine" id="cb90-36" data-line-number="36">    ax2.set_aspect(<span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb90-37" data-line-number="37">    ax2.set_yticks(np.arange(<span class="dv">10</span>))</a>
<a class="sourceLine" id="cb90-38" data-line-number="38">    <span class="cf">if</span> version <span class="op">==</span> <span class="st">&quot;MNIST&quot;</span>:</a>
<a class="sourceLine" id="cb90-39" data-line-number="39">        ax2.set_yticklabels(np.arange(<span class="dv">10</span>))</a>
<a class="sourceLine" id="cb90-40" data-line-number="40">    <span class="cf">elif</span> version <span class="op">==</span> <span class="st">&quot;Fashion&quot;</span>:</a>
<a class="sourceLine" id="cb90-41" data-line-number="41">        ax2.set_yticklabels([<span class="st">&#39;T-shirt/top&#39;</span>,</a>
<a class="sourceLine" id="cb90-42" data-line-number="42">                            <span class="st">&#39;Trouser&#39;</span>,</a>
<a class="sourceLine" id="cb90-43" data-line-number="43">                            <span class="st">&#39;Pullover&#39;</span>,</a>
<a class="sourceLine" id="cb90-44" data-line-number="44">                            <span class="st">&#39;Dress&#39;</span>,</a>
<a class="sourceLine" id="cb90-45" data-line-number="45">                            <span class="st">&#39;Coat&#39;</span>,</a>
<a class="sourceLine" id="cb90-46" data-line-number="46">                            <span class="st">&#39;Sandal&#39;</span>,</a>
<a class="sourceLine" id="cb90-47" data-line-number="47">                            <span class="st">&#39;Shirt&#39;</span>,</a>
<a class="sourceLine" id="cb90-48" data-line-number="48">                            <span class="st">&#39;Sneaker&#39;</span>,</a>
<a class="sourceLine" id="cb90-49" data-line-number="49">                            <span class="st">&#39;Bag&#39;</span>,</a>
<a class="sourceLine" id="cb90-50" data-line-number="50">                            <span class="st">&#39;Ankle Boot&#39;</span>], size<span class="op">=</span><span class="st">&#39;small&#39;</span>)<span class="op">;</span></a>
<a class="sourceLine" id="cb90-51" data-line-number="51">    ax2.set_title(<span class="st">&#39;Class Probability&#39;</span>)</a>
<a class="sourceLine" id="cb90-52" data-line-number="52">    ax2.set_xlim(<span class="dv">0</span>, <span class="fl">1.1</span>)</a>
<a class="sourceLine" id="cb90-53" data-line-number="53"></a>
<a class="sourceLine" id="cb90-54" data-line-number="54">    plt.tight_layout()</a>
<a class="sourceLine" id="cb90-55" data-line-number="55">    plt.show()</a></code></pre></div>
</div>
<div id="load-datasets-1" class="section level2">
<h2><span class="header-section-number">8.2</span> Load datasets</h2>
<p>As usual, let’s start by loading the dataset through <code>torchvision</code>.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb91-1" data-line-number="1"><span class="im">import</span> torch</a>
<a class="sourceLine" id="cb91-2" data-line-number="2"><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms</a>
<a class="sourceLine" id="cb91-3" data-line-number="3"></a>
<a class="sourceLine" id="cb91-4" data-line-number="4"><span class="co"># Define a transform to normalize the data</span></a>
<a class="sourceLine" id="cb91-5" data-line-number="5">transform <span class="op">=</span> transforms.Compose([transforms.ToTensor(),</a>
<a class="sourceLine" id="cb91-6" data-line-number="6">                                transforms.Normalize([<span class="fl">0.5</span>], [<span class="fl">0.5</span>])])</a>
<a class="sourceLine" id="cb91-7" data-line-number="7">                                </a>
<a class="sourceLine" id="cb91-8" data-line-number="8"><span class="co"># Download and load the training data</span></a>
<a class="sourceLine" id="cb91-9" data-line-number="9">trainset <span class="op">=</span> datasets.FashionMNIST(<span class="st">&#39;mnist_fashion&#39;</span>, download<span class="op">=</span><span class="va">True</span>, train<span class="op">=</span><span class="va">True</span>,  </a>
<a class="sourceLine" id="cb91-10" data-line-number="10">        transform<span class="op">=</span>transform)</a>
<a class="sourceLine" id="cb91-11" data-line-number="11">train_loader <span class="op">=</span> torch.utils.data.DataLoader(trainset, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb91-12" data-line-number="12"></a>
<a class="sourceLine" id="cb91-13" data-line-number="13"><span class="co"># Download and load the test data</span></a>
<a class="sourceLine" id="cb91-14" data-line-number="14">testset <span class="op">=</span> datasets.FashionMNIST(<span class="st">&#39;mnist_fashion&#39;</span>, download<span class="op">=</span><span class="va">True</span>, train<span class="op">=</span><span class="va">False</span>,</a>
<a class="sourceLine" id="cb91-15" data-line-number="15">    transform<span class="op">=</span>transform)</a>
<a class="sourceLine" id="cb91-16" data-line-number="16">test_loader <span class="op">=</span> torch.utils.data.DataLoader(testset, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">True</span>)</a></code></pre></div>
</div>
<div id="model" class="section level2">
<h2><span class="header-section-number">8.3</span> Model</h2>
<div class="sourceCode" id="cb92"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb92-1" data-line-number="1"><span class="im">from</span> torch <span class="im">import</span> nn, optim</a>
<a class="sourceLine" id="cb92-2" data-line-number="2"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</a>
<a class="sourceLine" id="cb92-3" data-line-number="3"></a>
<a class="sourceLine" id="cb92-4" data-line-number="4"><span class="kw">class</span> Classifier(nn.Module):</a>
<a class="sourceLine" id="cb92-5" data-line-number="5">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb92-6" data-line-number="6">        <span class="bu">super</span>().<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb92-7" data-line-number="7">        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">784</span>, <span class="dv">256</span>)</a>
<a class="sourceLine" id="cb92-8" data-line-number="8">        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>)</a>
<a class="sourceLine" id="cb92-9" data-line-number="9">        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>)</a>
<a class="sourceLine" id="cb92-10" data-line-number="10">        <span class="va">self</span>.fc4 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb92-11" data-line-number="11">        </a>
<a class="sourceLine" id="cb92-12" data-line-number="12">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb92-13" data-line-number="13">        <span class="co"># make sure input tensor is flattened</span></a>
<a class="sourceLine" id="cb92-14" data-line-number="14">        x <span class="op">=</span> x.view(x.shape[<span class="dv">0</span>], <span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb92-15" data-line-number="15">        </a>
<a class="sourceLine" id="cb92-16" data-line-number="16">        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</a>
<a class="sourceLine" id="cb92-17" data-line-number="17">        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</a>
<a class="sourceLine" id="cb92-18" data-line-number="18">        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc3(x))</a>
<a class="sourceLine" id="cb92-19" data-line-number="19">        x <span class="op">=</span> F.log_softmax(<span class="va">self</span>.fc4(x), dim<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb92-20" data-line-number="20">        </a>
<a class="sourceLine" id="cb92-21" data-line-number="21">        <span class="cf">return</span> x</a>
<a class="sourceLine" id="cb92-22" data-line-number="22"></a>
<a class="sourceLine" id="cb92-23" data-line-number="23">model <span class="op">=</span> Classifier()        </a></code></pre></div>
<div id="instrospection" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Instrospection</h3>
<div class="sourceCode" id="cb93"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb93-1" data-line-number="1"><span class="co"># number of elements in test</span></a>
<a class="sourceLine" id="cb93-2" data-line-number="2"><span class="bu">len</span>(test_loader)    <span class="co"># len: 157</span></a>
<a class="sourceLine" id="cb93-3" data-line-number="3"><span class="co">#&gt; 157</span></a>
<a class="sourceLine" id="cb93-4" data-line-number="4">test_examples <span class="op">=</span> <span class="bu">enumerate</span>(test_loader)</a>
<a class="sourceLine" id="cb93-5" data-line-number="5">test_examples</a>
<a class="sourceLine" id="cb93-6" data-line-number="6"><span class="co"># &lt;enumerate object at 0x7f793ce486c0&gt;</span></a>
<a class="sourceLine" id="cb93-7" data-line-number="7"></a>
<a class="sourceLine" id="cb93-8" data-line-number="8"><span class="co"># iterate through dataset</span></a>
<a class="sourceLine" id="cb93-9" data-line-number="9"><span class="co">#&gt; &lt;enumerate object at 0x7f7e23d4f828&gt;</span></a>
<a class="sourceLine" id="cb93-10" data-line-number="10">batch_idx, (img, label) <span class="op">=</span> <span class="bu">next</span>(test_examples)</a>
<a class="sourceLine" id="cb93-11" data-line-number="11"></a>
<a class="sourceLine" id="cb93-12" data-line-number="12">batch_idx</a>
<a class="sourceLine" id="cb93-13" data-line-number="13"><span class="co">#&gt; 0</span></a>
<a class="sourceLine" id="cb93-14" data-line-number="14">img.shape     <span class="co"># torch.Size([64, 1, 28, 28])</span></a>
<a class="sourceLine" id="cb93-15" data-line-number="15"><span class="co">#&gt; torch.Size([64, 1, 28, 28])</span></a>
<a class="sourceLine" id="cb93-16" data-line-number="16">label.shape   <span class="co"># torch.Size([64])</span></a>
<a class="sourceLine" id="cb93-17" data-line-number="17"><span class="co">#&gt; torch.Size([64])</span></a>
<a class="sourceLine" id="cb93-18" data-line-number="18">images, labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(test_loader))</a>
<a class="sourceLine" id="cb93-19" data-line-number="19"></a>
<a class="sourceLine" id="cb93-20" data-line-number="20"><span class="co"># Get the class probabilities</span></a>
<a class="sourceLine" id="cb93-21" data-line-number="21">ps <span class="op">=</span> torch.exp(model(images))</a>
<a class="sourceLine" id="cb93-22" data-line-number="22"></a>
<a class="sourceLine" id="cb93-23" data-line-number="23"><span class="co"># Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples</span></a>
<a class="sourceLine" id="cb93-24" data-line-number="24"><span class="bu">print</span>(ps.shape)</a>
<a class="sourceLine" id="cb93-25" data-line-number="25"><span class="co"># torch.Size([64, 10])</span></a>
<a class="sourceLine" id="cb93-26" data-line-number="26"><span class="co">#&gt; torch.Size([64, 10])</span></a></code></pre></div>
</div>
<div id="most-likely-class" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Most likely class</h3>
<p>With the probabilities, we can get the most likely class using the <code>ps.topk</code> method. This returns the <code>k</code> highest values. Since we just want the most likely class, we can use <code>ps.topk(1)</code>. This returns a tuple of the <code>top-k</code> values and the <code>top-k</code> indices. If the highest value is the fifth element, we’ll get back 4 as the index.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb94-1" data-line-number="1">top_p, top_class <span class="op">=</span> ps.topk(<span class="dv">1</span>, dim<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb94-2" data-line-number="2"></a>
<a class="sourceLine" id="cb94-3" data-line-number="3">top_p.shape</a>
<a class="sourceLine" id="cb94-4" data-line-number="4"><span class="co">#&gt; torch.Size([64, 1])</span></a>
<a class="sourceLine" id="cb94-5" data-line-number="5">top_class.shape</a>
<a class="sourceLine" id="cb94-6" data-line-number="6"><span class="co"># Look at the most likely classes for the first 10 examples</span></a>
<a class="sourceLine" id="cb94-7" data-line-number="7"><span class="co">#&gt; torch.Size([64, 1])</span></a>
<a class="sourceLine" id="cb94-8" data-line-number="8"><span class="bu">print</span>(top_class[:<span class="dv">10</span>,:])</a>
<a class="sourceLine" id="cb94-9" data-line-number="9"><span class="co">#&gt; tensor([[9],</span></a>
<a class="sourceLine" id="cb94-10" data-line-number="10"><span class="co">#&gt;         [9],</span></a>
<a class="sourceLine" id="cb94-11" data-line-number="11"><span class="co">#&gt;         [4],</span></a>
<a class="sourceLine" id="cb94-12" data-line-number="12"><span class="co">#&gt;         [4],</span></a>
<a class="sourceLine" id="cb94-13" data-line-number="13"><span class="co">#&gt;         [9],</span></a>
<a class="sourceLine" id="cb94-14" data-line-number="14"><span class="co">#&gt;         [4],</span></a>
<a class="sourceLine" id="cb94-15" data-line-number="15"><span class="co">#&gt;         [4],</span></a>
<a class="sourceLine" id="cb94-16" data-line-number="16"><span class="co">#&gt;         [9],</span></a>
<a class="sourceLine" id="cb94-17" data-line-number="17"><span class="co">#&gt;         [9],</span></a>
<a class="sourceLine" id="cb94-18" data-line-number="18"><span class="co">#&gt;         [9]])</span></a></code></pre></div>
</div>
<div id="compare-predicted-vs-true-labels" class="section level3">
<h3><span class="header-section-number">8.3.3</span> Compare predicted vs true labels</h3>
<p>Now we can check if the predicted classes match the labels. This is simple to do by equating top_class and labels, but we have to be careful of the shapes. Here top_class is a 2D tensor with shape (64, 1) while labels is 1D with shape (64). To get the equality to work out the way we want, top_class and labels must have the same shape.</p>
<p><code>equals</code> will have shape (64, 64), try it yourself. What it’s doing is comparing the one element in each row of top_class with each element in labels which returns 64 True/False boolean values for each row.</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb95-1" data-line-number="1">top_class.shape</a>
<a class="sourceLine" id="cb95-2" data-line-number="2"><span class="co">#&gt; torch.Size([64, 1])</span></a>
<a class="sourceLine" id="cb95-3" data-line-number="3">labels.shape</a>
<a class="sourceLine" id="cb95-4" data-line-number="4"><span class="co">#&gt; torch.Size([64])</span></a></code></pre></div>
<div class="sourceCode" id="cb96"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb96-1" data-line-number="1">top_class <span class="op">==</span> labels  <span class="co"># tensor of 1,0</span></a>
<a class="sourceLine" id="cb96-2" data-line-number="2"><span class="co">#&gt; tensor([[0, 0, 0,  ..., 0, 0, 0],</span></a>
<a class="sourceLine" id="cb96-3" data-line-number="3"><span class="co">#&gt;         [0, 0, 0,  ..., 0, 0, 0],</span></a>
<a class="sourceLine" id="cb96-4" data-line-number="4"><span class="co">#&gt;         [0, 0, 0,  ..., 0, 0, 0],</span></a>
<a class="sourceLine" id="cb96-5" data-line-number="5"><span class="co">#&gt;         ...,</span></a>
<a class="sourceLine" id="cb96-6" data-line-number="6"><span class="co">#&gt;         [0, 0, 0,  ..., 0, 0, 0],</span></a>
<a class="sourceLine" id="cb96-7" data-line-number="7"><span class="co">#&gt;         [0, 0, 0,  ..., 0, 0, 0],</span></a>
<a class="sourceLine" id="cb96-8" data-line-number="8"><span class="co">#&gt;         [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)</span></a></code></pre></div>
<div class="sourceCode" id="cb97"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb97-1" data-line-number="1">equals <span class="op">=</span> top_class <span class="op">==</span> labels</a>
<a class="sourceLine" id="cb97-2" data-line-number="2">equals.shape</a>
<a class="sourceLine" id="cb97-3" data-line-number="3"><span class="co"># torch.Size([64, 64])</span></a>
<a class="sourceLine" id="cb97-4" data-line-number="4"><span class="co">#&gt; torch.Size([64, 64])</span></a></code></pre></div>
<div class="sourceCode" id="cb98"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb98-1" data-line-number="1">equals <span class="op">=</span> top_class <span class="op">==</span> labels.view(<span class="op">*</span>top_class.shape)</a>
<a class="sourceLine" id="cb98-2" data-line-number="2">equals</a>
<a class="sourceLine" id="cb98-3" data-line-number="3"><span class="co">#&gt; tensor([[0],</span></a>
<a class="sourceLine" id="cb98-4" data-line-number="4"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-5" data-line-number="5"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-6" data-line-number="6"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-7" data-line-number="7"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-8" data-line-number="8"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-9" data-line-number="9"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-10" data-line-number="10"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-11" data-line-number="11"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-12" data-line-number="12"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-13" data-line-number="13"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-14" data-line-number="14"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-15" data-line-number="15"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-16" data-line-number="16"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-17" data-line-number="17"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-18" data-line-number="18"><span class="co">#&gt;         [1],</span></a>
<a class="sourceLine" id="cb98-19" data-line-number="19"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-20" data-line-number="20"><span class="co">#&gt;         [1],</span></a>
<a class="sourceLine" id="cb98-21" data-line-number="21"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-22" data-line-number="22"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-23" data-line-number="23"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-24" data-line-number="24"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-25" data-line-number="25"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-26" data-line-number="26"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-27" data-line-number="27"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-28" data-line-number="28"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-29" data-line-number="29"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-30" data-line-number="30"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-31" data-line-number="31"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-32" data-line-number="32"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-33" data-line-number="33"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-34" data-line-number="34"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-35" data-line-number="35"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-36" data-line-number="36"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-37" data-line-number="37"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-38" data-line-number="38"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-39" data-line-number="39"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-40" data-line-number="40"><span class="co">#&gt;         [1],</span></a>
<a class="sourceLine" id="cb98-41" data-line-number="41"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-42" data-line-number="42"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-43" data-line-number="43"><span class="co">#&gt;         [1],</span></a>
<a class="sourceLine" id="cb98-44" data-line-number="44"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-45" data-line-number="45"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-46" data-line-number="46"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-47" data-line-number="47"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-48" data-line-number="48"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-49" data-line-number="49"><span class="co">#&gt;         [1],</span></a>
<a class="sourceLine" id="cb98-50" data-line-number="50"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-51" data-line-number="51"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-52" data-line-number="52"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-53" data-line-number="53"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-54" data-line-number="54"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-55" data-line-number="55"><span class="co">#&gt;         [1],</span></a>
<a class="sourceLine" id="cb98-56" data-line-number="56"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-57" data-line-number="57"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-58" data-line-number="58"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-59" data-line-number="59"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-60" data-line-number="60"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-61" data-line-number="61"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-62" data-line-number="62"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-63" data-line-number="63"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-64" data-line-number="64"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-65" data-line-number="65"><span class="co">#&gt;         [0],</span></a>
<a class="sourceLine" id="cb98-66" data-line-number="66"><span class="co">#&gt;         [0]], dtype=torch.uint8)</span></a></code></pre></div>
</div>
<div id="untrained-model" class="section level3">
<h3><span class="header-section-number">8.3.4</span> Untrained model</h3>
<p>Now we need to calculate the percentage of correct predictions. equals has binary values, either 0 or 1. This means that if we just sum up all the values and divide by the number of values, we get the percentage of correct predictions.</p>
<p>we’ll need to convert equals to a float tensor. Note that when we take torch.mean it returns a scalar tensor, to get the actual value as a float we’ll need to do accuracy.item().</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb99-1" data-line-number="1">accuracy <span class="op">=</span> torch.mean(equals.<span class="bu">type</span>(torch.FloatTensor))</a>
<a class="sourceLine" id="cb99-2" data-line-number="2"><span class="bu">print</span>(<span class="ss">f&#39;Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">.</span>item()<span class="op">*</span><span class="dv">100</span><span class="sc">}</span><span class="ss">%&#39;</span>)</a>
<a class="sourceLine" id="cb99-3" data-line-number="3"><span class="co">#&gt; Accuracy: 9.375%</span></a></code></pre></div>
<p>The network is untrained so it’s making random guesses and we should see an accuracy around 10%.</p>
</div>
<div id="train-the-model" class="section level3">
<h3><span class="header-section-number">8.3.5</span> Train the model</h3>
<p>Now let’s train our network and include our validation pass so we can measure how well the network is performing on the test set. Since we’re not updating our parameters in the validation pass, we can speed up our code by turning off gradients using <code>torch.no_grad()</code>:</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb100-1" data-line-number="1"><span class="im">import</span> sys</a>
<a class="sourceLine" id="cb100-2" data-line-number="2"></a>
<a class="sourceLine" id="cb100-3" data-line-number="3">model <span class="op">=</span> Classifier()</a>
<a class="sourceLine" id="cb100-4" data-line-number="4">criterion <span class="op">=</span> nn.NLLLoss()</a>
<a class="sourceLine" id="cb100-5" data-line-number="5">optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.003</span>)</a>
<a class="sourceLine" id="cb100-6" data-line-number="6"></a>
<a class="sourceLine" id="cb100-7" data-line-number="7">epochs <span class="op">=</span> <span class="dv">5</span></a>
<a class="sourceLine" id="cb100-8" data-line-number="8">steps <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb100-9" data-line-number="9"></a>
<a class="sourceLine" id="cb100-10" data-line-number="10">train_losses, test_losses <span class="op">=</span> [], []</a>
<a class="sourceLine" id="cb100-11" data-line-number="11"><span class="cf">for</span> e <span class="kw">in</span> <span class="bu">range</span>(epochs):</a>
<a class="sourceLine" id="cb100-12" data-line-number="12">    running_loss <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb100-13" data-line-number="13">    <span class="cf">for</span> images, labels <span class="kw">in</span> train_loader:</a>
<a class="sourceLine" id="cb100-14" data-line-number="14">        </a>
<a class="sourceLine" id="cb100-15" data-line-number="15">        optimizer.zero_grad()</a>
<a class="sourceLine" id="cb100-16" data-line-number="16">        </a>
<a class="sourceLine" id="cb100-17" data-line-number="17">        log_ps <span class="op">=</span> model(images)</a>
<a class="sourceLine" id="cb100-18" data-line-number="18">        loss <span class="op">=</span> criterion(log_ps, labels)</a>
<a class="sourceLine" id="cb100-19" data-line-number="19">        loss.backward()</a>
<a class="sourceLine" id="cb100-20" data-line-number="20">        optimizer.step()</a>
<a class="sourceLine" id="cb100-21" data-line-number="21">        </a>
<a class="sourceLine" id="cb100-22" data-line-number="22">        running_loss <span class="op">+=</span> loss.item()</a>
<a class="sourceLine" id="cb100-23" data-line-number="23">        </a>
<a class="sourceLine" id="cb100-24" data-line-number="24">    <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb100-25" data-line-number="25">        test_loss <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb100-26" data-line-number="26">        accuracy <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb100-27" data-line-number="27">        <span class="cf">with</span> torch.no_grad():</a>
<a class="sourceLine" id="cb100-28" data-line-number="28">            images , labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(test_loader))</a>
<a class="sourceLine" id="cb100-29" data-line-number="29">            <span class="cf">for</span> images,labels <span class="kw">in</span> test_loader:</a>
<a class="sourceLine" id="cb100-30" data-line-number="30">                output <span class="op">=</span> model(images)</a>
<a class="sourceLine" id="cb100-31" data-line-number="31">                test_loss <span class="op">+=</span> criterion(output,labels)</a>
<a class="sourceLine" id="cb100-32" data-line-number="32">                </a>
<a class="sourceLine" id="cb100-33" data-line-number="33">                ps <span class="op">=</span> torch.exp(model(images))</a>
<a class="sourceLine" id="cb100-34" data-line-number="34">                top_p, top_class <span class="op">=</span> ps.topk(<span class="dv">1</span>, dim<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb100-35" data-line-number="35">                equals <span class="op">=</span> top_class <span class="op">==</span> labels.view(<span class="op">*</span>top_class.shape)</a>
<a class="sourceLine" id="cb100-36" data-line-number="36">                accuracy <span class="op">+=</span> torch.mean(equals.<span class="bu">type</span>(torch.FloatTensor))</a>
<a class="sourceLine" id="cb100-37" data-line-number="37">                <span class="co"># Look at the most likely classes for the first 10 examples</span></a>
<a class="sourceLine" id="cb100-38" data-line-number="38">                </a>
<a class="sourceLine" id="cb100-39" data-line-number="39">        train_losses.append(running_loss<span class="op">/</span><span class="bu">len</span>(train_loader))</a>
<a class="sourceLine" id="cb100-40" data-line-number="40">        test_losses.append(test_loss<span class="op">/</span><span class="bu">len</span>(test_loader))</a>
<a class="sourceLine" id="cb100-41" data-line-number="41">        <span class="bu">print</span>(<span class="st">&quot;Epoch: </span><span class="sc">{}</span><span class="st">/</span><span class="sc">{}</span><span class="st">.. &quot;</span>.<span class="bu">format</span>(e<span class="op">+</span><span class="dv">1</span>, epochs),</a>
<a class="sourceLine" id="cb100-42" data-line-number="42">              <span class="st">&quot;Training Loss: </span><span class="sc">{:.3f}</span><span class="st">.. &quot;</span>.<span class="bu">format</span>(running_loss<span class="op">/</span><span class="bu">len</span>(train_loader)),</a>
<a class="sourceLine" id="cb100-43" data-line-number="43">              <span class="st">&quot;Test Loss: </span><span class="sc">{:.3f}</span><span class="st">.. &quot;</span>.<span class="bu">format</span>(test_loss<span class="op">/</span><span class="bu">len</span>(test_loader)),</a>
<a class="sourceLine" id="cb100-44" data-line-number="44">              <span class="st">&quot;Test Accuracy: </span><span class="sc">{:.3f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(accuracy<span class="op">/</span><span class="bu">len</span>(test_loader)))</a>
<a class="sourceLine" id="cb100-45" data-line-number="45">        sys.stdout.flush()</a>
<a class="sourceLine" id="cb100-46" data-line-number="46"><span class="co">#&gt; Epoch: 1/5..  Training Loss: 0.510..  Test Loss: 0.433..  Test Accuracy: 0.840</span></a>
<a class="sourceLine" id="cb100-47" data-line-number="47"><span class="co">#&gt; Epoch: 2/5..  Training Loss: 0.386..  Test Loss: 0.434..  Test Accuracy: 0.846</span></a>
<a class="sourceLine" id="cb100-48" data-line-number="48"><span class="co">#&gt; Epoch: 3/5..  Training Loss: 0.357..  Test Loss: 0.443..  Test Accuracy: 0.845</span></a>
<a class="sourceLine" id="cb100-49" data-line-number="49"><span class="co">#&gt; Epoch: 4/5..  Training Loss: 0.330..  Test Loss: 0.360..  Test Accuracy: 0.871</span></a>
<a class="sourceLine" id="cb100-50" data-line-number="50"><span class="co">#&gt; Epoch: 5/5..  Training Loss: 0.312..  Test Loss: 0.365..  Test Accuracy: 0.869</span></a></code></pre></div>
</div>
</div>
<div id="overfitting" class="section level2">
<h2><span class="header-section-number">8.4</span> Overfitting</h2>
<p>If we look at the training and validation losses as we train the network, we can see a phenomenon known as overfitting.</p>
<p>The network learns the training set better and better, resulting in lower training losses. However, it starts having problems generalizing to data outside the training set leading to the validation loss increasing. The ultimate goal of any deep learning model is to make predictions on new data, so we should strive to get the lowest validation loss possible. One option is to use the version of the model with the lowest validation loss, here the one around 8-10 training epochs. This strategy is called early-stopping. In practice, you’d save the model frequently as you’re training then later choose the model with the lowest validation loss.</p>
<div id="model-with-dropout" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Model with dropout</h3>
<p>The most common method to reduce overfitting (outside of early-stopping) is <strong>dropout</strong>, where we randomly drop input units. This forces the network to share information between weights, increasing it’s ability to generalize to new data. Adding dropout in PyTorch is straightforward using the <code>nn.Dropout</code> module.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb101-1" data-line-number="1"><span class="kw">class</span> ClassifierDO(nn.Module):</a>
<a class="sourceLine" id="cb101-2" data-line-number="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb101-3" data-line-number="3">        <span class="bu">super</span>().<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb101-4" data-line-number="4">        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">784</span>, <span class="dv">256</span>)</a>
<a class="sourceLine" id="cb101-5" data-line-number="5">        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>)</a>
<a class="sourceLine" id="cb101-6" data-line-number="6">        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">64</span>)</a>
<a class="sourceLine" id="cb101-7" data-line-number="7">        <span class="va">self</span>.fc4 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb101-8" data-line-number="8"></a>
<a class="sourceLine" id="cb101-9" data-line-number="9">        <span class="co"># Dropout module with 0.2 drop probability</span></a>
<a class="sourceLine" id="cb101-10" data-line-number="10">        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(p<span class="op">=</span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb101-11" data-line-number="11"></a>
<a class="sourceLine" id="cb101-12" data-line-number="12">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb101-13" data-line-number="13">        <span class="co"># make sure input tensor is flattened</span></a>
<a class="sourceLine" id="cb101-14" data-line-number="14">        x <span class="op">=</span> x.view(x.shape[<span class="dv">0</span>], <span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb101-15" data-line-number="15"></a>
<a class="sourceLine" id="cb101-16" data-line-number="16">        <span class="co"># Now with dropout</span></a>
<a class="sourceLine" id="cb101-17" data-line-number="17">        x <span class="op">=</span> <span class="va">self</span>.dropout(F.relu(<span class="va">self</span>.fc1(x)))</a>
<a class="sourceLine" id="cb101-18" data-line-number="18">        x <span class="op">=</span> <span class="va">self</span>.dropout(F.relu(<span class="va">self</span>.fc2(x)))</a>
<a class="sourceLine" id="cb101-19" data-line-number="19">        x <span class="op">=</span> <span class="va">self</span>.dropout(F.relu(<span class="va">self</span>.fc3(x)))</a>
<a class="sourceLine" id="cb101-20" data-line-number="20"></a>
<a class="sourceLine" id="cb101-21" data-line-number="21">        <span class="co"># output so no dropout here</span></a>
<a class="sourceLine" id="cb101-22" data-line-number="22">        x <span class="op">=</span> F.log_softmax(<span class="va">self</span>.fc4(x), dim<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb101-23" data-line-number="23"></a>
<a class="sourceLine" id="cb101-24" data-line-number="24">        <span class="cf">return</span> x</a></code></pre></div>
</div>
<div id="train-the-model-with-dropout" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Train the model with dropout</h3>
<p>During training we want to use dropout to prevent overfitting, but during inference we want to use the entire network. So, we need to turn off dropout during validation, testing, and whenever we’re using the network to make predictions. To do this, you use <code>model.eval()</code>. This sets the model to <strong>evaluation mode</strong> where the dropout probability is 0. You can turn dropout back on by setting the model to train mode with <code>model.train()</code>. In general, the pattern for the validation loop will look like this, where you turn off gradients, set the model to evaluation mode, calculate the validation loss and metric, then set the model back to train mode.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb102-1" data-line-number="1">modelDO <span class="op">=</span> ClassifierDO()</a>
<a class="sourceLine" id="cb102-2" data-line-number="2">criterion <span class="op">=</span> nn.NLLLoss()</a>
<a class="sourceLine" id="cb102-3" data-line-number="3">optimizerDO <span class="op">=</span> optim.Adam(modelDO.parameters(), lr<span class="op">=</span><span class="fl">0.003</span>)</a>
<a class="sourceLine" id="cb102-4" data-line-number="4"></a>
<a class="sourceLine" id="cb102-5" data-line-number="5">epochs <span class="op">=</span> <span class="dv">5</span></a>
<a class="sourceLine" id="cb102-6" data-line-number="6">steps <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb102-7" data-line-number="7"></a>
<a class="sourceLine" id="cb102-8" data-line-number="8">train_losses, test_losses <span class="op">=</span> [], []</a>
<a class="sourceLine" id="cb102-9" data-line-number="9"><span class="cf">for</span> e <span class="kw">in</span> <span class="bu">range</span>(epochs):</a>
<a class="sourceLine" id="cb102-10" data-line-number="10">    running_loss <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb102-11" data-line-number="11">    <span class="cf">for</span> images, labels <span class="kw">in</span> train_loader:</a>
<a class="sourceLine" id="cb102-12" data-line-number="12">        </a>
<a class="sourceLine" id="cb102-13" data-line-number="13">        optimizerDO.zero_grad()</a>
<a class="sourceLine" id="cb102-14" data-line-number="14">        </a>
<a class="sourceLine" id="cb102-15" data-line-number="15">        log_ps <span class="op">=</span> modelDO(images)</a>
<a class="sourceLine" id="cb102-16" data-line-number="16">        loss <span class="op">=</span> criterion(log_ps, labels)</a>
<a class="sourceLine" id="cb102-17" data-line-number="17">        loss.backward()</a>
<a class="sourceLine" id="cb102-18" data-line-number="18">        optimizerDO.step()</a>
<a class="sourceLine" id="cb102-19" data-line-number="19">        </a>
<a class="sourceLine" id="cb102-20" data-line-number="20">        running_loss <span class="op">+=</span> loss.item()</a>
<a class="sourceLine" id="cb102-21" data-line-number="21">        </a>
<a class="sourceLine" id="cb102-22" data-line-number="22">    <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb102-23" data-line-number="23">        test_loss <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb102-24" data-line-number="24">        accuracy <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb102-25" data-line-number="25">        <span class="cf">with</span> torch.no_grad():</a>
<a class="sourceLine" id="cb102-26" data-line-number="26">            images , labels <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(test_loader))</a>
<a class="sourceLine" id="cb102-27" data-line-number="27">            <span class="cf">for</span> images,labels <span class="kw">in</span> test_loader:</a>
<a class="sourceLine" id="cb102-28" data-line-number="28">                output <span class="op">=</span> modelDO(images)</a>
<a class="sourceLine" id="cb102-29" data-line-number="29">                test_loss <span class="op">+=</span> criterion(output,labels)</a>
<a class="sourceLine" id="cb102-30" data-line-number="30">                </a>
<a class="sourceLine" id="cb102-31" data-line-number="31">                ps <span class="op">=</span> torch.exp(modelDO(images))</a>
<a class="sourceLine" id="cb102-32" data-line-number="32">                top_p, top_class <span class="op">=</span> ps.topk(<span class="dv">1</span>, dim<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb102-33" data-line-number="33">                equals <span class="op">=</span> top_class <span class="op">==</span> labels.view(<span class="op">*</span>top_class.shape)</a>
<a class="sourceLine" id="cb102-34" data-line-number="34">                accuracy <span class="op">+=</span> torch.mean(equals.<span class="bu">type</span>(torch.FloatTensor))</a>
<a class="sourceLine" id="cb102-35" data-line-number="35">                <span class="co"># Look at the most likely classes for the first 10 examples</span></a>
<a class="sourceLine" id="cb102-36" data-line-number="36">                </a>
<a class="sourceLine" id="cb102-37" data-line-number="37">        train_losses.append(running_loss<span class="op">/</span><span class="bu">len</span>(train_loader))</a>
<a class="sourceLine" id="cb102-38" data-line-number="38">        test_losses.append(test_loss<span class="op">/</span><span class="bu">len</span>(test_loader))</a>
<a class="sourceLine" id="cb102-39" data-line-number="39">        <span class="bu">print</span>(<span class="st">&quot;Epoch: </span><span class="sc">{}</span><span class="st">/</span><span class="sc">{}</span><span class="st">.. &quot;</span>.<span class="bu">format</span>(e<span class="op">+</span><span class="dv">1</span>, epochs),</a>
<a class="sourceLine" id="cb102-40" data-line-number="40">              <span class="st">&quot;Training Loss: </span><span class="sc">{:.3f}</span><span class="st">.. &quot;</span>.<span class="bu">format</span>(running_loss<span class="op">/</span><span class="bu">len</span>(train_loader)),</a>
<a class="sourceLine" id="cb102-41" data-line-number="41">              <span class="st">&quot;Test Loss: </span><span class="sc">{:.3f}</span><span class="st">.. &quot;</span>.<span class="bu">format</span>(test_loss<span class="op">/</span><span class="bu">len</span>(test_loader)),</a>
<a class="sourceLine" id="cb102-42" data-line-number="42">              <span class="st">&quot;Test Accuracy: </span><span class="sc">{:.3f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(accuracy<span class="op">/</span><span class="bu">len</span>(test_loader)))</a>
<a class="sourceLine" id="cb102-43" data-line-number="43">        sys.stdout.flush()</a>
<a class="sourceLine" id="cb102-44" data-line-number="44"><span class="co">#&gt; Epoch: 1/5..  Training Loss: 0.605..  Test Loss: 0.543..  Test Accuracy: 0.809</span></a>
<a class="sourceLine" id="cb102-45" data-line-number="45"><span class="co">#&gt; Epoch: 2/5..  Training Loss: 0.482..  Test Loss: 0.529..  Test Accuracy: 0.816</span></a>
<a class="sourceLine" id="cb102-46" data-line-number="46"><span class="co">#&gt; Epoch: 3/5..  Training Loss: 0.450..  Test Loss: 0.466..  Test Accuracy: 0.836</span></a>
<a class="sourceLine" id="cb102-47" data-line-number="47"><span class="co">#&gt; Epoch: 4/5..  Training Loss: 0.431..  Test Loss: 0.471..  Test Accuracy: 0.837</span></a>
<a class="sourceLine" id="cb102-48" data-line-number="48"><span class="co">#&gt; Epoch: 5/5..  Training Loss: 0.416..  Test Loss: 0.465..  Test Accuracy: 0.832</span></a></code></pre></div>
</div>
<div id="inference-on-dropout-model" class="section level3">
<h3><span class="header-section-number">8.4.3</span> Inference on dropout model</h3>
<p>Now that the model is trained, we can use it for inference. We’ve done this before, but now we need to remember to set the model in inference mode with <code>model.eval()</code>. You’ll also want to turn off autograd with the <code>torch.no_grad()</code> context.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb103-1" data-line-number="1"><span class="co"># Test out your network!</span></a>
<a class="sourceLine" id="cb103-2" data-line-number="2"></a>
<a class="sourceLine" id="cb103-3" data-line-number="3"><span class="co"># Switch to evaluation mode</span></a>
<a class="sourceLine" id="cb103-4" data-line-number="4">modelDO.<span class="bu">eval</span>()</a>
<a class="sourceLine" id="cb103-5" data-line-number="5"></a>
<a class="sourceLine" id="cb103-6" data-line-number="6"><span class="co"># load test dataset and get one image</span></a>
<a class="sourceLine" id="cb103-7" data-line-number="7"><span class="co">#&gt; ClassifierDO(</span></a>
<a class="sourceLine" id="cb103-8" data-line-number="8"><span class="co">#&gt;   (fc1): Linear(in_features=784, out_features=256, bias=True)</span></a>
<a class="sourceLine" id="cb103-9" data-line-number="9"><span class="co">#&gt;   (fc2): Linear(in_features=256, out_features=128, bias=True)</span></a>
<a class="sourceLine" id="cb103-10" data-line-number="10"><span class="co">#&gt;   (fc3): Linear(in_features=128, out_features=64, bias=True)</span></a>
<a class="sourceLine" id="cb103-11" data-line-number="11"><span class="co">#&gt;   (fc4): Linear(in_features=64, out_features=10, bias=True)</span></a>
<a class="sourceLine" id="cb103-12" data-line-number="12"><span class="co">#&gt;   (dropout): Dropout(p=0.2)</span></a>
<a class="sourceLine" id="cb103-13" data-line-number="13"><span class="co">#&gt; )</span></a>
<a class="sourceLine" id="cb103-14" data-line-number="14">dataiter <span class="op">=</span> <span class="bu">iter</span>(test_loader)      <span class="co"># len(test_loader): 157</span></a>
<a class="sourceLine" id="cb103-15" data-line-number="15">images, labels <span class="op">=</span> dataiter.<span class="bu">next</span>()  </a>
<a class="sourceLine" id="cb103-16" data-line-number="16">images.shape       <span class="co"># images.shape: torch.Size([64, 1, 28, 28])</span></a>
<a class="sourceLine" id="cb103-17" data-line-number="17"><span class="co">#&gt; torch.Size([64, 1, 28, 28])</span></a>
<a class="sourceLine" id="cb103-18" data-line-number="18">labels.shape       <span class="co"># labels.shape: torch.Size([64])</span></a>
<a class="sourceLine" id="cb103-19" data-line-number="19"></a>
<a class="sourceLine" id="cb103-20" data-line-number="20"><span class="co"># take first image</span></a>
<a class="sourceLine" id="cb103-21" data-line-number="21"><span class="co">#&gt; torch.Size([64])</span></a>
<a class="sourceLine" id="cb103-22" data-line-number="22">img <span class="op">=</span> images[<span class="dv">0</span>]  <span class="co"># shape:  torch.Size([1, 28, 28])</span></a>
<a class="sourceLine" id="cb103-23" data-line-number="23"></a>
<a class="sourceLine" id="cb103-24" data-line-number="24"><span class="co"># Convert 2D image to 1D vector</span></a>
<a class="sourceLine" id="cb103-25" data-line-number="25">img_pick <span class="op">=</span> img.view(<span class="dv">1</span>, <span class="dv">784</span>)   <span class="co"># shape: torch.Size([1, 784])</span></a>
<a class="sourceLine" id="cb103-26" data-line-number="26">img_pick.shape</a>
<a class="sourceLine" id="cb103-27" data-line-number="27"></a>
<a class="sourceLine" id="cb103-28" data-line-number="28"><span class="co"># Calculate the class probabilities (softmax) for img</span></a>
<a class="sourceLine" id="cb103-29" data-line-number="29"><span class="co">#&gt; torch.Size([1, 784])</span></a>
<a class="sourceLine" id="cb103-30" data-line-number="30"><span class="cf">with</span> torch.no_grad():</a>
<a class="sourceLine" id="cb103-31" data-line-number="31">    output <span class="op">=</span> modelDO.forward(img_pick)</a>
<a class="sourceLine" id="cb103-32" data-line-number="32"></a>
<a class="sourceLine" id="cb103-33" data-line-number="33">ps <span class="op">=</span> torch.exp(output)</a>
<a class="sourceLine" id="cb103-34" data-line-number="34">ps.shape                     <span class="co"># torch.Size([1, 10])</span></a>
<a class="sourceLine" id="cb103-35" data-line-number="35"><span class="co">#&gt; torch.Size([1, 10])</span></a></code></pre></div>
<div class="sourceCode" id="cb104"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb104-1" data-line-number="1"><span class="co"># Plot the image and probabilities</span></a>
<a class="sourceLine" id="cb104-2" data-line-number="2">view_classify(img_pick.view(<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>), ps, version<span class="op">=</span><span class="st">&#39;Fashion&#39;</span>)</a></code></pre></div>
<p><img src="0816-mnist_fashion_files/figure-html/unnamed-chunk-9-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="whats-next" class="section level2">
<h2><span class="header-section-number">8.5</span> What’s next</h2>
<p>In general, you won’t want to train a model everytime you need it. Instead, you’ll train once, save it, then load the model when you want to train more or use if for inference.</p>
<ol style="list-style-type: decimal">
<li>Save</li>
<li>Load</li>
<li>Read image</li>
<li>Find label</li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="autograd.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["one_at_a_time.pdf", "one_at_a_time.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
