\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={One at a time},
            pdfauthor={Yihui Xie},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{One at a time}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Yihui Xie}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-09-20}

\usepackage{booktabs}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{prerequisites}{%
\chapter*{Prerequisites}\label{prerequisites}}
\addcontentsline{toc}{chapter}{Prerequisites}

This is a \emph{sample} book written in \textbf{Markdown}. You can use anything that Pandoc's Markdown supports, e.g., a math equation \(a^2 + b^2 = c^2\).

The \textbf{bookdown} package can be installed from CRAN or Github:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"bookdown"}\NormalTok{)}
\CommentTok{# or the development version}
\CommentTok{# devtools::install_github("rstudio/bookdown")}
\end{Highlighting}
\end{Shaded}

Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading \texttt{\#}.

To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): \url{https://yihui.name/tinytex/}.

\hypertarget{part-image-recognition}{%
\part{Image Recognition}\label{part-image-recognition}}

\hypertarget{logistic-regression-on-mnist-digits.-idx-images---yunjey}{%
\chapter{Logistic Regression on MNIST digits. IDX images - Yunjey}\label{logistic-regression-on-mnist-digits.-idx-images---yunjey}}

\begin{quote}
Restart RStudio before running
\end{quote}

Source: \url{https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/logistic_regression/main.py}

\hypertarget{code-in-r}{%
\section{Code in R}\label{code-in-r}}

The code in R of this example can be found in Chapter \ref{mnistdigits}.

\hypertarget{code-in-python}{%
\section{Code in Python}\label{code-in-python}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rTorch)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ torchvision}
\ImportTok{import}\NormalTok{ torchvision.transforms }\ImportTok{as}\NormalTok{ transforms}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Hyper-parameters }
\NormalTok{input_size }\OperatorTok{=} \DecValTok{784}
\NormalTok{num_classes }\OperatorTok{=} \DecValTok{10}
\NormalTok{num_epochs }\OperatorTok{=} \DecValTok{5}
\NormalTok{batch_size }\OperatorTok{=} \DecValTok{100}
\NormalTok{learning_rate }\OperatorTok{=} \FloatTok{0.001}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# MNIST dataset (images and labels)}
\CommentTok{# IDX format}
\NormalTok{train_dataset }\OperatorTok{=}\NormalTok{ torchvision.datasets.MNIST(root}\OperatorTok{=}\StringTok{'../../data'}\NormalTok{, }
\NormalTok{                                           train}\OperatorTok{=}\VariableTok{True}\NormalTok{, }
\NormalTok{                                           transform}\OperatorTok{=}\NormalTok{transforms.ToTensor(),}
\NormalTok{                                           download}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{test_dataset }\OperatorTok{=}\NormalTok{ torchvision.datasets.MNIST(root}\OperatorTok{=}\StringTok{'../../data'}\NormalTok{, }
\NormalTok{                                          train}\OperatorTok{=}\VariableTok{False}\NormalTok{, }
\NormalTok{                                          transform}\OperatorTok{=}\NormalTok{transforms.ToTensor())}

\CommentTok{# Data loader (input pipeline)}
\NormalTok{train_loader }\OperatorTok{=}\NormalTok{ torch.utils.data.DataLoader(dataset}\OperatorTok{=}\NormalTok{train_dataset, }
\NormalTok{                                           batch_size}\OperatorTok{=}\NormalTok{batch_size, }
\NormalTok{                                           shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{test_loader }\OperatorTok{=}\NormalTok{ torch.utils.data.DataLoader(dataset}\OperatorTok{=}\NormalTok{test_dataset, }
\NormalTok{                                          batch_size}\OperatorTok{=}\NormalTok{batch_size, }
\NormalTok{                                          shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Logistic regression model}
\NormalTok{model }\OperatorTok{=}\NormalTok{ nn.Linear(input_size, num_classes)}

\CommentTok{# Loss and optimizer}
\CommentTok{# nn.CrossEntropyLoss() computes softmax internally}
\NormalTok{criterion }\OperatorTok{=}\NormalTok{ nn.CrossEntropyLoss()  }
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ torch.optim.SGD(model.parameters(), lr}\OperatorTok{=}\NormalTok{learning_rate)  }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Train the model}
\NormalTok{total_step }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(train_loader)}
\ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num_epochs):}
    \ControlFlowTok{for}\NormalTok{ i, (images, labels) }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(train_loader):}
        \CommentTok{# Reshape images to (batch_size, input_size)}
\NormalTok{        images }\OperatorTok{=}\NormalTok{ images.reshape(}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\DecValTok{28}\OperatorTok{*}\DecValTok{28}\NormalTok{)}
        
        \CommentTok{# Forward pass}
\NormalTok{        outputs }\OperatorTok{=}\NormalTok{ model(images)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ criterion(outputs, labels)}
        
        \CommentTok{# Backward and optimize}
\NormalTok{        optimizer.zero_grad()}
\NormalTok{        loss.backward()}
\NormalTok{        optimizer.step()}
        
        \ControlFlowTok{if}\NormalTok{ (i}\OperatorTok{+}\DecValTok{1}\NormalTok{) }\OperatorTok{%} \DecValTok{100} \OperatorTok{==} \DecValTok{0}\NormalTok{:}
            \BuiltInTok{print}\NormalTok{ (}\StringTok{'Epoch [}\SpecialCharTok{\{\}}\StringTok{/}\SpecialCharTok{\{\}}\StringTok{], Step [}\SpecialCharTok{\{\}}\StringTok{/}\SpecialCharTok{\{\}}\StringTok{], Loss: }\SpecialCharTok{\{:.4f\}}\StringTok{'} 
\NormalTok{                   .}\BuiltInTok{format}\NormalTok{(epoch}\OperatorTok{+}\DecValTok{1}\NormalTok{, num_epochs, i}\OperatorTok{+}\DecValTok{1}\NormalTok{, total_step, loss.item()))}
\CommentTok{#> Epoch [1/5], Step [100/600], Loss: 2.2214}
\CommentTok{#> Epoch [1/5], Step [200/600], Loss: 2.1097}
\CommentTok{#> Epoch [1/5], Step [300/600], Loss: 1.9834}
\CommentTok{#> Epoch [1/5], Step [400/600], Loss: 1.9353}
\CommentTok{#> Epoch [1/5], Step [500/600], Loss: 1.8226}
\CommentTok{#> Epoch [1/5], Step [600/600], Loss: 1.7723}
\CommentTok{#> Epoch [2/5], Step [100/600], Loss: 1.7138}
\CommentTok{#> Epoch [2/5], Step [200/600], Loss: 1.6539}
\CommentTok{#> Epoch [2/5], Step [300/600], Loss: 1.6344}
\CommentTok{#> Epoch [2/5], Step [400/600], Loss: 1.6265}
\CommentTok{#> Epoch [2/5], Step [500/600], Loss: 1.5405}
\CommentTok{#> Epoch [2/5], Step [600/600], Loss: 1.4648}
\CommentTok{#> Epoch [3/5], Step [100/600], Loss: 1.3308}
\CommentTok{#> Epoch [3/5], Step [200/600], Loss: 1.3706}
\CommentTok{#> Epoch [3/5], Step [300/600], Loss: 1.3376}
\CommentTok{#> Epoch [3/5], Step [400/600], Loss: 1.3485}
\CommentTok{#> Epoch [3/5], Step [500/600], Loss: 1.2913}
\CommentTok{#> Epoch [3/5], Step [600/600], Loss: 1.3771}
\CommentTok{#> Epoch [4/5], Step [100/600], Loss: 1.2499}
\CommentTok{#> Epoch [4/5], Step [200/600], Loss: 1.1708}
\CommentTok{#> Epoch [4/5], Step [300/600], Loss: 1.2234}
\CommentTok{#> Epoch [4/5], Step [400/600], Loss: 1.1204}
\CommentTok{#> Epoch [4/5], Step [500/600], Loss: 1.1811}
\CommentTok{#> Epoch [4/5], Step [600/600], Loss: 1.1054}
\CommentTok{#> Epoch [5/5], Step [100/600], Loss: 1.0072}
\CommentTok{#> Epoch [5/5], Step [200/600], Loss: 1.0944}
\CommentTok{#> Epoch [5/5], Step [300/600], Loss: 1.0523}
\CommentTok{#> Epoch [5/5], Step [400/600], Loss: 1.0789}
\CommentTok{#> Epoch [5/5], Step [500/600], Loss: 1.0850}
\CommentTok{#> Epoch [5/5], Step [600/600], Loss: 0.9936}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Test the model}
\CommentTok{# In test phase, we don't need to compute gradients (for memory efficiency)}
\ControlFlowTok{with}\NormalTok{ torch.no_grad():}
\NormalTok{    correct }\OperatorTok{=} \DecValTok{0}
\NormalTok{    total }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ images, labels }\KeywordTok{in}\NormalTok{ test_loader:}
\NormalTok{        images }\OperatorTok{=}\NormalTok{ images.reshape(}\OperatorTok{-}\DecValTok{1}\NormalTok{, }\DecValTok{28}\OperatorTok{*}\DecValTok{28}\NormalTok{)}
\NormalTok{        outputs }\OperatorTok{=}\NormalTok{ model(images)}
\NormalTok{        _, predicted }\OperatorTok{=}\NormalTok{ torch.}\BuiltInTok{max}\NormalTok{(outputs.data, }\DecValTok{1}\NormalTok{)}
\NormalTok{        total }\OperatorTok{+=}\NormalTok{ labels.size(}\DecValTok{0}\NormalTok{)}
\NormalTok{        correct }\OperatorTok{+=}\NormalTok{ (predicted }\OperatorTok{==}\NormalTok{ labels).}\BuiltInTok{sum}\NormalTok{()}

    \BuiltInTok{print}\NormalTok{(}\StringTok{'Accuracy of the model on the 10000 test images: }\SpecialCharTok{\{\}}\StringTok{ %'}\NormalTok{.}\BuiltInTok{format}\NormalTok{(}\DecValTok{100} \OperatorTok{*}\NormalTok{ correct }\OperatorTok{/}\NormalTok{ total))}
\CommentTok{#> Accuracy of the model on the 10000 test images: 82 %}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Save the model checkpoint}
\NormalTok{torch.save(model.state_dict(), }\StringTok{'model.ckpt'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{r-digits-recognition-on-idx-images---deeplearningwizard}{%
\chapter{R: Digits recognition on IDX images - DeepLearningWizard}\label{r-digits-recognition-on-idx-images---deeplearningwizard}}

Source: \url{https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_logistic_regression/}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rTorch)}

\NormalTok{torch       <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torch"}\NormalTok{)}
\NormalTok{torchvision <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torchvision"}\NormalTok{)}
\NormalTok{nn          <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torch.nn"}\NormalTok{)}
\NormalTok{transforms  <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torchvision.transforms"}\NormalTok{)}
\NormalTok{dsets       <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torchvision.datasets"}\NormalTok{)}
\NormalTok{builtins    <-}\StringTok{ }\KeywordTok{import_builtins}\NormalTok{()}
\NormalTok{np          <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"numpy"}\NormalTok{)}

\NormalTok{batch_size_train <-}\StringTok{  }\NormalTok{64L}
\end{Highlighting}
\end{Shaded}

\hypertarget{load-datasets}{%
\section{Load datasets}\label{load-datasets}}

\hypertarget{load-training-dataset}{%
\subsection{Load training dataset}\label{load-training-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train_dataset =}\StringTok{ }\NormalTok{dsets}\OperatorTok{$}\KeywordTok{MNIST}\NormalTok{(}\DataTypeTok{root=}\KeywordTok{file.path}\NormalTok{(}\StringTok{"."}\NormalTok{, }\StringTok{'data'}\NormalTok{), }
                            \DataTypeTok{train=}\OtherTok{TRUE}\NormalTok{, }
                            \DataTypeTok{transform=}\NormalTok{transforms}\OperatorTok{$}\KeywordTok{ToTensor}\NormalTok{(),}
                            \DataTypeTok{download=}\OtherTok{TRUE}\NormalTok{)}
                            
\NormalTok{train_dataset}
\CommentTok{#> Dataset MNIST}
\CommentTok{#>     Number of datapoints: 60000}
\CommentTok{#>     Root location: ./data}
\CommentTok{#>     Split: Train}
\NormalTok{builtins}\OperatorTok{$}\KeywordTok{len}\NormalTok{(train_dataset)}
\CommentTok{#> [1] 60000}
\end{Highlighting}
\end{Shaded}

\hypertarget{introspection}{%
\subsection{Introspection}\label{introspection}}

\hypertarget{class-and-length-of-train_dataset}{%
\subsection{\texorpdfstring{Class and length of \texttt{train\_dataset}}{Class and length of train\_dataset}}\label{class-and-length-of-train_dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# R}
\KeywordTok{class}\NormalTok{(train_dataset)}
\CommentTok{#> [1] "torchvision.datasets.mnist.MNIST"         }
\CommentTok{#> [2] "torchvision.datasets.vision.VisionDataset"}
\CommentTok{#> [3] "torch.utils.data.dataset.Dataset"         }
\CommentTok{#> [4] "python.builtin.object"}
\KeywordTok{length}\NormalTok{(train_dataset)}
\CommentTok{#> [1] 2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Python}
\NormalTok{builtins}\OperatorTok{$}\KeywordTok{type}\NormalTok{(train_dataset)}
\CommentTok{#> <class 'torchvision.datasets.mnist.MNIST'>}
\NormalTok{builtins}\OperatorTok{$}\KeywordTok{len}\NormalTok{(train_dataset)}
\CommentTok{#> [1] 60000}
\KeywordTok{py_len}\NormalTok{(train_dataset)}
\CommentTok{#> [1] 60000}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Note that both similar commands produce different results
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(train_dataset)}
\CommentTok{#>  [1] "class_to_idx"     "classes"          "data"            }
\CommentTok{#>  [4] "download"         "extra_repr"       "extract_gzip"    }
\CommentTok{#>  [7] "processed_folder" "raw_folder"       "root"            }
\CommentTok{#> [10] "target_transform" "targets"          "test_data"       }
\CommentTok{#> [13] "test_file"        "test_labels"      "train"           }
\CommentTok{#> [16] "train_data"       "train_labels"     "training_file"   }
\CommentTok{#> [19] "transform"        "transforms"       "urls"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reticulate}\OperatorTok{::}\KeywordTok{py_list_attributes}\NormalTok{(train_dataset)}
\CommentTok{#>  [1] "__add__"                "__class__"             }
\CommentTok{#>  [3] "__delattr__"            "__dict__"              }
\CommentTok{#>  [5] "__dir__"                "__doc__"               }
\CommentTok{#>  [7] "__eq__"                 "__format__"            }
\CommentTok{#>  [9] "__ge__"                 "__getattribute__"      }
\CommentTok{#> [11] "__getitem__"            "__gt__"                }
\CommentTok{#> [13] "__hash__"               "__init__"              }
\CommentTok{#> [15] "__init_subclass__"      "__le__"                }
\CommentTok{#> [17] "__len__"                "__lt__"                }
\CommentTok{#> [19] "__module__"             "__ne__"                }
\CommentTok{#> [21] "__new__"                "__reduce__"            }
\CommentTok{#> [23] "__reduce_ex__"          "__repr__"              }
\CommentTok{#> [25] "__setattr__"            "__sizeof__"            }
\CommentTok{#> [27] "__str__"                "__subclasshook__"      }
\CommentTok{#> [29] "__weakref__"            "_check_exists"         }
\CommentTok{#> [31] "_format_transform_repr" "_repr_indent"          }
\CommentTok{#> [33] "class_to_idx"           "classes"               }
\CommentTok{#> [35] "data"                   "download"              }
\CommentTok{#> [37] "extra_repr"             "extract_gzip"          }
\CommentTok{#> [39] "processed_folder"       "raw_folder"            }
\CommentTok{#> [41] "root"                   "target_transform"      }
\CommentTok{#> [43] "targets"                "test_data"             }
\CommentTok{#> [45] "test_file"              "test_labels"           }
\CommentTok{#> [47] "train"                  "train_data"            }
\CommentTok{#> [49] "train_labels"           "training_file"         }
\CommentTok{#> [51] "transform"              "transforms"            }
\CommentTok{#> [53] "urls"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is identical to Python len() function}
\NormalTok{train_dataset}\OperatorTok{$}\StringTok{`}\DataTypeTok{__len__}\StringTok{`}\NormalTok{()}
\CommentTok{#> [1] 60000}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is not what we are looking for which is torch.Size([1, 28, 28])}
\CommentTok{# d0 <- train_dataset$data[0][0]}
\NormalTok{d0 <-}\StringTok{ }\NormalTok{train_dataset}\OperatorTok{$}\NormalTok{data[}\DecValTok{1}\NormalTok{][}\DecValTok{1}\NormalTok{]}
\CommentTok{#> Warning in `[.torch.Tensor`(train_dataset$data, 1): Incorrect number of}
\CommentTok{#> dimensions supplied. The number of supplied arguments, (not counting any}
\CommentTok{#> NULL, tf$newaxis or np$newaxis) must match thenumber of dimensions in the}
\CommentTok{#> tensor, unless an all_dims() was supplied (this will produce an error in}
\CommentTok{#> the future)}
\CommentTok{#> Warning in `[.torch.Tensor`(train_dataset$data[1], 1): Incorrect number of}
\CommentTok{#> dimensions supplied. The number of supplied arguments, (not counting any}
\CommentTok{#> NULL, tf$newaxis or np$newaxis) must match thenumber of dimensions in the}
\CommentTok{#> tensor, unless an all_dims() was supplied (this will produce an error in}
\CommentTok{#> the future)}
\KeywordTok{class}\NormalTok{(d0)}
\CommentTok{#> [1] "torch.Tensor"          "torch._C._TensorBase"  "python.builtin.object"}
\NormalTok{d0}\OperatorTok{$}\KeywordTok{size}\NormalTok{()}
\CommentTok{#> torch.Size([28])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
d0 <- train_dataset$data[0][0]
Error: It looks like you might be using 0-based indexing to extract using `[`. The rTorch package now uses 1-based extraction by default. You can switch to the old behavior (0-based extraction) with: options(torch.extract.one_based = FALSE)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is identical to train_dataset.data.size() in Python}
\NormalTok{train_dataset}\OperatorTok{$}\NormalTok{data}\OperatorTok{$}\KeywordTok{size}\NormalTok{()}
\CommentTok{#> torch.Size([60000, 28, 28])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is not a dimension we are looking for either}
\NormalTok{train_dataset}\OperatorTok{$}\NormalTok{data[}\KeywordTok{c}\NormalTok{(1L)][1L]}\OperatorTok{$}\KeywordTok{size}\NormalTok{()}
\CommentTok{#> Warning in `[.torch.Tensor`(train_dataset$data, c(1L)): Incorrect number}
\CommentTok{#> of dimensions supplied. The number of supplied arguments, (not counting any}
\CommentTok{#> NULL, tf$newaxis or np$newaxis) must match thenumber of dimensions in the}
\CommentTok{#> tensor, unless an all_dims() was supplied (this will produce an error in}
\CommentTok{#> the future)}
\CommentTok{#> Warning in `[.torch.Tensor`(train_dataset$data[c(1L)], 1L): Incorrect}
\CommentTok{#> number of dimensions supplied. The number of supplied arguments, (not}
\CommentTok{#> counting any NULL, tf$newaxis or np$newaxis) must match thenumber of}
\CommentTok{#> dimensions in the tensor, unless an all_dims() was supplied (this will}
\CommentTok{#> produce an error in the future)}
\CommentTok{#> torch.Size([28])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# py = import_builtins()}
\NormalTok{enum_train_dataset <-}\StringTok{ }\NormalTok{builtins}\OperatorTok{$}\KeywordTok{enumerate}\NormalTok{(train_dataset)}
\KeywordTok{class}\NormalTok{(enum_train_dataset)}
\CommentTok{#> [1] "python.builtin.iterator"  "python.builtin.enumerate"}
\CommentTok{#> [3] "python.builtin.object"}
\CommentTok{# enum_train_dataset$`__count__`}
\NormalTok{reticulate}\OperatorTok{::}\KeywordTok{py_list_attributes}\NormalTok{(enum_train_dataset)}
\CommentTok{#>  [1] "__class__"         "__delattr__"       "__dir__"          }
\CommentTok{#>  [4] "__doc__"           "__eq__"            "__format__"       }
\CommentTok{#>  [7] "__ge__"            "__getattribute__"  "__gt__"           }
\CommentTok{#> [10] "__hash__"          "__init__"          "__init_subclass__"}
\CommentTok{#> [13] "__iter__"          "__le__"            "__lt__"           }
\CommentTok{#> [16] "__ne__"            "__new__"           "__next__"         }
\CommentTok{#> [19] "__reduce__"        "__reduce_ex__"     "__repr__"         }
\CommentTok{#> [22] "__setattr__"       "__sizeof__"        "__str__"          }
\CommentTok{#> [25] "__subclasshook__"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this is not a number we were expecting}
\NormalTok{enum_train_dataset}\OperatorTok{$}\StringTok{`}\DataTypeTok{__sizeof__}\StringTok{`}\NormalTok{()}
\CommentTok{#> [1] 48}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train_dataset}\OperatorTok{$}\NormalTok{data}\OperatorTok{$}\KeywordTok{nelement}\NormalTok{()  }\CommentTok{# total number of elements in the tensor}
\CommentTok{#> [1] 47040000}
\NormalTok{train_dataset}\OperatorTok{$}\NormalTok{data}\OperatorTok{$}\NormalTok{shape       }\CommentTok{# shape}
\CommentTok{#> torch.Size([60000, 28, 28])}
\NormalTok{train_dataset}\OperatorTok{$}\NormalTok{data}\OperatorTok{$}\KeywordTok{size}\NormalTok{()      }\CommentTok{# size}
\CommentTok{#> torch.Size([60000, 28, 28])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get index, label and image}
\CommentTok{# the pointer will move forward everytime we run the chunk}
\NormalTok{obj   <-}\StringTok{ }\NormalTok{reticulate}\OperatorTok{::}\KeywordTok{iter_next}\NormalTok{(enum_train_dataset)}
\NormalTok{idx   <-}\StringTok{ }\NormalTok{obj[[}\DecValTok{1}\NormalTok{]]        }\CommentTok{# index number}

\NormalTok{image <-}\StringTok{ }\NormalTok{obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{1}\NormalTok{]]}
\NormalTok{label <-}\StringTok{ }\NormalTok{obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{2}\NormalTok{]]}

\KeywordTok{cat}\NormalTok{(idx, label, }\KeywordTok{class}\NormalTok{(label), }\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\CommentTok{#> 0 5 integer  }
\KeywordTok{print}\NormalTok{(image}\OperatorTok{$}\KeywordTok{size}\NormalTok{())}
\CommentTok{#> torch.Size([1, 28, 28])}
\end{Highlighting}
\end{Shaded}

\hypertarget{introspection-training-dataset}{%
\subsection{Introspection training dataset}\label{introspection-training-dataset}}

\hypertarget{inspecting-a-single-image}{%
\subsubsection{Inspecting a single image}\label{inspecting-a-single-image}}

So this is how a single image is represented in numbers. It's actually a 28 pixel x 28 pixel image which is why you would end up with this 28x28 matrix of numbers.

\hypertarget{inspecting-training-dataset-first-element-of-tuple}{%
\subsubsection{Inspecting training dataset first element of tuple}\label{inspecting-training-dataset-first-element-of-tuple}}

This means to access the image, you need to access the first element in the tuple.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Input Matrix}
\NormalTok{image}\OperatorTok{$}\KeywordTok{size}\NormalTok{()}
\CommentTok{#> torch.Size([1, 28, 28])}

\CommentTok{# A 28x28 sized image of a digit}
\CommentTok{# torch.Size([1, 28, 28])}
\end{Highlighting}
\end{Shaded}

\hypertarget{mnist-image-from-training-dataset}{%
\subsection{MNIST image from training dataset}\label{mnist-image-from-training-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(image}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{())}
\CommentTok{#> [1] "array"}
\KeywordTok{dim}\NormalTok{(image}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{())}
\CommentTok{#> [1]  1 28 28}
\end{Highlighting}
\end{Shaded}

\hypertarget{plot-one-image}{%
\subsubsection{Plot one image}\label{plot-one-image}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rotate <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{t}\NormalTok{(}\KeywordTok{apply}\NormalTok{(x, }\DecValTok{2}\NormalTok{, rev))   }\CommentTok{#function to rotate the matrix}

\CommentTok{# read label for digit}
\NormalTok{label}
\CommentTok{#> [1] 5}

\CommentTok{# read tensor for image}
\CommentTok{# img_tensor_2d <- image[0]}
\NormalTok{img_tensor_2d <-}\StringTok{ }\NormalTok{image[1L]}
\CommentTok{#> Warning in `[.torch.Tensor`(image, 1L): Incorrect number of dimensions}
\CommentTok{#> supplied. The number of supplied arguments, (not counting any NULL,}
\CommentTok{#> tf$newaxis or np$newaxis) must match thenumber of dimensions in the tensor,}
\CommentTok{#> unless an all_dims() was supplied (this will produce an error in the}
\CommentTok{#> future)}
\NormalTok{img_tensor_2d}\OperatorTok{$}\NormalTok{shape       }\CommentTok{# shape of the 2D tensor: torch.Size([28, 28])}
\CommentTok{#> torch.Size([28, 28])}

\CommentTok{# convert tensor to numpy array}
\NormalTok{img_mat_2d <-}\StringTok{ }\NormalTok{img_tensor_2d}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{()}
\KeywordTok{dim}\NormalTok{(img_mat_2d)}
\CommentTok{#> [1] 28 28}

\CommentTok{# show digit image}
\KeywordTok{image}\NormalTok{(}\KeywordTok{rotate}\NormalTok{(img_mat_2d))}
\KeywordTok{title}\NormalTok{(label)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{0611-mnist_idx_download_files/figure-latex/unnamed-chunk-18-1} \end{center}

\hypertarget{plot-a-second-image}{%
\subsection{Plot a second image}\label{plot-a-second-image}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# iterate to the next tensor}
\NormalTok{obj <-}\StringTok{ }\NormalTok{reticulate}\OperatorTok{::}\KeywordTok{iter_next}\NormalTok{(enum_train_dataset)   }\CommentTok{# iterator}
\NormalTok{idx <-}\StringTok{ }\NormalTok{obj[[}\DecValTok{1}\NormalTok{]]}
\NormalTok{img <-}\StringTok{ }\NormalTok{obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{1}\NormalTok{]]}
\NormalTok{lbl <-}\StringTok{ }\NormalTok{obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{2}\NormalTok{]]}

\NormalTok{img_tensor_2d <-}\StringTok{ }\NormalTok{img[}\DecValTok{1}\NormalTok{]            }\CommentTok{# get 2D tensor}
\CommentTok{#> Warning in `[.torch.Tensor`(img, 1): Incorrect number of dimensions}
\CommentTok{#> supplied. The number of supplied arguments, (not counting any NULL,}
\CommentTok{#> tf$newaxis or np$newaxis) must match thenumber of dimensions in the tensor,}
\CommentTok{#> unless an all_dims() was supplied (this will produce an error in the}
\CommentTok{#> future)}
\NormalTok{img_mat_2d <-}\StringTok{ }\NormalTok{img_tensor_2d}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{()  }\CommentTok{# convert to 2D array}

\CommentTok{# show digit image}
\KeywordTok{image}\NormalTok{(}\KeywordTok{rotate}\NormalTok{(img_mat_2d))            }\CommentTok{# rotate and plot}
\KeywordTok{title}\NormalTok{(lbl)                         }\CommentTok{# label as plot title}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{0611-mnist_idx_download_files/figure-latex/unnamed-chunk-19-1} \end{center}

\hypertarget{loading-the-test-dataset}{%
\subsection{Loading the test dataset}\label{loading-the-test-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test_dataset =}\StringTok{ }\NormalTok{dsets}\OperatorTok{$}\KeywordTok{MNIST}\NormalTok{(}\DataTypeTok{root =} \StringTok{'../../data'}\NormalTok{, }
                           \DataTypeTok{train=}\OtherTok{FALSE}\NormalTok{, }
                           \DataTypeTok{transform=}\NormalTok{transforms}\OperatorTok{$}\KeywordTok{ToTensor}\NormalTok{())}

\KeywordTok{py_len}\NormalTok{(test_dataset)}
\CommentTok{#> [1] 10000}
\end{Highlighting}
\end{Shaded}

\hypertarget{introspection-of-the-test-dataset}{%
\subsubsection{Introspection of the test dataset}\label{introspection-of-the-test-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# we'll get all the attributes of the class}
\NormalTok{reticulate}\OperatorTok{::}\KeywordTok{py_list_attributes}\NormalTok{(test_dataset)}
\CommentTok{#>  [1] "__add__"                "__class__"             }
\CommentTok{#>  [3] "__delattr__"            "__dict__"              }
\CommentTok{#>  [5] "__dir__"                "__doc__"               }
\CommentTok{#>  [7] "__eq__"                 "__format__"            }
\CommentTok{#>  [9] "__ge__"                 "__getattribute__"      }
\CommentTok{#> [11] "__getitem__"            "__gt__"                }
\CommentTok{#> [13] "__hash__"               "__init__"              }
\CommentTok{#> [15] "__init_subclass__"      "__le__"                }
\CommentTok{#> [17] "__len__"                "__lt__"                }
\CommentTok{#> [19] "__module__"             "__ne__"                }
\CommentTok{#> [21] "__new__"                "__reduce__"            }
\CommentTok{#> [23] "__reduce_ex__"          "__repr__"              }
\CommentTok{#> [25] "__setattr__"            "__sizeof__"            }
\CommentTok{#> [27] "__str__"                "__subclasshook__"      }
\CommentTok{#> [29] "__weakref__"            "_check_exists"         }
\CommentTok{#> [31] "_format_transform_repr" "_repr_indent"          }
\CommentTok{#> [33] "class_to_idx"           "classes"               }
\CommentTok{#> [35] "data"                   "download"              }
\CommentTok{#> [37] "extra_repr"             "extract_gzip"          }
\CommentTok{#> [39] "processed_folder"       "raw_folder"            }
\CommentTok{#> [41] "root"                   "target_transform"      }
\CommentTok{#> [43] "targets"                "test_data"             }
\CommentTok{#> [45] "test_file"              "test_labels"           }
\CommentTok{#> [47] "train"                  "train_data"            }
\CommentTok{#> [49] "train_labels"           "training_file"         }
\CommentTok{#> [51] "transform"              "transforms"            }
\CommentTok{#> [53] "urls"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the Python type}
\NormalTok{builtins}\OperatorTok{$}\KeywordTok{type}\NormalTok{(test_dataset}\OperatorTok{$}\StringTok{`}\DataTypeTok{__getitem__}\StringTok{`}\NormalTok{(0L))   }\CommentTok{# in Python a tuple gets converted to a list}
\CommentTok{#> <class 'list'>}
\CommentTok{# in Python: type(test_dataset[0]) -> <class 'tuple'>}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the size of the first and last image tensor}
\NormalTok{test_dataset}\OperatorTok{$}\StringTok{`}\DataTypeTok{__getitem__}\StringTok{`}\NormalTok{(0L)[[}\DecValTok{1}\NormalTok{]]}\OperatorTok{$}\KeywordTok{size}\NormalTok{()      }\CommentTok{# same as test_dataset[0][0].size()}
\CommentTok{#> torch.Size([1, 28, 28])}
\NormalTok{test_dataset}\OperatorTok{$}\StringTok{`}\DataTypeTok{__getitem__}\StringTok{`}\NormalTok{(9999L)[[}\DecValTok{1}\NormalTok{]]}\OperatorTok{$}\KeywordTok{size}\NormalTok{()  }
\CommentTok{#> torch.Size([1, 28, 28])}
\end{Highlighting}
\end{Shaded}

This is the same as:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{py_to_r}\NormalTok{(test_dataset)}
\CommentTok{#> Dataset MNIST}
\CommentTok{#>     Number of datapoints: 10000}
\CommentTok{#>     Root location: ../../data}
\CommentTok{#>     Split: Test}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the size of the first and last image tensor}
\CommentTok{# py_get_item(test_dataset, 0L)[[1]]$size()}
\KeywordTok{py_get_item}\NormalTok{(test_dataset, 0L)[[}\DecValTok{1}\NormalTok{]]}\OperatorTok{$}\KeywordTok{size}\NormalTok{()}
\CommentTok{#> torch.Size([1, 28, 28])}
\KeywordTok{py_get_item}\NormalTok{(test_dataset, 9999L)[[}\DecValTok{1}\NormalTok{]]}\OperatorTok{$}\KeywordTok{size}\NormalTok{()}
\CommentTok{#> torch.Size([1, 28, 28])}
\CommentTok{# same as test_dataset[0][0].size()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the label is the second list member}
\NormalTok{label <-}\StringTok{ }\NormalTok{test_dataset}\OperatorTok{$}\StringTok{`}\DataTypeTok{__getitem__}\StringTok{`}\NormalTok{(0L)[[}\DecValTok{2}\NormalTok{]]  }\CommentTok{# in Python: test_dataset[0][1]}
\NormalTok{label}
\CommentTok{#> [1] 7}
\end{Highlighting}
\end{Shaded}

\hypertarget{plot-image-test-dataset}{%
\subsection{Plot image test dataset}\label{plot-image-test-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert tensor to numpy array}
\NormalTok{.show_img <-}\StringTok{ }\NormalTok{test_dataset}\OperatorTok{$}\StringTok{`}\DataTypeTok{__getitem__}\StringTok{`}\NormalTok{(0L)[[}\DecValTok{1}\NormalTok{]]}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{() }
\KeywordTok{dim}\NormalTok{(.show_img)                                           }\CommentTok{# numpy 3D array}
\CommentTok{#> [1]  1 28 28}

\CommentTok{# reshape 3D array to 2D }
\NormalTok{show_img <-}\StringTok{ }\NormalTok{np}\OperatorTok{$}\KeywordTok{reshape}\NormalTok{(.show_img, }\KeywordTok{c}\NormalTok{(28L, 28L))}
\KeywordTok{dim}\NormalTok{(show_img)}
\CommentTok{#> [1] 28 28}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# another way to reshape the array}
\NormalTok{show_img <-}\StringTok{ }\NormalTok{np}\OperatorTok{$}\KeywordTok{reshape}\NormalTok{(test_dataset}\OperatorTok{$}\StringTok{`}\DataTypeTok{__getitem__}\StringTok{`}\NormalTok{(0L)[[}\DecValTok{1}\NormalTok{]]}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{(), }\KeywordTok{c}\NormalTok{(28L, 28L))}
\KeywordTok{dim}\NormalTok{(show_img)}
\CommentTok{#> [1] 28 28}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# show in grays and rotate}
\KeywordTok{image}\NormalTok{(}\KeywordTok{rotate}\NormalTok{(show_img), }\DataTypeTok{col =} \KeywordTok{gray.colors}\NormalTok{(}\DecValTok{64}\NormalTok{))}
\KeywordTok{title}\NormalTok{(label)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{0611-mnist_idx_download_files/figure-latex/unnamed-chunk-29-1} \end{center}

\hypertarget{plot-a-second-test-image}{%
\subsection{Plot a second test image}\label{plot-a-second-test-image}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# next image, index moves from (0L) to (1L), and so on}
\NormalTok{idx <-}\StringTok{ }\NormalTok{1L}
\NormalTok{.show_img <-}\StringTok{ }\NormalTok{test_dataset}\OperatorTok{$}\StringTok{`}\DataTypeTok{__getitem__}\StringTok{`}\NormalTok{(idx)[[}\DecValTok{1}\NormalTok{]]}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{()}
\NormalTok{show_img  <-}\StringTok{ }\NormalTok{np}\OperatorTok{$}\KeywordTok{reshape}\NormalTok{(.show_img, }\KeywordTok{c}\NormalTok{(28L, 28L))}
\NormalTok{label     <-}\StringTok{ }\NormalTok{test_dataset}\OperatorTok{$}\StringTok{`}\DataTypeTok{__getitem__}\StringTok{`}\NormalTok{(idx)[[}\DecValTok{2}\NormalTok{]]}

\KeywordTok{image}\NormalTok{(}\KeywordTok{rotate}\NormalTok{(show_img), }\DataTypeTok{col =} \KeywordTok{gray.colors}\NormalTok{(}\DecValTok{64}\NormalTok{))}
\KeywordTok{title}\NormalTok{(label)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{0611-mnist_idx_download_files/figure-latex/unnamed-chunk-30-1} \end{center}

\hypertarget{plot-the-last-test-image}{%
\subsection{Plot the last test image}\label{plot-the-last-test-image}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# next image, index moves from (0L) to (1L), and so on}
\CommentTok{# first image is 0, last image would be 9999}
\NormalTok{idx <-}\StringTok{ }\KeywordTok{py_len}\NormalTok{(test_dataset) }\OperatorTok{-}\StringTok{ }\NormalTok{1L}
\NormalTok{.show_img <-}\StringTok{ }\NormalTok{test_dataset}\OperatorTok{$}\StringTok{`}\DataTypeTok{__getitem__}\StringTok{`}\NormalTok{(idx)[[}\DecValTok{1}\NormalTok{]]}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{()}
\NormalTok{show_img  <-}\StringTok{ }\NormalTok{np}\OperatorTok{$}\KeywordTok{reshape}\NormalTok{(.show_img, }\KeywordTok{c}\NormalTok{(28L, 28L))}
\NormalTok{label     <-}\StringTok{ }\NormalTok{test_dataset}\OperatorTok{$}\StringTok{`}\DataTypeTok{__getitem__}\StringTok{`}\NormalTok{(idx)[[}\DecValTok{2}\NormalTok{]]}

\KeywordTok{image}\NormalTok{(}\KeywordTok{rotate}\NormalTok{(show_img), }\DataTypeTok{col =} \KeywordTok{gray.colors}\NormalTok{(}\DecValTok{64}\NormalTok{))}
\KeywordTok{title}\NormalTok{(label)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{0611-mnist_idx_download_files/figure-latex/unnamed-chunk-31-1} \end{center}

\hypertarget{defining-epochs}{%
\section{Defining epochs}\label{defining-epochs}}

When the model goes through the whole 60k images once, learning how to classify 0-9, it's consider \textbf{1 epoch}.

However, there's a concept of batch size where it means the model would look at 100 images before updating the model's weights, thereby learning. When the model updates its weights (parameters) after looking at all the images, this is considered 1 iteration.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{batch_size <-}\StringTok{ }\NormalTok{100L}
\end{Highlighting}
\end{Shaded}

We arbitrarily set 3000 iterations here which means the model would update 3000 times.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n_iters <-}\StringTok{ }\NormalTok{3000L}
\end{Highlighting}
\end{Shaded}

One epoch consists of 60,000 / 100 = 600 iterations. Because we would like to go through 3000 iterations, this implies we would have 3000 / 600 = 5 epochs as each epoch has 600 iterations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num_epochs =}\StringTok{ }\NormalTok{n_iters }\OperatorTok{/}\StringTok{ }\NormalTok{(}\KeywordTok{py_len}\NormalTok{(train_dataset) }\OperatorTok{/}\StringTok{ }\NormalTok{batch_size)}
\NormalTok{num_epochs =}\StringTok{ }\KeywordTok{as.integer}\NormalTok{(num_epochs)}
\NormalTok{num_epochs}
\CommentTok{#> [1] 5}
\end{Highlighting}
\end{Shaded}

\hypertarget{create-iterable-objects-training-and-testing-dataset}{%
\section{Create iterable objects: training and testing dataset}\label{create-iterable-objects-training-and-testing-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train_loader =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\NormalTok{utils}\OperatorTok{$}\NormalTok{data}\OperatorTok{$}\KeywordTok{DataLoader}\NormalTok{(}\DataTypeTok{dataset=}\NormalTok{train_dataset, }
                                           \DataTypeTok{batch_size=}\NormalTok{batch_size, }
                                           \DataTypeTok{shuffle=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Iterable object}
\NormalTok{test_loader =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\NormalTok{utils}\OperatorTok{$}\NormalTok{data}\OperatorTok{$}\KeywordTok{DataLoader}\NormalTok{(}\DataTypeTok{dataset=}\NormalTok{test_dataset, }
                                          \DataTypeTok{batch_size=}\NormalTok{batch_size, }
                                          \DataTypeTok{shuffle=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{check-iteraibility}{%
\subsection{Check iteraibility}\label{check-iteraibility}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{collections <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"collections"}\NormalTok{)}

\NormalTok{builtins}\OperatorTok{$}\KeywordTok{isinstance}\NormalTok{(train_loader, collections}\OperatorTok{$}\NormalTok{Iterable)}
\CommentTok{#> [1] TRUE}
\NormalTok{builtins}\OperatorTok{$}\KeywordTok{isinstance}\NormalTok{(test_loader, collections}\OperatorTok{$}\NormalTok{Iterable)}
\CommentTok{#> [1] TRUE}
\end{Highlighting}
\end{Shaded}

\hypertarget{building-the-model}{%
\section{Building the model}\label{building-the-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Same as linear regression! }
\NormalTok{main <-}\StringTok{ }\KeywordTok{py_run_string}\NormalTok{(}
\StringTok{"}
\StringTok{import torch.nn as nn}

\StringTok{class LogisticRegressionModel(nn.Module):}
\StringTok{    def __init__(self, input_dim, output_dim):}
\StringTok{        super(LogisticRegressionModel, self).__init__()}
\StringTok{        self.linear = nn.Linear(input_dim, output_dim)}

\StringTok{    def forward(self, x):}
\StringTok{        out = self.linear(x)}
\StringTok{        return out}
\StringTok{"}\NormalTok{)}

\CommentTok{# build a Linear Rgression model}
\NormalTok{LogisticRegressionModel <-}\StringTok{ }\NormalTok{main}\OperatorTok{$}\NormalTok{LogisticRegressionModel}
\end{Highlighting}
\end{Shaded}

\hypertarget{instantiate-model-class-based-on-input-and-out-dimensions}{%
\subsection{Instantiate model class based on input and out dimensions}\label{instantiate-model-class-based-on-input-and-out-dimensions}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# feeding the model with 28x28 images}
\NormalTok{input_dim =}\StringTok{ }\NormalTok{28L}\OperatorTok{*}\NormalTok{28L}

\CommentTok{# classify digits 0-9 a total of 10 classes,}
\NormalTok{output_dim =}\StringTok{ }\NormalTok{10L}

\NormalTok{model =}\StringTok{ }\KeywordTok{LogisticRegressionModel}\NormalTok{(input_dim, output_dim)}
\end{Highlighting}
\end{Shaded}

\hypertarget{instantiate-cross-entropy-loss-class}{%
\subsection{Instantiate Cross Entropy Loss class}\label{instantiate-cross-entropy-loss-class}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# need Cross Entropy Loss to calculate loss before we backpropagation}
\NormalTok{criterion =}\StringTok{ }\NormalTok{nn}\OperatorTok{$}\KeywordTok{CrossEntropyLoss}\NormalTok{()  }
\end{Highlighting}
\end{Shaded}

\hypertarget{instantiate-optimizer-class}{%
\subsection{Instantiate Optimizer class}\label{instantiate-optimizer-class}}

Similar to what we've covered above, this calculates the parameters' gradients and update them subsequently.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate parameters' gradients and update}
\NormalTok{learning_rate =}\StringTok{ }\FloatTok{0.001}

\NormalTok{optimizer =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\NormalTok{optim}\OperatorTok{$}\KeywordTok{SGD}\NormalTok{(model}\OperatorTok{$}\KeywordTok{parameters}\NormalTok{(), }\DataTypeTok{lr=}\NormalTok{learning_rate)  }
\end{Highlighting}
\end{Shaded}

\hypertarget{parameters-introspection}{%
\subsection{Parameters introspection}\label{parameters-introspection}}

You'll realize we have 2 sets of parameters, 10x784 which is \(A\) and 10x1 which is \(b\) in the \(y = AX + b\) equation, where \(X\) is our input of size 784.

We'll go into details subsequently how these parameters interact with our input to produce our 10x1 output.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Type of parameter object}
\KeywordTok{print}\NormalTok{(model}\OperatorTok{$}\KeywordTok{parameters}\NormalTok{())}
\CommentTok{#> <generator object Module.parameters at 0x7f309296df10>}
\NormalTok{model_parameters <-}\StringTok{ }\NormalTok{builtins}\OperatorTok{$}\KeywordTok{list}\NormalTok{(model}\OperatorTok{$}\KeywordTok{parameters}\NormalTok{())}

\CommentTok{# Length of parameters}
\KeywordTok{print}\NormalTok{(builtins}\OperatorTok{$}\KeywordTok{len}\NormalTok{(model_parameters))}
\CommentTok{#> [1] 2}

\CommentTok{# FC 1 Parameters }
\NormalTok{builtins}\OperatorTok{$}\KeywordTok{list}\NormalTok{(model_parameters)[[}\DecValTok{1}\NormalTok{]]}\OperatorTok{$}\KeywordTok{size}\NormalTok{()}
\CommentTok{#> torch.Size([10, 784])}

\CommentTok{# FC 1 Bias Parameters}
\NormalTok{builtins}\OperatorTok{$}\KeywordTok{list}\NormalTok{(model_parameters)[[}\DecValTok{2}\NormalTok{]]}\OperatorTok{$}\KeywordTok{size}\NormalTok{()}
\CommentTok{#> torch.Size([10])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{builtins}\OperatorTok{$}\KeywordTok{len}\NormalTok{(builtins}\OperatorTok{$}\KeywordTok{list}\NormalTok{(model}\OperatorTok{$}\KeywordTok{parameters}\NormalTok{()))}
\CommentTok{#> [1] 2}
\end{Highlighting}
\end{Shaded}

\hypertarget{train-the-model-and-test-per-epoch}{%
\section{Train the model and test per epoch}\label{train-the-model-and-test-per-epoch}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iter =}\StringTok{ }\DecValTok{0}

\ControlFlowTok{for}\NormalTok{ (epoch }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{num_epochs) \{}
\NormalTok{  iter_train_dataset <-}\StringTok{ }\NormalTok{builtins}\OperatorTok{$}\KeywordTok{enumerate}\NormalTok{(train_loader) }\CommentTok{# convert to iterator}
  \ControlFlowTok{for}\NormalTok{ (obj }\ControlFlowTok{in} \KeywordTok{iterate}\NormalTok{(iter_train_dataset)) \{}
      \CommentTok{# get the tensors for images and labels}
\NormalTok{      images <-}\StringTok{ }\NormalTok{obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{1}\NormalTok{]]}
\NormalTok{      labels <-}\StringTok{ }\NormalTok{obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{2}\NormalTok{]]}
      
      \CommentTok{# Reshape images to (batch_size, input_size)}
\NormalTok{      images <-}\StringTok{ }\NormalTok{images}\OperatorTok{$}\KeywordTok{view}\NormalTok{(}\OperatorTok{-}\NormalTok{1L, 28L}\OperatorTok{*}\NormalTok{28L)}\OperatorTok{$}\KeywordTok{requires_grad_}\NormalTok{()}
      
      \CommentTok{# Clear gradients w.r.t. parameters}
\NormalTok{      optimizer}\OperatorTok{$}\KeywordTok{zero_grad}\NormalTok{()}
      
      \CommentTok{# Forward pass to get output/logits}
\NormalTok{      outputs =}\StringTok{ }\KeywordTok{model}\NormalTok{(images)}
      
      \CommentTok{# Calculate Loss: softmax --> cross entropy loss}
\NormalTok{      loss =}\StringTok{ }\KeywordTok{criterion}\NormalTok{(outputs, labels)}
      
      \CommentTok{# Getting gradients w.r.t. parameters}
\NormalTok{      loss}\OperatorTok{$}\KeywordTok{backward}\NormalTok{()}
      
      \CommentTok{# Updating parameters}
\NormalTok{      optimizer}\OperatorTok{$}\KeywordTok{step}\NormalTok{()}
      
\NormalTok{      iter =}\StringTok{ }\NormalTok{iter }\OperatorTok{+}\StringTok{ }\DecValTok{1}
      
      \ControlFlowTok{if}\NormalTok{ (iter }\OperatorTok{%%}\StringTok{ }\DecValTok{500} \OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
          \CommentTok{# Calculate Accuracy         }
\NormalTok{          correct =}\StringTok{ }\DecValTok{0}
\NormalTok{          total =}\StringTok{ }\DecValTok{0}
          
          \CommentTok{# Iterate through test dataset}
\NormalTok{          iter_test_dataset <-}\StringTok{ }\NormalTok{builtins}\OperatorTok{$}\KeywordTok{enumerate}\NormalTok{(test_loader) }\CommentTok{# convert to iterator}
          \ControlFlowTok{for}\NormalTok{ (obj2 }\ControlFlowTok{in} \KeywordTok{iterate}\NormalTok{(iter_test_dataset)) \{}
              \CommentTok{# Load images to a Torch Variable}
\NormalTok{              images <-}\StringTok{ }\NormalTok{obj2[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{1}\NormalTok{]]}
\NormalTok{              labels <-}\StringTok{ }\NormalTok{obj2[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{2}\NormalTok{]]}
\NormalTok{              images <-}\StringTok{ }\NormalTok{images}\OperatorTok{$}\KeywordTok{view}\NormalTok{(}\OperatorTok{-}\NormalTok{1L, 28L}\OperatorTok{*}\NormalTok{28L)}\OperatorTok{$}\KeywordTok{requires_grad_}\NormalTok{()}
          
              \CommentTok{# Forward pass only to get logits/output}
\NormalTok{              outputs =}\StringTok{ }\KeywordTok{model}\NormalTok{(images)}
          
              \CommentTok{# Get predictions from the maximum value}
\NormalTok{              .predicted =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{max}\NormalTok{(outputs}\OperatorTok{$}\NormalTok{data, 1L)}
\NormalTok{              predicted <-}\StringTok{ }\NormalTok{.predicted[1L]}
          
              \CommentTok{# Total number of labels}
\NormalTok{              total =}\StringTok{ }\NormalTok{total }\OperatorTok{+}\StringTok{ }\NormalTok{labels}\OperatorTok{$}\KeywordTok{size}\NormalTok{(0L)}
          
              \CommentTok{# Total correct predictions}
\NormalTok{              correct =}\StringTok{ }\NormalTok{correct }\OperatorTok{+}\StringTok{ }\KeywordTok{sum}\NormalTok{((predicted}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{() }\OperatorTok{==}\StringTok{ }\NormalTok{labels}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{()))}
\NormalTok{          \}}
\NormalTok{          accuracy =}\StringTok{ }\DecValTok{100} \OperatorTok{*}\StringTok{ }\NormalTok{correct }\OperatorTok{/}\StringTok{ }\NormalTok{total}
          
          \CommentTok{# Print Loss}
          \KeywordTok{cat}\NormalTok{(}\KeywordTok{sprintf}\NormalTok{(}\StringTok{'Iteration: %5d. Loss: %f. Accuracy: %8.2f }\CharTok{\textbackslash{}n}\StringTok{'}\NormalTok{, }
\NormalTok{                      iter, loss}\OperatorTok{$}\KeywordTok{item}\NormalTok{(), accuracy))}
\NormalTok{      \}}
\NormalTok{  \}}
\NormalTok{\}  }
\CommentTok{#> Iteration:   500. Loss: 1.881020. Accuracy:    70.27 }
\CommentTok{#> Iteration:  1000. Loss: 1.517125. Accuracy:    77.97 }
\CommentTok{#> Iteration:  1500. Loss: 1.354405. Accuracy:    80.60 }
\CommentTok{#> Iteration:  2000. Loss: 1.211496. Accuracy:    81.69 }
\CommentTok{#> Iteration:  2500. Loss: 1.132213. Accuracy:    82.59 }
\CommentTok{#> Iteration:  3000. Loss: 1.019603. Accuracy:    83.38}
\end{Highlighting}
\end{Shaded}

\hypertarget{break-down-accuracy-calculation}{%
\section{Break down accuracy calculation}\label{break-down-accuracy-calculation}}

As we've trained our model, we can extract the accuracy calculation portion to understand what's happening without re-training the model.

This would print out the output of the model's predictions on your notebook.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Iterate through test dataset}
\NormalTok{iter_test <-}\StringTok{ }\DecValTok{0}
\NormalTok{iter_test_dataset <-}\StringTok{ }\NormalTok{builtins}\OperatorTok{$}\KeywordTok{enumerate}\NormalTok{(test_loader) }\CommentTok{# convert to iterator}
\ControlFlowTok{for}\NormalTok{ (test_obj }\ControlFlowTok{in} \KeywordTok{iterate}\NormalTok{(iter_test_dataset)) \{}
\NormalTok{    iter_test <-}\StringTok{ }\NormalTok{iter_test }\OperatorTok{+}\StringTok{ }\DecValTok{1}
    \CommentTok{# Load images to a Torch Variable}
\NormalTok{    images <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{1}\NormalTok{]]}
\NormalTok{    labels <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{2}\NormalTok{]]}
\NormalTok{    images <-}\StringTok{ }\NormalTok{images}\OperatorTok{$}\KeywordTok{view}\NormalTok{(}\OperatorTok{-}\NormalTok{1L, 28L}\OperatorTok{*}\NormalTok{28L)}\OperatorTok{$}\KeywordTok{requires_grad_}\NormalTok{()}
    
    \CommentTok{# Forward pass only to get logits/output}
\NormalTok{    outputs =}\StringTok{ }\KeywordTok{model}\NormalTok{(images)}
    
    \ControlFlowTok{if}\NormalTok{ (iter_test }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) \{}
        \KeywordTok{print}\NormalTok{(}\StringTok{'OUTPUTS'}\NormalTok{)}
        \KeywordTok{print}\NormalTok{(outputs)}
\NormalTok{    \}}
    \CommentTok{# Get predictions from the maximum value}
\NormalTok{    .predicted =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{max}\NormalTok{(outputs}\OperatorTok{$}\NormalTok{data, 1L)}
\NormalTok{    predicted <-}\StringTok{ }\NormalTok{.predicted[1L]}
\NormalTok{\}}
\CommentTok{#> [1] "OUTPUTS"}
\CommentTok{#> tensor([[-2.0479e-01, -1.3593e+00, -3.0488e-01, -1.4720e-01,  4.0833e-02,}
\CommentTok{#>          -5.3239e-01, -9.8131e-01,  2.8635e+00, -4.0464e-01,  9.0220e-01],}
\CommentTok{#>         [ 3.2250e-01,  6.8070e-02,  1.4779e+00,  1.0052e+00, -1.7874e+00,}
\CommentTok{#>           8.2865e-01,  1.0045e+00, -1.9955e+00,  4.5074e-01, -1.3909e+00],}
\CommentTok{#>         [-8.4276e-01,  2.2414e+00,  1.2184e-01, -2.9651e-02, -5.4253e-01,}
\CommentTok{#>          -3.3983e-01, -1.5156e-01, -2.3036e-01,  2.2024e-01, -3.3406e-01],}
\CommentTok{#>         [ 2.9647e+00, -2.5175e+00, -8.8717e-02, -2.4168e-01, -9.3389e-01,}
\CommentTok{#>           6.5263e-01,  1.1432e+00,  1.9990e-01, -6.4065e-01, -1.2482e-01],}
\CommentTok{#>         [-1.4304e-01, -2.0405e+00,  3.6333e-01, -8.0922e-01,  1.7364e+00,}
\CommentTok{#>          -6.6911e-01,  2.1178e-01,  4.0157e-01, -1.0758e-01,  7.5692e-01],}
\CommentTok{#>         [-1.2987e+00,  2.7115e+00,  2.7505e-01,  1.3521e-01, -6.1180e-01,}
\CommentTok{#>          -4.1990e-01, -7.4985e-01,  4.2535e-03,  4.3589e-01, -2.0743e-01],}
\CommentTok{#>         [-1.2870e+00, -1.3138e+00, -7.4987e-01,  8.9608e-02,  1.5270e+00,}
\CommentTok{#>          -4.6191e-02, -4.6467e-01,  7.4020e-01,  5.6403e-01,  9.5931e-01],}
\CommentTok{#>         [-1.2569e+00, -3.7767e-01, -7.0048e-01, -1.1057e-01,  7.2034e-01,}
\CommentTok{#>           2.7289e-01,  3.0095e-01, -1.4627e-02,  2.8486e-01,  1.3238e+00],}
\CommentTok{#>         [ 1.0152e-01, -3.0979e-01,  9.4464e-01, -1.3014e+00,  6.3443e-01,}
\CommentTok{#>           1.4799e-01,  6.7136e-01, -8.3160e-01,  3.8755e-02,  2.4443e-01],}
\CommentTok{#>         [-1.2765e-01, -8.8092e-01, -1.0809e+00, -1.1238e+00,  1.1095e+00,}
\CommentTok{#>          -2.1077e-01, -4.2052e-01,  1.9280e+00,  8.6710e-02,  1.6435e+00],}
\CommentTok{#>         [ 3.1574e+00, -1.9309e+00,  6.6490e-01,  6.8592e-01, -7.4162e-01,}
\CommentTok{#>           9.8014e-01, -3.9286e-01, -1.4621e+00,  5.7645e-01, -1.7922e+00],}
\CommentTok{#>         [ 8.1289e-01, -5.4395e-01,  6.2101e-01, -3.8470e-01,  4.7841e-01,}
\CommentTok{#>          -5.1522e-01,  9.4043e-01, -8.0787e-01,  2.5931e-01, -5.0283e-01],}
\CommentTok{#>         [-7.9017e-01, -1.3696e+00, -8.5082e-01, -9.1603e-01,  1.6260e+00,}
\CommentTok{#>          -7.7058e-02, -1.6040e-01,  1.1269e+00,  2.0468e-01,  2.2587e+00],}
\CommentTok{#>         [ 3.1616e+00, -2.3800e+00, -2.7943e-01, -2.4139e-01, -3.6120e-01,}
\CommentTok{#>           7.3184e-01, -3.4811e-01, -4.4688e-01,  4.1694e-01,  2.4069e-01],}
\CommentTok{#>         [-1.2642e+00,  2.8791e+00, -1.1524e-01,  4.3556e-01, -1.1386e+00,}
\CommentTok{#>          -2.1253e-01,  3.7044e-02, -1.2600e-01,  3.3838e-01, -1.7522e-01],}
\CommentTok{#>         [ 3.7750e-01, -9.0595e-01, -1.8303e-01,  1.1638e+00, -2.9494e-01,}
\CommentTok{#>           7.6012e-01, -4.4813e-01, -4.9704e-01,  6.9392e-01, -5.9989e-01],}
\CommentTok{#>         [-4.5471e-01, -1.9645e+00,  1.6492e-01, -7.3672e-01,  1.4368e+00,}
\CommentTok{#>          -6.8745e-01, -1.7953e-01,  7.6996e-01,  2.2585e-01,  1.6515e+00],}
\CommentTok{#>         [ 2.1165e-01, -1.9543e+00, -4.4266e-01,  3.6177e-01, -3.1148e-01,}
\CommentTok{#>          -2.6539e-01, -7.6945e-01,  2.6859e+00, -6.0181e-01,  6.2520e-01],}
\CommentTok{#>         [-6.6666e-01, -6.3672e-01, -4.8508e-02,  1.1945e+00, -6.4643e-01,}
\CommentTok{#>           3.0000e-01,  1.0074e+00, -5.9322e-01,  1.0006e-01, -3.4331e-01],}
\CommentTok{#>         [-8.5328e-01, -1.3704e+00, -4.8718e-01, -1.4346e-01,  1.9699e+00,}
\CommentTok{#>           1.6221e-03, -8.5731e-02,  2.3024e-01,  1.1548e-01,  1.3424e+00],}
\CommentTok{#>         [-7.7359e-01, -2.8210e-01, -1.3802e+00,  1.9043e-01,  6.7667e-01,}
\CommentTok{#>           1.1207e-01, -1.2175e+00,  1.7131e+00,  1.4865e-01,  1.6175e+00],}
\CommentTok{#>         [-5.0142e-01, -1.5318e+00, -2.6184e-02,  4.2558e-01,  3.6907e-01,}
\CommentTok{#>           6.6034e-01,  2.3316e+00, -1.5406e+00,  4.8098e-01, -1.3304e-01],}
\CommentTok{#>         [-2.4639e-01, -1.0642e-01,  5.2688e-01, -7.9012e-01,  9.2872e-01,}
\CommentTok{#>          -1.1287e+00,  1.2245e+00,  1.2407e-01, -3.1249e-01, -2.1534e-03],}
\CommentTok{#>         [-1.0451e-01, -9.4606e-01, -5.9241e-01,  3.9987e-01, -1.3748e-01,}
\CommentTok{#>           1.5447e+00,  1.9996e-01, -9.4401e-01,  8.8916e-01,  1.5567e-01],}
\CommentTok{#>         [-7.7939e-01, -1.1531e+00,  1.6778e-01, -1.4628e-01,  1.3819e+00,}
\CommentTok{#>          -2.5042e-01, -1.5504e-01,  3.4741e-01, -2.4857e-01,  1.0174e+00],}
\CommentTok{#>         [ 4.3512e+00, -2.9414e+00,  8.7806e-01, -1.3870e+00, -3.3717e-01,}
\CommentTok{#>           1.0033e+00,  1.2734e+00, -7.3720e-01,  1.1137e-02, -1.3699e+00],}
\CommentTok{#>         [-2.8166e-01, -1.3024e+00, -4.5727e-01,  7.2126e-02,  1.7663e-01,}
\CommentTok{#>          -2.4383e-01, -4.9624e-01,  1.6236e+00, -6.0035e-01,  1.1375e+00],}
\CommentTok{#>         [-3.7374e-01, -2.1148e+00, -5.2038e-01, -6.6244e-01,  2.3591e+00,}
\CommentTok{#>           9.7800e-02,  2.6121e-01,  2.0765e-02,  1.6089e-01,  1.4404e+00],}
\CommentTok{#>         [ 2.7762e+00, -2.4484e+00,  3.7692e-02,  8.9276e-01, -1.1306e+00,}
\CommentTok{#>           6.9539e-01, -2.5336e-01, -7.3195e-01,  3.6679e-01, -4.0027e-01],}
\CommentTok{#>         [-1.0824e+00,  1.4544e+00, -4.0074e-01,  2.3187e-01, -4.6137e-01,}
\CommentTok{#>           1.5103e-01,  1.8921e-01, -7.8811e-02,  3.1913e-01, -2.7850e-01],}
\CommentTok{#>         [-8.8933e-01, -4.6435e-01, -8.9819e-01,  2.1559e+00, -1.3384e+00,}
\CommentTok{#>           1.0193e+00, -3.9569e-01,  1.7003e-01,  1.8287e-01, -1.3543e-01],}
\CommentTok{#>         [-9.8079e-01,  1.1783e+00, -4.3017e-01,  3.1909e-01, -3.2282e-01,}
\CommentTok{#>           5.9966e-02,  2.4061e-03,  1.6737e-01,  1.7956e-01,  1.2991e-01],}
\CommentTok{#>         [-9.0914e-01, -8.1733e-01, -7.7819e-01,  2.4002e+00, -4.6174e-01,}
\CommentTok{#>           1.4284e+00, -5.5151e-01, -1.2451e+00,  4.4246e-01, -8.2233e-02],}
\CommentTok{#>         [ 1.8606e+00, -1.8194e+00,  6.8703e-01, -1.8911e+00,  8.0873e-01,}
\CommentTok{#>           4.2704e-01,  1.7418e+00, -8.1675e-01, -1.1473e-01, -2.8603e-01],}
\CommentTok{#>         [-8.7625e-01, -2.8564e-01,  7.9909e-01, -3.0945e-01,  6.9551e-02,}
\CommentTok{#>          -5.6687e-01, -1.3608e+00,  1.7866e+00,  7.8766e-01,  3.9826e-01],}
\CommentTok{#>         [ 5.2140e-01, -1.2894e+00,  2.5902e+00,  2.8523e-01, -6.7693e-01,}
\CommentTok{#>           3.0978e-01,  4.7118e-01, -3.0533e-01,  2.6335e-01, -1.7316e+00],}
\CommentTok{#>         [-6.0106e-01, -1.6226e+00,  1.5349e-01, -7.6830e-03, -1.7812e-02,}
\CommentTok{#>          -6.0396e-01, -4.6993e-01,  2.3227e+00, -2.9389e-01,  1.1643e+00],}
\CommentTok{#>         [-1.2897e+00,  2.1840e+00, -5.1492e-01,  1.3564e-01, -6.2325e-01,}
\CommentTok{#>          -2.8047e-02,  2.5177e-02,  9.4724e-02,  4.6202e-01, -6.7154e-03],}
\CommentTok{#>         [ 2.5418e-01,  1.9973e-01,  1.0947e+00,  9.8032e-01, -1.8865e+00,}
\CommentTok{#>           3.9306e-01,  6.4038e-01, -1.3409e+00,  7.8411e-01, -1.2461e+00],}
\CommentTok{#>         [-1.4295e+00,  3.0419e+00, -2.9736e-01,  3.3749e-01, -1.3243e+00,}
\CommentTok{#>          -2.3459e-02, -8.5192e-02, -5.8925e-01,  6.2745e-01, -2.1938e-01],}
\CommentTok{#>         [-7.0331e-01,  1.6353e+00, -3.0117e-02,  1.8527e-01, -5.3464e-01,}
\CommentTok{#>          -1.5881e-01, -1.8888e-02, -1.5384e-01,  1.3406e-02, -1.7692e-01],}
\CommentTok{#>         [-7.3279e-01, -9.4782e-01, -2.9877e-02, -4.6342e-01,  2.9866e-02,}
\CommentTok{#>          -2.3611e-01, -4.4630e-01,  2.0039e+00, -3.7444e-01,  1.2795e+00],}
\CommentTok{#>         [-2.0154e+00, -3.2036e-01, -3.9857e-02, -3.6067e-01,  2.2775e+00,}
\CommentTok{#>          -7.1071e-01, -8.0430e-01,  7.2148e-01,  5.2522e-01,  1.7124e+00],}
\CommentTok{#>         [-4.7475e-01,  7.9963e-01,  1.2281e+00, -1.3052e-01, -7.3618e-02,}
\CommentTok{#>          -3.4371e-01,  3.0799e-01, -9.8563e-01,  4.7410e-01, -3.3280e-01],}
\CommentTok{#>         [-1.0504e+00,  7.3264e-02, -7.3355e-02,  1.3657e+00, -8.6593e-01,}
\CommentTok{#>           5.3903e-01,  4.3451e-01, -4.2514e-01,  7.7801e-02, -2.4562e-01],}
\CommentTok{#>         [ 7.4037e-02, -1.2902e+00, -7.2728e-01,  1.8037e+00, -6.5481e-01,}
\CommentTok{#>           1.4527e+00, -1.0006e-01, -1.3808e+00,  1.0110e+00, -4.6459e-01],}
\CommentTok{#>         [-1.5870e+00,  3.0939e-01, -1.5997e-01,  7.4274e-01, -1.4331e-01,}
\CommentTok{#>           3.3919e-01,  1.1756e-01,  4.8152e-02,  2.5385e-01,  2.5526e-01],}
\CommentTok{#>         [-7.3733e-01, -2.3935e-01,  1.6282e+00, -2.4430e-01,  6.6310e-01,}
\CommentTok{#>          -6.9060e-01,  5.1241e-01, -2.3024e-02,  4.7846e-02,  1.7471e-01],}
\CommentTok{#>         [-1.0746e+00, -2.5196e+00, -1.3470e+00,  1.5832e-02,  2.8318e+00,}
\CommentTok{#>           3.6442e-01, -5.6977e-01,  3.9916e-01,  7.3580e-01,  2.1160e+00],}
\CommentTok{#>         [-9.2246e-02, -2.0376e+00,  6.1287e-01, -7.3385e-01,  2.2582e+00,}
\CommentTok{#>          -9.1551e-01,  7.1243e-02,  4.2175e-01, -2.2545e-01,  1.1509e+00],}
\CommentTok{#>         [-3.9357e-03, -1.0385e+00,  1.2450e-01,  3.3858e-01, -3.4441e-01,}
\CommentTok{#>           5.1258e-01,  2.0654e+00, -1.4793e+00,  8.8135e-02, -2.6334e-01],}
\CommentTok{#>         [-6.1840e-02, -1.1377e+00, -2.7466e-01,  1.8441e+00, -6.5354e-01,}
\CommentTok{#>           6.8002e-01,  1.0520e-01, -4.1124e-01, -1.3710e-01,  9.1419e-02],}
\CommentTok{#>         [ 5.9769e-01, -1.1936e+00, -1.3663e+00,  3.1573e-03,  6.3732e-01,}
\CommentTok{#>           1.3869e+00, -7.9784e-02, -3.3405e-01, -1.1160e-01,  6.0435e-01],}
\CommentTok{#>         [ 2.1860e-01, -1.1580e+00, -4.7119e-01,  8.8685e-01, -3.5183e-02,}
\CommentTok{#>           7.0088e-01, -1.6251e-01, -6.6693e-01,  4.7858e-01, -1.4018e-01],}
\CommentTok{#>         [ 3.5646e-01, -7.3620e-01,  1.4141e+00, -6.4968e-01,  7.3994e-01,}
\CommentTok{#>          -1.0560e+00,  6.6393e-01, -5.0864e-01, -1.0077e-01, -1.2591e-01],}
\CommentTok{#>         [ 1.5414e+00, -2.2194e+00, -6.5543e-01,  6.8563e-01, -7.1479e-01,}
\CommentTok{#>           9.8243e-01,  2.4973e-01, -8.0620e-01,  1.0722e+00, -1.4052e-01],}
\CommentTok{#>         [-3.3331e-02, -2.7590e+00, -2.8456e-02, -4.7988e-01,  2.7396e+00,}
\CommentTok{#>           3.8984e-02,  4.1396e-01, -1.8820e-01,  2.7952e-02,  9.3577e-01],}
\CommentTok{#>         [-9.0653e-01,  2.2782e+00,  4.2394e-02,  1.7376e-01, -7.5019e-01,}
\CommentTok{#>          -3.2438e-01, -5.2303e-01, -1.4208e-01,  2.8690e-01, -1.6694e-01],}
\CommentTok{#>         [-1.9219e-01, -1.8828e+00, -9.3559e-01, -7.7169e-01,  1.7327e+00,}
\CommentTok{#>          -3.1115e-01, -2.6599e-01,  1.1501e+00, -2.4615e-01,  2.2717e+00],}
\CommentTok{#>         [ 1.9192e-01,  2.8162e-01, -4.1308e-01, -2.9724e-01,  1.1830e-01,}
\CommentTok{#>           4.1103e-01, -2.0661e-01,  2.8052e-01,  1.4448e-01, -2.1229e-02],}
\CommentTok{#>         [-1.0695e-01, -1.6508e+00, -5.7504e-01,  7.4708e-01,  1.4695e-01,}
\CommentTok{#>          -2.7459e-01, -1.1023e-01,  2.1898e+00, -7.6062e-01,  6.6331e-01],}
\CommentTok{#>         [ 1.7897e-01, -8.4741e-01,  9.4273e-01, -1.4337e+00, -8.1309e-02,}
\CommentTok{#>           2.3699e-01,  5.5836e-01, -6.4794e-01,  1.3851e+00,  1.7151e-01],}
\CommentTok{#>         [-9.5296e-01, -7.7065e-01,  7.5880e-03, -4.1062e-01,  7.7437e-01,}
\CommentTok{#>           1.5579e-01,  3.7150e-02, -4.6469e-02,  5.4424e-01,  8.8268e-01],}
\CommentTok{#>         [-8.5767e-01,  1.4701e-01,  1.1428e+00,  7.9135e-01, -1.5296e-01,}
\CommentTok{#>          -1.5779e-01, -3.0358e-01, -4.6426e-01,  6.7691e-01,  3.2591e-01],}
\CommentTok{#>         [-9.1459e-01, -8.2141e-01,  4.2666e-01, -3.5101e-01,  9.0355e-01,}
\CommentTok{#>          -5.0283e-01, -3.2031e-01,  1.8320e+00,  2.8279e-01,  6.2437e-01],}
\CommentTok{#>         [-1.3905e+00, -6.0861e-01, -6.2662e-01,  5.7454e-01,  5.7814e-01,}
\CommentTok{#>           4.3941e-01,  7.7798e-02, -2.7515e-01,  4.5189e-01,  8.7133e-01],}
\CommentTok{#>         [ 5.6257e-01, -4.4437e-01,  9.6367e-01, -2.6671e-01,  4.1857e-01,}
\CommentTok{#>          -8.0970e-01,  6.7704e-01,  1.6887e-01, -3.3091e-01, -5.0817e-01],}
\CommentTok{#>         [-7.1065e-01, -1.3373e+00,  5.2476e-01, -9.8477e-01,  2.3167e+00,}
\CommentTok{#>          -8.8322e-01, -8.2509e-02,  5.4098e-01,  8.6976e-02,  9.3541e-01],}
\CommentTok{#>         [-1.4806e+00, -4.6107e-01, -2.6849e-01,  2.6974e+00, -4.8143e-01,}
\CommentTok{#>           8.8503e-01, -9.5357e-01, -1.0841e+00,  1.0180e+00,  8.6351e-02],}
\CommentTok{#>         [ 2.4628e+00, -1.5071e+00,  7.3117e-01, -6.4603e-01, -1.4958e+00,}
\CommentTok{#>           3.0829e-01,  4.0907e-01, -2.0519e-01, -1.6792e-01, -4.6206e-01],}
\CommentTok{#>         [ 5.6877e-01, -1.4969e+00, -6.6706e-01, -6.0053e-02, -3.0994e-01,}
\CommentTok{#>          -4.9848e-01, -7.6749e-01,  2.8585e+00, -6.1360e-01,  6.5914e-01],}
\CommentTok{#>         [ 4.4205e+00, -2.8581e+00,  1.0462e+00,  1.5467e-01, -9.9871e-01,}
\CommentTok{#>           1.0504e+00,  9.4292e-02, -1.4378e+00,  5.7136e-01, -1.6437e+00],}
\CommentTok{#>         [ 1.6883e+00, -1.3482e+00,  1.8820e+00,  1.2426e+00, -1.2594e+00,}
\CommentTok{#>           2.9173e-01,  3.9851e-01, -1.0084e+00,  1.4115e-01, -1.9422e+00],}
\CommentTok{#>         [-1.4171e+00,  8.9162e-01,  2.0539e-01,  9.5908e-02, -6.4104e-01,}
\CommentTok{#>          -4.3308e-01, -9.6421e-01,  8.9537e-01,  1.1609e+00,  4.8390e-01],}
\CommentTok{#>         [-1.2176e+00,  2.3829e+00, -4.6562e-01,  8.9853e-02, -9.5672e-01,}
\CommentTok{#>           3.1173e-02, -2.4581e-02, -8.8059e-02,  5.9428e-01, -9.2682e-02],}
\CommentTok{#>         [-1.7307e+00,  5.6224e-01, -3.2265e-01, -4.9043e-01,  7.3022e-01,}
\CommentTok{#>          -5.9717e-01, -6.5566e-01,  1.8073e+00,  7.3746e-02,  9.3846e-01],}
\CommentTok{#>         [-4.2901e-01,  1.3826e-01, -3.4844e-01,  2.1697e+00, -1.0114e+00,}
\CommentTok{#>           9.8128e-01, -4.6798e-01, -1.0823e+00,  6.5561e-01, -7.2020e-01],}
\CommentTok{#>         [-8.6825e-01, -3.3111e-01,  7.9404e-01, -6.1261e-01, -3.6129e-02,}
\CommentTok{#>          -6.8388e-01, -2.0317e-02,  1.6777e+00, -3.8377e-01,  5.7952e-01],}
\CommentTok{#>         [-1.6577e+00,  1.0589e+00, -7.0461e-01,  9.3005e-02, -5.5483e-02,}
\CommentTok{#>          -2.9957e-02, -6.6914e-01,  3.7134e-01,  9.6462e-01,  8.6798e-01],}
\CommentTok{#>         [-2.5657e-01, -6.0222e-01, -3.6740e-01, -1.0563e+00,  5.0916e-02,}
\CommentTok{#>          -6.2516e-01, -1.1167e+00,  3.3912e+00,  1.6242e-01,  8.7198e-01],}
\CommentTok{#>         [-6.1038e-01, -1.6467e+00, -1.0599e+00, -1.9766e-01,  8.3050e-01,}
\CommentTok{#>           3.9744e-01, -1.9508e-01,  1.3288e+00, -5.6222e-01,  1.7754e+00],}
\CommentTok{#>         [ 2.2199e-01, -1.7508e+00,  4.8196e-01, -4.2748e-01,  3.1753e-01,}
\CommentTok{#>           3.2987e-01,  2.5157e+00, -4.7985e-01, -1.7763e-01,  4.7502e-03],}
\CommentTok{#>         [-6.0122e-01, -9.8158e-01,  4.0671e+00,  1.2384e-02, -9.1785e-03,}
\CommentTok{#>          -1.1502e+00,  8.5871e-01, -9.3362e-01,  7.1117e-01, -1.0299e+00],}
\CommentTok{#>         [-5.8807e-01, -1.8779e+00, -4.9661e-01, -4.8811e-02,  7.0428e-01,}
\CommentTok{#>          -2.8727e-01, -8.2898e-01,  2.0634e+00, -3.1366e-01,  1.5578e+00],}
\CommentTok{#>         [-7.6150e-01,  1.0849e-01, -3.3634e-01, -5.2216e-01,  5.9955e-01,}
\CommentTok{#>           5.9659e-01, -5.6445e-01, -5.5787e-01,  1.2763e+00,  5.5385e-01],}
\CommentTok{#>         [-1.0205e+00, -2.3503e+00, -7.6701e-01,  1.6570e-01,  3.0343e+00,}
\CommentTok{#>           1.5617e-01, -1.2460e-01, -1.9061e-01,  4.7652e-01,  1.4649e+00],}
\CommentTok{#>         [-1.5438e+00,  4.9600e-01, -1.8081e-01, -7.0795e-01, -1.3971e-01,}
\CommentTok{#>          -9.8486e-01, -1.1418e+00,  2.9865e+00,  1.5504e-01,  1.0458e+00],}
\CommentTok{#>         [ 1.0249e-01, -1.7000e+00, -1.0777e+00,  1.8996e+00, -2.8052e-01,}
\CommentTok{#>           1.0650e+00,  7.0742e-01, -5.8938e-01, -1.1658e-01, -1.5945e-01],}
\CommentTok{#>         [-2.6694e-01, -2.0829e+00,  9.4647e-01, -1.3480e+00,  1.2449e+00,}
\CommentTok{#>          -4.8505e-01,  2.6142e+00,  5.0344e-02, -5.4981e-01,  1.8449e-01],}
\CommentTok{#>         [-1.4871e+00,  2.9607e+00,  4.7902e-01,  5.0073e-03, -5.0540e-01,}
\CommentTok{#>          -3.3616e-01, -2.9949e-01, -2.1439e-01,  5.7632e-01, -4.6274e-01],}
\CommentTok{#>         [ 1.7111e-01, -6.7651e-01, -2.3165e-01,  2.7955e+00, -1.1800e+00,}
\CommentTok{#>           6.9570e-01, -1.4718e+00, -4.3201e-01,  8.1469e-01, -8.1572e-01],}
\CommentTok{#>         [-8.7862e-01, -5.0456e-01,  3.2428e-01, -5.3243e-01,  4.6667e-01,}
\CommentTok{#>           4.3436e-02,  2.7644e+00, -1.2242e+00,  2.5778e-01, -5.7057e-02],}
\CommentTok{#>         [-9.5491e-01,  2.5703e-01,  3.4359e-02, -4.5068e-01,  6.8173e-01,}
\CommentTok{#>          -8.8618e-02, -1.1938e-01, -7.8896e-02,  7.9619e-01,  5.8036e-01],}
\CommentTok{#>         [-9.4097e-01, -7.9686e-01, -8.5962e-01,  2.4931e+00, -1.5386e+00,}
\CommentTok{#>           1.0913e+00, -1.1633e+00,  4.2138e-01,  6.8648e-01,  4.1477e-02],}
\CommentTok{#>         [-2.1038e+00,  1.8299e+00, -3.0775e-01,  2.3694e-01, -5.8992e-01,}
\CommentTok{#>           1.1753e-01,  5.0003e-01, -2.1849e-01,  7.3212e-01, -6.3468e-02],}
\CommentTok{#>         [-6.2806e-01, -3.7262e-01, -3.6970e-01, -1.2948e+00,  2.3200e+00,}
\CommentTok{#>          -4.6523e-01,  4.4164e-01,  2.5105e-01,  9.5926e-02,  1.2765e+00],}
\CommentTok{#>         [-1.0934e+00,  4.9081e-01, -5.4858e-01,  3.7769e-01, -1.8211e-01,}
\CommentTok{#>           2.7351e-01,  5.7915e-02,  1.6241e-01,  2.5341e-01,  3.2979e-01],}
\CommentTok{#>         [-1.7189e+00,  1.2996e+00, -4.9783e-01,  1.2690e-01, -2.8852e-01,}
\CommentTok{#>           5.7689e-03, -1.1065e-01,  8.5996e-01,  3.7666e-01,  4.4925e-01],}
\CommentTok{#>         [ 1.1295e+00, -1.1470e+00,  7.2451e-01, -8.1282e-01, -6.1164e-02,}
\CommentTok{#>           5.1537e-01,  1.9665e+00, -1.3334e+00,  6.0729e-02, -5.3673e-01],}
\CommentTok{#>         [-1.2605e+00, -1.6713e+00,  9.0893e-02, -6.1368e-01,  1.4691e+00,}
\CommentTok{#>          -9.3188e-01, -3.8812e-01,  8.1698e-01,  1.7610e-01,  2.3959e+00]],}
\CommentTok{#>        grad_fn=<AddmmBackward>)}
\KeywordTok{print}\NormalTok{(predicted)}
\CommentTok{#> tensor([8, 9, 0, 1, 2, 7, 4, 8, 6, 7, 1, 0, 2, 2, 9, 4, 7, 8, 4, 7, 8, 6, 1, 1,}
\CommentTok{#>         4, 7, 8, 4, 4, 7, 0, 1, 9, 2, 8, 7, 8, 2, 6, 0, 0, 6, 3, 8, 8, 9, 1, 4,}
\CommentTok{#>         0, 6, 1, 0, 0, 0, 0, 8, 1, 7, 7, 1, 4, 6, 0, 7, 0, 3, 6, 8, 7, 1, 3, 2,}
\CommentTok{#>         4, 4, 4, 2, 6, 4, 1, 7, 2, 6, 6, 0, 1, 2, 8, 4, 5, 6, 7, 8, 4, 0, 1, 2,}
\CommentTok{#>         3, 4, 8, 6])}
\end{Highlighting}
\end{Shaded}

\hypertarget{printing-output-size}{%
\subsection{Printing output size}\label{printing-output-size}}

This produces a 100x10 matrix because each iteration has a batch size of 100 and each prediction across the 10 classes, with the largest number indicating the likely number it is predicting.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Iterate through test dataset}
\NormalTok{iter_test <-}\StringTok{ }\DecValTok{0}
\NormalTok{iter_test_dataset <-}\StringTok{ }\NormalTok{builtins}\OperatorTok{$}\KeywordTok{enumerate}\NormalTok{(test_loader) }\CommentTok{# convert to iterator}
\ControlFlowTok{for}\NormalTok{ (test_obj }\ControlFlowTok{in} \KeywordTok{iterate}\NormalTok{(iter_test_dataset)) \{}
\NormalTok{    iter_test <-}\StringTok{ }\NormalTok{iter_test }\OperatorTok{+}\StringTok{ }\DecValTok{1}
    \CommentTok{# Load images to a Torch Variable}
\NormalTok{    images <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{1}\NormalTok{]]}
\NormalTok{    labels <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{2}\NormalTok{]]}
\NormalTok{    images <-}\StringTok{ }\NormalTok{images}\OperatorTok{$}\KeywordTok{view}\NormalTok{(}\OperatorTok{-}\NormalTok{1L, 28L}\OperatorTok{*}\NormalTok{28L)}\OperatorTok{$}\KeywordTok{requires_grad_}\NormalTok{()}
    
    \CommentTok{# Forward pass only to get logits/output}
\NormalTok{    outputs =}\StringTok{ }\KeywordTok{model}\NormalTok{(images)}
    
    \ControlFlowTok{if}\NormalTok{ (iter_test }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) \{}
        \KeywordTok{print}\NormalTok{(}\StringTok{'OUTPUTS'}\NormalTok{)}
        \KeywordTok{print}\NormalTok{(outputs}\OperatorTok{$}\KeywordTok{size}\NormalTok{())}
\NormalTok{    \}}
    \CommentTok{# Get predictions from the maximum value}
\NormalTok{    .predicted =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{max}\NormalTok{(outputs}\OperatorTok{$}\NormalTok{data, 1L)}
\NormalTok{    predicted <-}\StringTok{ }\NormalTok{.predicted[1L]}
\NormalTok{\}}
\CommentTok{#> [1] "OUTPUTS"}
\CommentTok{#> torch.Size([100, 10])}
\KeywordTok{print}\NormalTok{(predicted}\OperatorTok{$}\KeywordTok{size}\NormalTok{())}
\CommentTok{#> torch.Size([100])}
\end{Highlighting}
\end{Shaded}

\begin{quote}
The \texttt{predicted} and \texttt{output} tensors have the same number of 1D members. It is obvious because \texttt{predicted} is calculated from the \texttt{output} maximum values.
\end{quote}

\hypertarget{printing-one-output}{%
\subsection{Printing one output}\label{printing-one-output}}

This would be a 1x10 matrix where the largest number is what the model thinks the image is. Here we can see that in the tensor, position 7 has the largest number, indicating the model thinks the image is 7.

\begin{verbatim}
number 0: -0.4181
number 1: -1.0784
...
number 7: 2.9352
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Iterate through test dataset}
\NormalTok{iter_test <-}\StringTok{ }\DecValTok{0}
\NormalTok{iter_test_dataset <-}\StringTok{ }\NormalTok{builtins}\OperatorTok{$}\KeywordTok{enumerate}\NormalTok{(test_loader) }\CommentTok{# convert to iterator}
\ControlFlowTok{for}\NormalTok{ (test_obj }\ControlFlowTok{in} \KeywordTok{iterate}\NormalTok{(iter_test_dataset)) \{}
\NormalTok{    iter_test <-}\StringTok{ }\NormalTok{iter_test }\OperatorTok{+}\StringTok{ }\DecValTok{1}
    \CommentTok{# Load images to a Torch Variable}
\NormalTok{    images <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{1}\NormalTok{]]}
\NormalTok{    labels <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{2}\NormalTok{]]}
\NormalTok{    images <-}\StringTok{ }\NormalTok{images}\OperatorTok{$}\KeywordTok{view}\NormalTok{(}\OperatorTok{-}\NormalTok{1L, 28L}\OperatorTok{*}\NormalTok{28L)}\OperatorTok{$}\KeywordTok{requires_grad_}\NormalTok{()}
    
    \CommentTok{# Forward pass only to get logits/output}
\NormalTok{    outputs =}\StringTok{ }\KeywordTok{model}\NormalTok{(images)}
    
    \ControlFlowTok{if}\NormalTok{ (iter_test }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) \{}
        \KeywordTok{print}\NormalTok{(}\StringTok{'OUTPUTS'}\NormalTok{)}
        \KeywordTok{print}\NormalTok{(outputs[}\DecValTok{1}\NormalTok{])    }\CommentTok{# show first tensor of 100}
        \KeywordTok{print}\NormalTok{(outputs[}\DecValTok{99}\NormalTok{])   }\CommentTok{# show last tensor of 100}
\NormalTok{    \}}
    \CommentTok{# Get predictions from the maximum value for 100 tensors}
\NormalTok{    .predicted =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{max}\NormalTok{(outputs}\OperatorTok{$}\NormalTok{data, 1L)}
\NormalTok{    predicted <-}\StringTok{ }\NormalTok{.predicted[1L]}
\NormalTok{\}}
\CommentTok{#> [1] "OUTPUTS"}
\CommentTok{#> Warning in `[.torch.Tensor`(outputs, 1): Incorrect number of dimensions}
\CommentTok{#> supplied. The number of supplied arguments, (not counting any NULL,}
\CommentTok{#> tf$newaxis or np$newaxis) must match thenumber of dimensions in the tensor,}
\CommentTok{#> unless an all_dims() was supplied (this will produce an error in the}
\CommentTok{#> future)}
\CommentTok{#> tensor([-0.2048, -1.3593, -0.3049, -0.1472,  0.0408, -0.5324, -0.9813,  2.8635,}
\CommentTok{#>         -0.4046,  0.9022], grad_fn=<SelectBackward>)}
\CommentTok{#> Warning in `[.torch.Tensor`(outputs, 99): Incorrect number of dimensions}
\CommentTok{#> supplied. The number of supplied arguments, (not counting any NULL,}
\CommentTok{#> tf$newaxis or np$newaxis) must match thenumber of dimensions in the tensor,}
\CommentTok{#> unless an all_dims() was supplied (this will produce an error in the}
\CommentTok{#> future)}
\CommentTok{#> tensor([ 1.1295, -1.1470,  0.7245, -0.8128, -0.0612,  0.5154,  1.9665, -1.3334,}
\CommentTok{#>          0.0607, -0.5367], grad_fn=<SelectBackward>)}
\KeywordTok{print}\NormalTok{(predicted)}
\CommentTok{#> tensor([8, 9, 0, 1, 2, 7, 4, 8, 6, 7, 1, 0, 2, 2, 9, 4, 7, 8, 4, 7, 8, 6, 1, 1,}
\CommentTok{#>         4, 7, 8, 4, 4, 7, 0, 1, 9, 2, 8, 7, 8, 2, 6, 0, 0, 6, 3, 8, 8, 9, 1, 4,}
\CommentTok{#>         0, 6, 1, 0, 0, 0, 0, 8, 1, 7, 7, 1, 4, 6, 0, 7, 0, 3, 6, 8, 7, 1, 3, 2,}
\CommentTok{#>         4, 4, 4, 2, 6, 4, 1, 7, 2, 6, 6, 0, 1, 2, 8, 4, 5, 6, 7, 8, 4, 0, 1, 2,}
\CommentTok{#>         3, 4, 8, 6])}
\end{Highlighting}
\end{Shaded}

\hypertarget{printing-prediction-output}{%
\subsection{Printing prediction output}\label{printing-prediction-output}}

Because our output is of size 100 (our batch size), our prediction size would also of the size 100.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Iterate through test dataset}
\NormalTok{iter_test <-}\StringTok{ }\DecValTok{0}
\NormalTok{iter_test_dataset <-}\StringTok{ }\NormalTok{builtins}\OperatorTok{$}\KeywordTok{enumerate}\NormalTok{(test_loader) }\CommentTok{# convert to iterator}
\ControlFlowTok{for}\NormalTok{ (test_obj }\ControlFlowTok{in} \KeywordTok{iterate}\NormalTok{(iter_test_dataset)) \{}
\NormalTok{    iter_test <-}\StringTok{ }\NormalTok{iter_test }\OperatorTok{+}\StringTok{ }\DecValTok{1}
    \CommentTok{# Load images to a Torch Variable}
\NormalTok{    images <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{1}\NormalTok{]]}
\NormalTok{    labels <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{2}\NormalTok{]]}
\NormalTok{    images <-}\StringTok{ }\NormalTok{images}\OperatorTok{$}\KeywordTok{view}\NormalTok{(}\OperatorTok{-}\NormalTok{1L, 28L}\OperatorTok{*}\NormalTok{28L)}\OperatorTok{$}\KeywordTok{requires_grad_}\NormalTok{()}
    
    \CommentTok{# Forward pass only to get logits/output}
\NormalTok{    outputs =}\StringTok{ }\KeywordTok{model}\NormalTok{(images)}
    
    \CommentTok{# Get predictions from the maximum value for a batch}
\NormalTok{    .predicted =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{max}\NormalTok{(outputs}\OperatorTok{$}\NormalTok{data, 1L)}
\NormalTok{    predicted <-}\StringTok{ }\NormalTok{.predicted[1L]}
    
    \ControlFlowTok{if}\NormalTok{ (iter_test }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) \{}
        \KeywordTok{print}\NormalTok{(}\StringTok{'PREDICTION'}\NormalTok{)}
        \KeywordTok{print}\NormalTok{(predicted}\OperatorTok{$}\KeywordTok{size}\NormalTok{())}
\NormalTok{    \}}
\NormalTok{\}}
\CommentTok{#> [1] "PREDICTION"}
\CommentTok{#> torch.Size([100])}
\end{Highlighting}
\end{Shaded}

\hypertarget{print-prediction-value}{%
\subsection{Print prediction value}\label{print-prediction-value}}

We are printing our prediction which as verified above, should be digit 7.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Iterate through test dataset}
\NormalTok{iter_test <-}\StringTok{ }\DecValTok{0}
\NormalTok{iter_test_dataset <-}\StringTok{ }\NormalTok{builtins}\OperatorTok{$}\KeywordTok{enumerate}\NormalTok{(test_loader) }\CommentTok{# convert to iterator}
\ControlFlowTok{for}\NormalTok{ (test_obj }\ControlFlowTok{in} \KeywordTok{iterate}\NormalTok{(iter_test_dataset)) \{}
\NormalTok{    iter_test <-}\StringTok{ }\NormalTok{iter_test }\OperatorTok{+}\StringTok{ }\DecValTok{1}
    \CommentTok{# Load images to a Torch Variable}
\NormalTok{    images <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{1}\NormalTok{]]}
\NormalTok{    labels <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{2}\NormalTok{]]}
\NormalTok{    images <-}\StringTok{ }\NormalTok{images}\OperatorTok{$}\KeywordTok{view}\NormalTok{(}\OperatorTok{-}\NormalTok{1L, 28L}\OperatorTok{*}\NormalTok{28L)}\OperatorTok{$}\KeywordTok{requires_grad_}\NormalTok{()}
    
    \CommentTok{# Forward pass only to get logits/output}
\NormalTok{    outputs =}\StringTok{ }\KeywordTok{model}\NormalTok{(images)}
    
    \CommentTok{# Get predictions from the maximum value}
\NormalTok{    .predicted =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{max}\NormalTok{(outputs}\OperatorTok{$}\NormalTok{data, 1L)}
\NormalTok{    predicted <-}\StringTok{ }\NormalTok{.predicted[1L]}
    
    \ControlFlowTok{if}\NormalTok{ (iter_test }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) \{}
        \KeywordTok{print}\NormalTok{(}\StringTok{'PREDICTION'}\NormalTok{)}
        \KeywordTok{print}\NormalTok{(predicted[1L])}
\NormalTok{    \}}
\NormalTok{\}}
\CommentTok{#> [1] "PREDICTION"}
\CommentTok{#> tensor(7)}
\end{Highlighting}
\end{Shaded}

\hypertarget{print-prediction-label-and-label-size}{%
\subsection{Print prediction, label and label size}\label{print-prediction-label-and-label-size}}

We are trying to show what we are predicting and the actual values. In this case, we're predicting the right value 7!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Iterate through test dataset}
\NormalTok{iter_test <-}\StringTok{ }\DecValTok{0}
\NormalTok{iter_test_dataset <-}\StringTok{ }\NormalTok{builtins}\OperatorTok{$}\KeywordTok{enumerate}\NormalTok{(test_loader) }\CommentTok{# convert to iterator}
\ControlFlowTok{for}\NormalTok{ (test_obj }\ControlFlowTok{in} \KeywordTok{iterate}\NormalTok{(iter_test_dataset)) \{}
\NormalTok{    iter_test <-}\StringTok{ }\NormalTok{iter_test }\OperatorTok{+}\StringTok{ }\DecValTok{1}
    \CommentTok{# Load images to a Torch Variable}
\NormalTok{    images <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{1}\NormalTok{]]}
\NormalTok{    labels <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{2}\NormalTok{]]}
\NormalTok{    images <-}\StringTok{ }\NormalTok{images}\OperatorTok{$}\KeywordTok{view}\NormalTok{(}\OperatorTok{-}\NormalTok{1L, 28L}\OperatorTok{*}\NormalTok{28L)}\OperatorTok{$}\KeywordTok{requires_grad_}\NormalTok{()}
    \CommentTok{# Forward pass only to get logits/output}
\NormalTok{    outputs =}\StringTok{ }\KeywordTok{model}\NormalTok{(images)}
    \CommentTok{# Get predictions from the maximum value}
\NormalTok{    .predicted =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{max}\NormalTok{(outputs}\OperatorTok{$}\NormalTok{data, 1L)}
\NormalTok{    predicted <-}\StringTok{ }\NormalTok{.predicted[1L]}
    
    \ControlFlowTok{if}\NormalTok{ (iter_test }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) \{}
        \KeywordTok{print}\NormalTok{(}\StringTok{'PREDICTION'}\NormalTok{)}
        \KeywordTok{print}\NormalTok{(predicted[}\DecValTok{1}\NormalTok{])}
        
        \KeywordTok{print}\NormalTok{(}\StringTok{'LABEL SIZE'}\NormalTok{)}
        \KeywordTok{print}\NormalTok{(labels}\OperatorTok{$}\KeywordTok{size}\NormalTok{())}

        \KeywordTok{print}\NormalTok{(}\StringTok{'LABEL FOR IMAGE 0'}\NormalTok{)}
        \KeywordTok{print}\NormalTok{(labels[}\DecValTok{1}\NormalTok{]}\OperatorTok{$}\KeywordTok{item}\NormalTok{())  }\CommentTok{# extract the scalar part only}
\NormalTok{    \}}
\NormalTok{\}}
\CommentTok{#> [1] "PREDICTION"}
\CommentTok{#> tensor(7)}
\CommentTok{#> [1] "LABEL SIZE"}
\CommentTok{#> torch.Size([100])}
\CommentTok{#> [1] "LABEL FOR IMAGE 0"}
\CommentTok{#> [1] 7}
\end{Highlighting}
\end{Shaded}

\hypertarget{print-second-prediction-and-ground-truth}{%
\subsection{Print second prediction and ground truth}\label{print-second-prediction-and-ground-truth}}

It should be the digit 2.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Iterate through test dataset}
\NormalTok{iter_test <-}\StringTok{ }\DecValTok{0}
\NormalTok{iter_test_dataset <-}\StringTok{ }\NormalTok{builtins}\OperatorTok{$}\KeywordTok{enumerate}\NormalTok{(test_loader) }\CommentTok{# convert to iterator}
\ControlFlowTok{for}\NormalTok{ (test_obj }\ControlFlowTok{in} \KeywordTok{iterate}\NormalTok{(iter_test_dataset)) \{}
\NormalTok{    iter_test <-}\StringTok{ }\NormalTok{iter_test }\OperatorTok{+}\StringTok{ }\DecValTok{1}
    \CommentTok{# Load images to a Torch Variable}
\NormalTok{    images <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{1}\NormalTok{]]}
\NormalTok{    labels <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{2}\NormalTok{]]}
\NormalTok{    images <-}\StringTok{ }\NormalTok{images}\OperatorTok{$}\KeywordTok{view}\NormalTok{(}\OperatorTok{-}\NormalTok{1L, 28L}\OperatorTok{*}\NormalTok{28L)}\OperatorTok{$}\KeywordTok{requires_grad_}\NormalTok{()}
    \CommentTok{# Forward pass only to get logits/output}
\NormalTok{    outputs =}\StringTok{ }\KeywordTok{model}\NormalTok{(images)}
    \CommentTok{# Get predictions from the maximum value}
\NormalTok{    .predicted =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{max}\NormalTok{(outputs}\OperatorTok{$}\NormalTok{data, 1L)}
\NormalTok{    predicted <-}\StringTok{ }\NormalTok{.predicted[1L]}
    
    \ControlFlowTok{if}\NormalTok{ (iter_test }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) \{}
        \KeywordTok{print}\NormalTok{(}\StringTok{'PREDICTION'}\NormalTok{)}
        \KeywordTok{print}\NormalTok{(predicted[}\DecValTok{1}\NormalTok{])}
        
        \KeywordTok{print}\NormalTok{(}\StringTok{'LABEL SIZE'}\NormalTok{)}
        \KeywordTok{print}\NormalTok{(labels}\OperatorTok{$}\KeywordTok{size}\NormalTok{())}

        \KeywordTok{print}\NormalTok{(}\StringTok{'LABEL FOR IMAGE 0'}\NormalTok{)}
        \KeywordTok{print}\NormalTok{(labels[}\DecValTok{1}\NormalTok{]}\OperatorTok{$}\KeywordTok{item}\NormalTok{())}
\NormalTok{    \}}
\NormalTok{\}}
\CommentTok{#> [1] "PREDICTION"}
\CommentTok{#> tensor(7)}
\CommentTok{#> [1] "LABEL SIZE"}
\CommentTok{#> torch.Size([100])}
\CommentTok{#> [1] "LABEL FOR IMAGE 0"}
\CommentTok{#> [1] 7}
\end{Highlighting}
\end{Shaded}

\hypertarget{print-accuracy}{%
\subsection{Print accuracy}\label{print-accuracy}}

Now we know what each object represents, we can understand how we arrived at our accuracy numbers.

One last thing to note is that \texttt{correct.item()} has this syntax is because correct is a PyTorch tensor and to get the value to compute with total which is an integer, we need to do this.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Iterate through test dataset}
\NormalTok{iter_test <-}\StringTok{ }\DecValTok{0}
\NormalTok{iter_test_dataset <-}\StringTok{ }\NormalTok{builtins}\OperatorTok{$}\KeywordTok{enumerate}\NormalTok{(test_loader) }\CommentTok{# convert to iterator}
\ControlFlowTok{for}\NormalTok{ (test_obj }\ControlFlowTok{in} \KeywordTok{iterate}\NormalTok{(iter_test_dataset)) \{}
\NormalTok{    iter_test <-}\StringTok{ }\NormalTok{iter_test }\OperatorTok{+}\StringTok{ }\DecValTok{1}
    \CommentTok{# Load images to a Torch Variable}
\NormalTok{    images <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{1}\NormalTok{]]}
\NormalTok{    labels <-}\StringTok{ }\NormalTok{test_obj[[}\DecValTok{2}\NormalTok{]][[}\DecValTok{2}\NormalTok{]]}
\NormalTok{    images <-}\StringTok{ }\NormalTok{images}\OperatorTok{$}\KeywordTok{view}\NormalTok{(}\OperatorTok{-}\NormalTok{1L, 28L}\OperatorTok{*}\NormalTok{28L)}\OperatorTok{$}\KeywordTok{requires_grad_}\NormalTok{()}
    \CommentTok{# Forward pass only to get logits/output}
\NormalTok{    outputs =}\StringTok{ }\KeywordTok{model}\NormalTok{(images)}
    \CommentTok{# Get predictions from the maximum value}
\NormalTok{    .predicted =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{max}\NormalTok{(outputs}\OperatorTok{$}\NormalTok{data, 1L)}
\NormalTok{    predicted <-}\StringTok{ }\NormalTok{.predicted[1L]}
    \CommentTok{# Total number of labels}
\NormalTok{    total =}\StringTok{ }\NormalTok{total }\OperatorTok{+}\StringTok{ }\NormalTok{labels}\OperatorTok{$}\KeywordTok{size}\NormalTok{(0L)}
    \CommentTok{# Total correct predictions}
\NormalTok{    correct =}\StringTok{ }\NormalTok{correct }\OperatorTok{+}\StringTok{ }\KeywordTok{sum}\NormalTok{((predicted}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{() }\OperatorTok{==}\StringTok{ }\NormalTok{labels}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{()))}
\NormalTok{\}}
\NormalTok{accuracy =}\StringTok{ }\DecValTok{100} \OperatorTok{*}\StringTok{ }\NormalTok{correct }\OperatorTok{/}\StringTok{ }\NormalTok{total}
\KeywordTok{print}\NormalTok{(accuracy)}
\CommentTok{#> [1] 83.4}
\end{Highlighting}
\end{Shaded}

\hypertarget{saving-pytorch-model}{%
\section{Saving PyTorch model}\label{saving-pytorch-model}}

This is how you save your model.
Feel free to just change \texttt{save\_model\ =\ TRUE} to save your model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{save_model =}\StringTok{ }\OtherTok{TRUE}
\ControlFlowTok{if}\NormalTok{ (save_model) \{}
    \CommentTok{# Saves only parameters}
\NormalTok{    torch}\OperatorTok{$}\KeywordTok{save}\NormalTok{(model}\OperatorTok{$}\KeywordTok{state_dict}\NormalTok{(), }\StringTok{'awesome_model.pkl'}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{part-pytorch-and-r-data-structures}{%
\part{PyTorch and R data structures}\label{part-pytorch-and-r-data-structures}}

\hypertarget{working-with-data.frame}{%
\chapter{Working with data.frame}\label{working-with-data.frame}}

\hypertarget{load-pytorch-libraries}{%
\section{Load PyTorch libraries}\label{load-pytorch-libraries}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rTorch)}

\NormalTok{torch       <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torch"}\NormalTok{)}
\NormalTok{torchvision <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torchvision"}\NormalTok{)}
\NormalTok{nn          <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torch.nn"}\NormalTok{)}
\NormalTok{transforms  <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torchvision.transforms"}\NormalTok{)}
\NormalTok{dsets       <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torchvision.datasets"}\NormalTok{)}
\NormalTok{builtins    <-}\StringTok{ }\KeywordTok{import_builtins}\NormalTok{()}
\NormalTok{np          <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"numpy"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{dataset-iteration-batch-settings}{%
\section{Dataset iteration batch settings}\label{dataset-iteration-batch-settings}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# folders where the images are located}
\NormalTok{train_data_path =}\StringTok{ '~/mnist_png_full/training/'}
\NormalTok{test_data_path  =}\StringTok{ '~/mnist_png_full/testing/'}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read the datasets without normalization}
\NormalTok{train_dataset =}\StringTok{ }\NormalTok{torchvision}\OperatorTok{$}\NormalTok{datasets}\OperatorTok{$}\KeywordTok{ImageFolder}\NormalTok{(}\DataTypeTok{root =}\NormalTok{ train_data_path, }
    \DataTypeTok{transform =}\NormalTok{ torchvision}\OperatorTok{$}\NormalTok{transforms}\OperatorTok{$}\KeywordTok{ToTensor}\NormalTok{()}
\NormalTok{)}

\KeywordTok{print}\NormalTok{(train_dataset)}
\CommentTok{#> Dataset ImageFolder}
\CommentTok{#>     Number of datapoints: 60000}
\CommentTok{#>     Root location: /home/msfz751/mnist_png_full/training/}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-statistics-for-tensors}{%
\section{Summary statistics for tensors}\label{summary-statistics-for-tensors}}

\hypertarget{using-data.frame}{%
\section{\texorpdfstring{using \texttt{data.frame}}{using data.frame}}\label{using-data.frame}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tictoc)}
\KeywordTok{tic}\NormalTok{()}

\NormalTok{fun_list <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
    \DataTypeTok{size  =} \KeywordTok{c}\NormalTok{(}\StringTok{"size"}\NormalTok{),}
    \DataTypeTok{numel =} \KeywordTok{c}\NormalTok{(}\StringTok{"numel"}\NormalTok{),}
    \DataTypeTok{sum   =} \KeywordTok{c}\NormalTok{(}\StringTok{"sum"}\NormalTok{,    }\StringTok{"item"}\NormalTok{),}
    \DataTypeTok{mean  =} \KeywordTok{c}\NormalTok{(}\StringTok{"mean"}\NormalTok{,   }\StringTok{"item"}\NormalTok{),}
    \DataTypeTok{std   =} \KeywordTok{c}\NormalTok{(}\StringTok{"std"}\NormalTok{,    }\StringTok{"item"}\NormalTok{),}
    \DataTypeTok{med   =} \KeywordTok{c}\NormalTok{(}\StringTok{"median"}\NormalTok{, }\StringTok{"item"}\NormalTok{),}
    \DataTypeTok{max   =} \KeywordTok{c}\NormalTok{(}\StringTok{"max"}\NormalTok{,    }\StringTok{"item"}\NormalTok{),}
    \DataTypeTok{min   =} \KeywordTok{c}\NormalTok{(}\StringTok{"min"}\NormalTok{,    }\StringTok{"item"}\NormalTok{)}
\NormalTok{    )}

\NormalTok{idx <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(0L, 599L)    }\CommentTok{# how many samples}

\NormalTok{fun_get_tensor <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{py_get_item}\NormalTok{(train_dataset, x)[[}\DecValTok{1}\NormalTok{]]}

\NormalTok{stat_fun <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, str_fun) \{}
\NormalTok{  fun_var <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{"fun_get_tensor(x)"}\NormalTok{, }\StringTok{"$"}\NormalTok{, str_fun, }\StringTok{"()"}\NormalTok{)}
  \KeywordTok{sapply}\NormalTok{(idx, }\ControlFlowTok{function}\NormalTok{(x) }
    \KeywordTok{ifelse}\NormalTok{(}\KeywordTok{is.numeric}\NormalTok{(}\KeywordTok{eval}\NormalTok{(}\KeywordTok{parse}\NormalTok{(}\DataTypeTok{text =}\NormalTok{ fun_var))),  }\CommentTok{# size return chracater}
           \KeywordTok{eval}\NormalTok{(}\KeywordTok{parse}\NormalTok{(}\DataTypeTok{text =}\NormalTok{ fun_var)),              }\CommentTok{# all else are numeric}
           \KeywordTok{as.character}\NormalTok{(}\KeywordTok{eval}\NormalTok{(}\KeywordTok{parse}\NormalTok{(}\DataTypeTok{text =}\NormalTok{ fun_var)))))}
\NormalTok{\}  }

\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{ridx =}\NormalTok{ idx}\OperatorTok{+}\DecValTok{1}\NormalTok{,      }\CommentTok{# index number for the sample}
  \KeywordTok{do.call}\NormalTok{(data.frame, }
          \KeywordTok{lapply}\NormalTok{(}
              \KeywordTok{sapply}\NormalTok{(fun_list, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{paste}\NormalTok{(x, }\DataTypeTok{collapse =} \StringTok{"()$"}\NormalTok{)), }
              \ControlFlowTok{function}\NormalTok{(y) }\KeywordTok{stat_fun}\NormalTok{(}\DecValTok{1}\NormalTok{, y)}
\NormalTok{          )}
\NormalTok{  )}
\NormalTok{)}
\KeywordTok{head}\NormalTok{(df)}
\CommentTok{#>   ridx                    size numel sum  mean   std med max min}
\CommentTok{#> 1    1 torch.Size([3, 28, 28])  2352 366 0.156 0.329   0   1   0}
\CommentTok{#> 2    2 torch.Size([3, 28, 28])  2352 284 0.121 0.297   0   1   0}
\CommentTok{#> 3    3 torch.Size([3, 28, 28])  2352 645 0.274 0.420   0   1   0}
\CommentTok{#> 4    4 torch.Size([3, 28, 28])  2352 410 0.174 0.355   0   1   0}
\CommentTok{#> 5    5 torch.Size([3, 28, 28])  2352 321 0.137 0.312   0   1   0}
\CommentTok{#> 6    6 torch.Size([3, 28, 28])  2352 654 0.278 0.421   0   1   0}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 13.016 sec elapsed}
\CommentTok{# 59    1.663s}
\CommentTok{#   599  13.5s}
\CommentTok{#  5999  54.321 sec; 137.6s}
\CommentTok{# 59999 553.489 sec elapsed}
\end{Highlighting}
\end{Shaded}

\hypertarget{working-with-data.table}{%
\chapter{Working with data.table}\label{working-with-data.table}}

\hypertarget{load-pytorch-libraries-1}{%
\section{Load PyTorch libraries}\label{load-pytorch-libraries-1}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rTorch)}

\NormalTok{torch       <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torch"}\NormalTok{)}
\NormalTok{torchvision <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torchvision"}\NormalTok{)}
\NormalTok{nn          <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torch.nn"}\NormalTok{)}
\NormalTok{transforms  <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torchvision.transforms"}\NormalTok{)}
\NormalTok{dsets       <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torchvision.datasets"}\NormalTok{)}
\NormalTok{builtins    <-}\StringTok{ }\KeywordTok{import_builtins}\NormalTok{()}
\NormalTok{np          <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"numpy"}\NormalTok{)}

\CommentTok{## Dataset iteration batch settings}
\CommentTok{# folders where the images are located}
\NormalTok{train_data_path =}\StringTok{ '~/mnist_png_full/training/'}
\NormalTok{test_data_path  =}\StringTok{ '~/mnist_png_full/testing/'}

\CommentTok{# read the datasets without normalization}
\NormalTok{train_dataset =}\StringTok{ }\NormalTok{torchvision}\OperatorTok{$}\NormalTok{datasets}\OperatorTok{$}\KeywordTok{ImageFolder}\NormalTok{(}\DataTypeTok{root =}\NormalTok{ train_data_path, }
    \DataTypeTok{transform =}\NormalTok{ torchvision}\OperatorTok{$}\NormalTok{transforms}\OperatorTok{$}\KeywordTok{ToTensor}\NormalTok{()}
\NormalTok{)}

\KeywordTok{print}\NormalTok{(train_dataset)}
\CommentTok{#> Dataset ImageFolder}
\CommentTok{#>     Number of datapoints: 60000}
\CommentTok{#>     Root location: /home/msfz751/mnist_png_full/training/}
\end{Highlighting}
\end{Shaded}

\hypertarget{using-data.table}{%
\subsection{Using `data.table}\label{using-data.table}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(data.table)}
\KeywordTok{library}\NormalTok{(tictoc)}
\KeywordTok{tic}\NormalTok{()}

\NormalTok{fun_list <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
    \DataTypeTok{numel =} \KeywordTok{c}\NormalTok{(}\StringTok{"numel"}\NormalTok{),}
    \DataTypeTok{sum   =} \KeywordTok{c}\NormalTok{(}\StringTok{"sum"}\NormalTok{,    }\StringTok{"item"}\NormalTok{),}
    \DataTypeTok{mean  =} \KeywordTok{c}\NormalTok{(}\StringTok{"mean"}\NormalTok{,   }\StringTok{"item"}\NormalTok{),}
    \DataTypeTok{std   =} \KeywordTok{c}\NormalTok{(}\StringTok{"std"}\NormalTok{,    }\StringTok{"item"}\NormalTok{),}
    \DataTypeTok{med   =} \KeywordTok{c}\NormalTok{(}\StringTok{"median"}\NormalTok{, }\StringTok{"item"}\NormalTok{),}
    \DataTypeTok{max   =} \KeywordTok{c}\NormalTok{(}\StringTok{"max"}\NormalTok{,    }\StringTok{"item"}\NormalTok{),}
    \DataTypeTok{min   =} \KeywordTok{c}\NormalTok{(}\StringTok{"min"}\NormalTok{,    }\StringTok{"item"}\NormalTok{)}
\NormalTok{    )}

\NormalTok{idx <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(0L, 5999L)}

\NormalTok{fun_get_tensor <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{py_get_item}\NormalTok{(train_dataset, x)[[}\DecValTok{1}\NormalTok{]]}

\NormalTok{stat_fun <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, str_fun) \{}
\NormalTok{  fun_var <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{"fun_get_tensor(x)"}\NormalTok{, }\StringTok{"$"}\NormalTok{, str_fun, }\StringTok{"()"}\NormalTok{)}
  \KeywordTok{sapply}\NormalTok{(idx, }\ControlFlowTok{function}\NormalTok{(x) }
    \KeywordTok{ifelse}\NormalTok{(}\KeywordTok{is.numeric}\NormalTok{(}\KeywordTok{eval}\NormalTok{(}\KeywordTok{parse}\NormalTok{(}\DataTypeTok{text =}\NormalTok{ fun_var))),  }\CommentTok{# size return chracater}
           \KeywordTok{eval}\NormalTok{(}\KeywordTok{parse}\NormalTok{(}\DataTypeTok{text =}\NormalTok{ fun_var)),              }\CommentTok{# all else are numeric}
           \KeywordTok{as.character}\NormalTok{(}\KeywordTok{eval}\NormalTok{(}\KeywordTok{parse}\NormalTok{(}\DataTypeTok{text =}\NormalTok{ fun_var)))))}
\NormalTok{\}  }

\NormalTok{dt <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\DataTypeTok{ridx =}\NormalTok{ idx}\OperatorTok{+}\DecValTok{1}\NormalTok{,}
  \KeywordTok{do.call}\NormalTok{(data.table, }
          \KeywordTok{lapply}\NormalTok{(}
            \KeywordTok{sapply}\NormalTok{(fun_list, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{paste}\NormalTok{(x, }\DataTypeTok{collapse =} \StringTok{"()$"}\NormalTok{)), }
            \ControlFlowTok{function}\NormalTok{(y) }\KeywordTok{stat_fun}\NormalTok{(}\DecValTok{1}\NormalTok{, y)}
\NormalTok{          )}
\NormalTok{  )}
\NormalTok{)}

\KeywordTok{head}\NormalTok{(dt)}
\CommentTok{#>    ridx numel sum  mean   std med max min}
\CommentTok{#> 1:    1  2352 366 0.156 0.329   0   1   0}
\CommentTok{#> 2:    2  2352 284 0.121 0.297   0   1   0}
\CommentTok{#> 3:    3  2352 645 0.274 0.420   0   1   0}
\CommentTok{#> 4:    4  2352 410 0.174 0.355   0   1   0}
\CommentTok{#> 5:    5  2352 321 0.137 0.312   0   1   0}
\CommentTok{#> 6:    6  2352 654 0.278 0.421   0   1   0}
\KeywordTok{toc}\NormalTok{()}
\CommentTok{#> 105.771 sec elapsed}

\CommentTok{#   60   1.266 sec elapsed}
\CommentTok{#  600   11.798 sec elapsed; 12.9s}
\CommentTok{# 6000  119.256 sec elapsed; 132.9s}
\CommentTok{# 1117.619 sec elapsed}
\end{Highlighting}
\end{Shaded}

\hypertarget{part-pytorch-with-rmarkdown}{%
\part{PyTorch with Rmarkdown}\label{part-pytorch-with-rmarkdown}}

\hypertarget{simple-regression-with-pytorch}{%
\chapter{Simple Regression with PyTorch}\label{simple-regression-with-pytorch}}

This examples combine Python and R code together.

Source: \url{https://www.guru99.com/pytorch-tutorial.html}

\hypertarget{creating-the-network-model}{%
\section{Creating the network model}\label{creating-the-network-model}}

Our network model is a simple Linear layer with an input and an output shape of 1.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rTorch)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ __future__ }\ImportTok{import}\NormalTok{ print_function}

\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ torch.nn.functional }\ImportTok{as}\NormalTok{ F}
\ImportTok{from}\NormalTok{ torch.autograd }\ImportTok{import}\NormalTok{ Variable}

\NormalTok{torch.manual_seed(}\DecValTok{123}\NormalTok{)}
\CommentTok{#> <torch._C.Generator object at 0x7fae1a382a90>}
\KeywordTok{class}\NormalTok{ Net(nn.Module):}
   \KeywordTok{def} \FunctionTok{__init__}\NormalTok{(}\VariableTok{self}\NormalTok{):}
       \BuiltInTok{super}\NormalTok{(Net, }\VariableTok{self}\NormalTok{).}\FunctionTok{__init__}\NormalTok{()}
       \VariableTok{self}\NormalTok{.layer }\OperatorTok{=}\NormalTok{ torch.nn.Linear(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}

   \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{       x }\OperatorTok{=} \VariableTok{self}\NormalTok{.layer(x)      }
       \ControlFlowTok{return}\NormalTok{ x}

\NormalTok{net }\OperatorTok{=}\NormalTok{ Net()}
\BuiltInTok{print}\NormalTok{(net)}
\CommentTok{#> Net(}
\CommentTok{#>   (layer): Linear(in_features=1, out_features=1, bias=True)}
\CommentTok{#> )}
\end{Highlighting}
\end{Shaded}

And the network output should be like this

\begin{verbatim}
Net(
  (hidden): Linear(in_features=1, out_features=1, bias=True)
)
\end{verbatim}

\hypertarget{code-in-r-1}{%
\subsection{Code in R}\label{code-in-r-1}}

This would be the equivalent code in R:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(reticulate)}
\CommentTok{#> }
\CommentTok{#> Attaching package: 'reticulate'}
\CommentTok{#> The following objects are masked from 'package:rTorch':}
\CommentTok{#> }
\CommentTok{#>     conda_install, conda_python}

\NormalTok{torch <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torch"}\NormalTok{)}
\NormalTok{nn    <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torch.nn"}\NormalTok{)}
\NormalTok{Variable <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torch.autograd"}\NormalTok{)}\OperatorTok{$}\NormalTok{Variable}

\NormalTok{torch}\OperatorTok{$}\KeywordTok{manual_seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{#> <torch._C.Generator>}

\NormalTok{main =}\StringTok{ }\KeywordTok{py_run_string}\NormalTok{(}
\StringTok{"}
\StringTok{import torch.nn as nn}

\StringTok{class Net(nn.Module):}
\StringTok{   def __init__(self):}
\StringTok{       super(Net, self).__init__()}
\StringTok{       self.layer = torch.nn.Linear(1, 1)}

\StringTok{   def forward(self, x):}
\StringTok{       x = self.layer(x)      }
\StringTok{       return x}
\StringTok{"}\NormalTok{)}


\CommentTok{# build a Linear Rgression model}
\NormalTok{net <-}\StringTok{ }\NormalTok{main}\OperatorTok{$}\KeywordTok{Net}\NormalTok{()}
\KeywordTok{print}\NormalTok{(net)}
\CommentTok{#> Net(}
\CommentTok{#>   (layer): Linear(in_features=1, out_features=1, bias=True)}
\CommentTok{#> )}
\end{Highlighting}
\end{Shaded}

\hypertarget{datasets}{%
\section{Datasets}\label{datasets}}

Before you start the training process, you need to know our data. You make a random function to test our model. \(Y = x3 sin(x)+ 3x+0.8 rand(100)\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Visualize our data}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{np.random.seed(}\DecValTok{123}\NormalTok{)}

\NormalTok{x }\OperatorTok{=}\NormalTok{ np.random.rand(}\DecValTok{100}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ np.sin(x) }\OperatorTok{*}\NormalTok{ np.power(x,}\DecValTok{3}\NormalTok{) }\OperatorTok{+} \DecValTok{3}\OperatorTok{*}\NormalTok{x }\OperatorTok{+}\NormalTok{ np.random.rand(}\DecValTok{100}\NormalTok{)}\OperatorTok{*}\FloatTok{0.8}

\NormalTok{plt.scatter(x, y)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-datasets-1}

This is the code in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np    <-}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"numpy"}\NormalTok{)}

\NormalTok{np}\OperatorTok{$}\NormalTok{random}\OperatorTok{$}\KeywordTok{seed}\NormalTok{(123L)}

\NormalTok{x =}\StringTok{ }\NormalTok{np}\OperatorTok{$}\NormalTok{random}\OperatorTok{$}\KeywordTok{rand}\NormalTok{(100L)}
\NormalTok{y =}\StringTok{ }\NormalTok{np}\OperatorTok{$}\KeywordTok{sin}\NormalTok{(x) }\OperatorTok{*}\StringTok{ }\NormalTok{np}\OperatorTok{$}\KeywordTok{power}\NormalTok{(x, 3L) }\OperatorTok{+}\StringTok{ }\DecValTok{3}\OperatorTok{*}\NormalTok{x }\OperatorTok{+}\StringTok{ }\NormalTok{np}\OperatorTok{$}\NormalTok{random}\OperatorTok{$}\KeywordTok{rand}\NormalTok{(100L)}\OperatorTok{*}\FloatTok{0.8}

\KeywordTok{plot}\NormalTok{(x, y)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/r-datasets-1} \end{center}

Before you start the training process, you need to convert the numpy array to Variables that supported by Torch and autograd.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert numpy array to tensor in shape of input size}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.from_numpy(x.reshape(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)).}\BuiltInTok{float}\NormalTok{()}
\NormalTok{y }\OperatorTok{=}\NormalTok{ torch.from_numpy(y.reshape(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)).}\BuiltInTok{float}\NormalTok{()}
\BuiltInTok{print}\NormalTok{(x, y)}
\CommentTok{#> tensor([[0.6965],}
\CommentTok{#>         [0.2861],}
\CommentTok{#>         [0.2269],}
\CommentTok{#>         [0.5513],}
\CommentTok{#>         [0.7195],}
\CommentTok{#>         [0.4231],}
\CommentTok{#>         [0.9808],}
\CommentTok{#>         [0.6848],}
\CommentTok{#>         [0.4809],}
\CommentTok{#>         [0.3921],}
\CommentTok{#>         [0.3432],}
\CommentTok{#>         [0.7290],}
\CommentTok{#>         [0.4386],}
\CommentTok{#>         [0.0597],}
\CommentTok{#>         [0.3980],}
\CommentTok{#>         [0.7380],}
\CommentTok{#>         [0.1825],}
\CommentTok{#>         [0.1755],}
\CommentTok{#>         [0.5316],}
\CommentTok{#>         [0.5318],}
\CommentTok{#>         [0.6344],}
\CommentTok{#>         [0.8494],}
\CommentTok{#>         [0.7245],}
\CommentTok{#>         [0.6110],}
\CommentTok{#>         [0.7224],}
\CommentTok{#>         [0.3230],}
\CommentTok{#>         [0.3618],}
\CommentTok{#>         [0.2283],}
\CommentTok{#>         [0.2937],}
\CommentTok{#>         [0.6310],}
\CommentTok{#>         [0.0921],}
\CommentTok{#>         [0.4337],}
\CommentTok{#>         [0.4309],}
\CommentTok{#>         [0.4937],}
\CommentTok{#>         [0.4258],}
\CommentTok{#>         [0.3123],}
\CommentTok{#>         [0.4264],}
\CommentTok{#>         [0.8934],}
\CommentTok{#>         [0.9442],}
\CommentTok{#>         [0.5018],}
\CommentTok{#>         [0.6240],}
\CommentTok{#>         [0.1156],}
\CommentTok{#>         [0.3173],}
\CommentTok{#>         [0.4148],}
\CommentTok{#>         [0.8663],}
\CommentTok{#>         [0.2505],}
\CommentTok{#>         [0.4830],}
\CommentTok{#>         [0.9856],}
\CommentTok{#>         [0.5195],}
\CommentTok{#>         [0.6129],}
\CommentTok{#>         [0.1206],}
\CommentTok{#>         [0.8263],}
\CommentTok{#>         [0.6031],}
\CommentTok{#>         [0.5451],}
\CommentTok{#>         [0.3428],}
\CommentTok{#>         [0.3041],}
\CommentTok{#>         [0.4170],}
\CommentTok{#>         [0.6813],}
\CommentTok{#>         [0.8755],}
\CommentTok{#>         [0.5104],}
\CommentTok{#>         [0.6693],}
\CommentTok{#>         [0.5859],}
\CommentTok{#>         [0.6249],}
\CommentTok{#>         [0.6747],}
\CommentTok{#>         [0.8423],}
\CommentTok{#>         [0.0832],}
\CommentTok{#>         [0.7637],}
\CommentTok{#>         [0.2437],}
\CommentTok{#>         [0.1942],}
\CommentTok{#>         [0.5725],}
\CommentTok{#>         [0.0957],}
\CommentTok{#>         [0.8853],}
\CommentTok{#>         [0.6272],}
\CommentTok{#>         [0.7234],}
\CommentTok{#>         [0.0161],}
\CommentTok{#>         [0.5944],}
\CommentTok{#>         [0.5568],}
\CommentTok{#>         [0.1590],}
\CommentTok{#>         [0.1531],}
\CommentTok{#>         [0.6955],}
\CommentTok{#>         [0.3188],}
\CommentTok{#>         [0.6920],}
\CommentTok{#>         [0.5544],}
\CommentTok{#>         [0.3890],}
\CommentTok{#>         [0.9251],}
\CommentTok{#>         [0.8417],}
\CommentTok{#>         [0.3574],}
\CommentTok{#>         [0.0436],}
\CommentTok{#>         [0.3048],}
\CommentTok{#>         [0.3982],}
\CommentTok{#>         [0.7050],}
\CommentTok{#>         [0.9954],}
\CommentTok{#>         [0.3559],}
\CommentTok{#>         [0.7625],}
\CommentTok{#>         [0.5932],}
\CommentTok{#>         [0.6917],}
\CommentTok{#>         [0.1511],}
\CommentTok{#>         [0.3989],}
\CommentTok{#>         [0.2409],}
\CommentTok{#>         [0.3435]]) tensor([[2.7166],}
\CommentTok{#>         [1.3983],}
\CommentTok{#>         [0.7679],}
\CommentTok{#>         [1.8464],}
\CommentTok{#>         [2.6614],}
\CommentTok{#>         [1.8297],}
\CommentTok{#>         [4.4034],}
\CommentTok{#>         [2.7003],}
\CommentTok{#>         [2.1778],}
\CommentTok{#>         [1.5073],}
\CommentTok{#>         [1.2966],}
\CommentTok{#>         [2.7287],}
\CommentTok{#>         [1.4884],}
\CommentTok{#>         [0.8423],}
\CommentTok{#>         [1.4895],}
\CommentTok{#>         [2.9263],}
\CommentTok{#>         [1.0114],}
\CommentTok{#>         [0.9445],}
\CommentTok{#>         [1.6729],}
\CommentTok{#>         [2.4624],}
\CommentTok{#>         [2.7788],}
\CommentTok{#>         [3.1746],}
\CommentTok{#>         [2.6593],}
\CommentTok{#>         [2.3800],}
\CommentTok{#>         [3.1382],}
\CommentTok{#>         [1.7665],}
\CommentTok{#>         [1.3082],}
\CommentTok{#>         [1.1390],}
\CommentTok{#>         [1.5341],}
\CommentTok{#>         [2.3566],}
\CommentTok{#>         [0.8612],}
\CommentTok{#>         [1.4642],}
\CommentTok{#>         [1.8066],}
\CommentTok{#>         [2.2308],}
\CommentTok{#>         [2.0962],}
\CommentTok{#>         [1.0096],}
\CommentTok{#>         [1.6538],}
\CommentTok{#>         [3.3994],}
\CommentTok{#>         [3.8747],}
\CommentTok{#>         [2.0045],}
\CommentTok{#>         [2.0884],}
\CommentTok{#>         [0.5845],}
\CommentTok{#>         [1.7039],}
\CommentTok{#>         [1.7285],}
\CommentTok{#>         [3.4602],}
\CommentTok{#>         [1.3581],}
\CommentTok{#>         [2.0949],}
\CommentTok{#>         [3.7935],}
\CommentTok{#>         [2.1950],}
\CommentTok{#>         [2.6425],}
\CommentTok{#>         [0.4948],}
\CommentTok{#>         [3.5188],}
\CommentTok{#>         [2.1628],}
\CommentTok{#>         [1.9643],}
\CommentTok{#>         [1.5740],}
\CommentTok{#>         [1.0099],}
\CommentTok{#>         [1.8123],}
\CommentTok{#>         [2.9534],}
\CommentTok{#>         [3.6986],}
\CommentTok{#>         [1.9485],}
\CommentTok{#>         [2.5445],}
\CommentTok{#>         [2.4811],}
\CommentTok{#>         [2.4700],}
\CommentTok{#>         [2.2838],}
\CommentTok{#>         [3.4392],}
\CommentTok{#>         [0.9015],}
\CommentTok{#>         [2.8687],}
\CommentTok{#>         [1.4766],}
\CommentTok{#>         [1.1847],}
\CommentTok{#>         [2.2782],}
\CommentTok{#>         [0.8885],}
\CommentTok{#>         [3.2565],}
\CommentTok{#>         [2.7141],}
\CommentTok{#>         [3.0781],}
\CommentTok{#>         [0.7763],}
\CommentTok{#>         [2.0038],}
\CommentTok{#>         [1.8270],}
\CommentTok{#>         [0.5882],}
\CommentTok{#>         [0.7793],}
\CommentTok{#>         [2.6416],}
\CommentTok{#>         [1.4162],}
\CommentTok{#>         [2.3851],}
\CommentTok{#>         [1.9140],}
\CommentTok{#>         [1.8385],}
\CommentTok{#>         [3.7822],}
\CommentTok{#>         [3.6160],}
\CommentTok{#>         [1.0941],}
\CommentTok{#>         [0.5721],}
\CommentTok{#>         [1.6683],}
\CommentTok{#>         [1.6848],}
\CommentTok{#>         [2.5068],}
\CommentTok{#>         [4.3876],}
\CommentTok{#>         [1.3866],}
\CommentTok{#>         [3.1286],}
\CommentTok{#>         [1.9197],}
\CommentTok{#>         [2.7949],}
\CommentTok{#>         [0.4797],}
\CommentTok{#>         [1.8171],}
\CommentTok{#>         [1.1042],}
\CommentTok{#>         [1.1414]])}
\end{Highlighting}
\end{Shaded}

\hypertarget{code-in-r-2}{%
\subsection{Code in R}\label{code-in-r-2}}

Notice that before converting to a Torch tensor, we need first to convert the R numeric vector to a \texttt{numpy} array:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert numpy array to tensor in shape of input size}
\NormalTok{x <-}\StringTok{ }\KeywordTok{r_to_py}\NormalTok{(x)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{r_to_py}\NormalTok{(y)}
\NormalTok{x =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{from_numpy}\NormalTok{(x}\OperatorTok{$}\KeywordTok{reshape}\NormalTok{(}\OperatorTok{-}\NormalTok{1L, 1L)) }\CommentTok{#$float()}
\NormalTok{y =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{from_numpy}\NormalTok{(y}\OperatorTok{$}\KeywordTok{reshape}\NormalTok{(}\OperatorTok{-}\NormalTok{1L, 1L)) }\CommentTok{#$float()}
\KeywordTok{print}\NormalTok{(x, y)}
\CommentTok{#> tensor([[0.6965],}
\CommentTok{#>         [0.2861],}
\CommentTok{#>         [0.2269],}
\CommentTok{#>         [0.5513],}
\CommentTok{#>         [0.7195],}
\CommentTok{#>         [0.4231],}
\CommentTok{#>         [0.9808],}
\CommentTok{#>         [0.6848],}
\CommentTok{#>         [0.4809],}
\CommentTok{#>         [0.3921],}
\CommentTok{#>         [0.3432],}
\CommentTok{#>         [0.7290],}
\CommentTok{#>         [0.4386],}
\CommentTok{#>         [0.0597],}
\CommentTok{#>         [0.3980],}
\CommentTok{#>         [0.7380],}
\CommentTok{#>         [0.1825],}
\CommentTok{#>         [0.1755],}
\CommentTok{#>         [0.5316],}
\CommentTok{#>         [0.5318],}
\CommentTok{#>         [0.6344],}
\CommentTok{#>         [0.8494],}
\CommentTok{#>         [0.7245],}
\CommentTok{#>         [0.6110],}
\CommentTok{#>         [0.7224],}
\CommentTok{#>         [0.3230],}
\CommentTok{#>         [0.3618],}
\CommentTok{#>         [0.2283],}
\CommentTok{#>         [0.2937],}
\CommentTok{#>         [0.6310],}
\CommentTok{#>         [0.0921],}
\CommentTok{#>         [0.4337],}
\CommentTok{#>         [0.4309],}
\CommentTok{#>         [0.4937],}
\CommentTok{#>         [0.4258],}
\CommentTok{#>         [0.3123],}
\CommentTok{#>         [0.4264],}
\CommentTok{#>         [0.8934],}
\CommentTok{#>         [0.9442],}
\CommentTok{#>         [0.5018],}
\CommentTok{#>         [0.6240],}
\CommentTok{#>         [0.1156],}
\CommentTok{#>         [0.3173],}
\CommentTok{#>         [0.4148],}
\CommentTok{#>         [0.8663],}
\CommentTok{#>         [0.2505],}
\CommentTok{#>         [0.4830],}
\CommentTok{#>         [0.9856],}
\CommentTok{#>         [0.5195],}
\CommentTok{#>         [0.6129],}
\CommentTok{#>         [0.1206],}
\CommentTok{#>         [0.8263],}
\CommentTok{#>         [0.6031],}
\CommentTok{#>         [0.5451],}
\CommentTok{#>         [0.3428],}
\CommentTok{#>         [0.3041],}
\CommentTok{#>         [0.4170],}
\CommentTok{#>         [0.6813],}
\CommentTok{#>         [0.8755],}
\CommentTok{#>         [0.5104],}
\CommentTok{#>         [0.6693],}
\CommentTok{#>         [0.5859],}
\CommentTok{#>         [0.6249],}
\CommentTok{#>         [0.6747],}
\CommentTok{#>         [0.8423],}
\CommentTok{#>         [0.0832],}
\CommentTok{#>         [0.7637],}
\CommentTok{#>         [0.2437],}
\CommentTok{#>         [0.1942],}
\CommentTok{#>         [0.5725],}
\CommentTok{#>         [0.0957],}
\CommentTok{#>         [0.8853],}
\CommentTok{#>         [0.6272],}
\CommentTok{#>         [0.7234],}
\CommentTok{#>         [0.0161],}
\CommentTok{#>         [0.5944],}
\CommentTok{#>         [0.5568],}
\CommentTok{#>         [0.1590],}
\CommentTok{#>         [0.1531],}
\CommentTok{#>         [0.6955],}
\CommentTok{#>         [0.3188],}
\CommentTok{#>         [0.6920],}
\CommentTok{#>         [0.5544],}
\CommentTok{#>         [0.3890],}
\CommentTok{#>         [0.9251],}
\CommentTok{#>         [0.8417],}
\CommentTok{#>         [0.3574],}
\CommentTok{#>         [0.0436],}
\CommentTok{#>         [0.3048],}
\CommentTok{#>         [0.3982],}
\CommentTok{#>         [0.7050],}
\CommentTok{#>         [0.9954],}
\CommentTok{#>         [0.3559],}
\CommentTok{#>         [0.7625],}
\CommentTok{#>         [0.5932],}
\CommentTok{#>         [0.6917],}
\CommentTok{#>         [0.1511],}
\CommentTok{#>         [0.3989],}
\CommentTok{#>         [0.2409],}
\CommentTok{#>         [0.3435]], dtype=torch.float64)}
\end{Highlighting}
\end{Shaded}

\hypertarget{optimizer-and-loss}{%
\section{Optimizer and Loss}\label{optimizer-and-loss}}

Next, you should define the Optimizer and the Loss Function for our training process.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Define Optimizer and Loss Function}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ torch.optim.SGD(net.parameters(), lr}\OperatorTok{=}\FloatTok{0.2}\NormalTok{)}
\NormalTok{loss_func }\OperatorTok{=}\NormalTok{ torch.nn.MSELoss()}
\BuiltInTok{print}\NormalTok{(optimizer)}
\CommentTok{#> SGD (}
\CommentTok{#> Parameter Group 0}
\CommentTok{#>     dampening: 0}
\CommentTok{#>     lr: 0.2}
\CommentTok{#>     momentum: 0}
\CommentTok{#>     nesterov: False}
\CommentTok{#>     weight_decay: 0}
\CommentTok{#> )}
\BuiltInTok{print}\NormalTok{(loss_func)}
\CommentTok{#> MSELoss()}
\end{Highlighting}
\end{Shaded}

\hypertarget{equivalent-code-in-r}{%
\subsection{Equivalent code in R}\label{equivalent-code-in-r}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Define Optimizer and Loss Function}
\NormalTok{optimizer <-}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\NormalTok{optim}\OperatorTok{$}\KeywordTok{SGD}\NormalTok{(net}\OperatorTok{$}\KeywordTok{parameters}\NormalTok{(), }\DataTypeTok{lr=}\FloatTok{0.2}\NormalTok{)}
\NormalTok{loss_func <-}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\NormalTok{nn}\OperatorTok{$}\KeywordTok{MSELoss}\NormalTok{()}
\KeywordTok{print}\NormalTok{(optimizer)}
\CommentTok{#> SGD (}
\CommentTok{#> Parameter Group 0}
\CommentTok{#>     dampening: 0}
\CommentTok{#>     lr: 0.2}
\CommentTok{#>     momentum: 0}
\CommentTok{#>     nesterov: False}
\CommentTok{#>     weight_decay: 0}
\CommentTok{#> )}
\KeywordTok{print}\NormalTok{(loss_func)}
\CommentTok{#> MSELoss()}
\end{Highlighting}
\end{Shaded}

\hypertarget{training}{%
\section{Training}\label{training}}

\hypertarget{code-in-python-1}{%
\subsection{Code in Python}\label{code-in-python-1}}

Now let's start our training process. With an epoch of 250, you will iterate our data to find the best value for our hyperparameters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inputs }\OperatorTok{=}\NormalTok{ Variable(x)}
\NormalTok{outputs }\OperatorTok{=}\NormalTok{ Variable(y)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{250}\NormalTok{):}
\NormalTok{   prediction }\OperatorTok{=}\NormalTok{ net(inputs)}
\NormalTok{   loss }\OperatorTok{=}\NormalTok{ loss_func(prediction, outputs) }
\NormalTok{   optimizer.zero_grad()}
\NormalTok{   loss.backward()        }
\NormalTok{   optimizer.step()       }

   \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{%} \DecValTok{10} \OperatorTok{==} \DecValTok{0}\NormalTok{:}
       \CommentTok{# plot and show learning process}
\NormalTok{       plt.cla()}
\NormalTok{       plt.scatter(x.data.numpy(), y.data.numpy())}
\NormalTok{       plt.plot(x.data.numpy(), prediction.data.numpy(), }\StringTok{'r-'}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{       plt.text(}\FloatTok{0.5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\StringTok{'Loss=}\SpecialCharTok{%.4f}\StringTok{'} \OperatorTok{%}\NormalTok{ loss.data.numpy(), fontdict}\OperatorTok{=}\NormalTok{\{}\StringTok{'size'}\NormalTok{: }\DecValTok{10}\NormalTok{, }\StringTok{'color'}\NormalTok{:  }\StringTok{'red'}\NormalTok{\})}
\NormalTok{       plt.pause(}\FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-1} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-2} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-3} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-4} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-5} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-6} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-7} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-8} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-9} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-10} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-11} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-12} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-13} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-14} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-15} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-16} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-17} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-18} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-19} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-20} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-21} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-22} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-23} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-24} \includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-25}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/py-iterate-26}

\hypertarget{code-in-r-3}{%
\subsection{Code in R}\label{code-in-r-3}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x =}\StringTok{ }\NormalTok{x}\OperatorTok{$}\KeywordTok{type}\NormalTok{(torch}\OperatorTok{$}\NormalTok{FloatTensor)   }\CommentTok{# make it a a FloatTensor}
\NormalTok{y =}\StringTok{ }\NormalTok{y}\OperatorTok{$}\KeywordTok{type}\NormalTok{(torch}\OperatorTok{$}\NormalTok{FloatTensor)}

\NormalTok{inputs =}\StringTok{ }\KeywordTok{Variable}\NormalTok{(x)}
\NormalTok{outputs =}\StringTok{ }\KeywordTok{Variable}\NormalTok{(y)}
\KeywordTok{plot}\NormalTok{(x}\OperatorTok{$}\NormalTok{data}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{(), y}\OperatorTok{$}\NormalTok{data}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{(), }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{250}\NormalTok{) \{}
\NormalTok{   prediction =}\StringTok{ }\KeywordTok{net}\NormalTok{(inputs)}
\NormalTok{   loss =}\StringTok{ }\KeywordTok{loss_func}\NormalTok{(prediction, outputs)}
\NormalTok{   optimizer}\OperatorTok{$}\KeywordTok{zero_grad}\NormalTok{()}
\NormalTok{   loss}\OperatorTok{$}\KeywordTok{backward}\NormalTok{()}
\NormalTok{   optimizer}\OperatorTok{$}\KeywordTok{step}\NormalTok{()}

   \ControlFlowTok{if}\NormalTok{ (i }\OperatorTok{%%}\StringTok{ }\DecValTok{10} \OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
       \CommentTok{# plot and show learning process}
      \CommentTok{# points(x$data$numpy(), y$data$numpy())}
      \KeywordTok{points}\NormalTok{(x}\OperatorTok{$}\NormalTok{data}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{(), prediction}\OperatorTok{$}\NormalTok{data}\OperatorTok{$}\KeywordTok{numpy}\NormalTok{(), }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
       \CommentTok{# cat(i, loss$data$numpy(), "\textbackslash{}n")}
\NormalTok{   \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{0814-simple_regression_files/figure-latex/r-iterate-1} \end{center}

\hypertarget{result}{%
\section{Result}\label{result}}

As you can see below, you successfully performed regression with a neural network. Actually, on every iteration, the red line in the plot will update and change its position to fit the data. But in this picture, you only show you the final result

\hypertarget{autograd}{%
\chapter{Autograd}\label{autograd}}

Source: \url{https://github.com/jcjohnson/pytorch-examples\#pytorch-autograd}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rTorch)}
\end{Highlighting}
\end{Shaded}

\hypertarget{python-code}{%
\section{Python code}\label{python-code}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Do not print from a function. Similar functionality to R invisible()}
\CommentTok{# https://stackoverflow.com/a/45669280/5270873}
\ImportTok{import}\NormalTok{ os, sys}

\KeywordTok{class}\NormalTok{ HiddenPrints:}
    \KeywordTok{def} \FunctionTok{__enter__}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \VariableTok{self}\NormalTok{._original_stdout }\OperatorTok{=}\NormalTok{ sys.stdout}
\NormalTok{        sys.stdout }\OperatorTok{=} \BuiltInTok{open}\NormalTok{(os.devnull, }\StringTok{'w'}\NormalTok{)}

    \KeywordTok{def} \FunctionTok{__exit__}\NormalTok{(}\VariableTok{self}\NormalTok{, exc_type, exc_val, exc_tb):}
\NormalTok{        sys.stdout.close()}
\NormalTok{        sys.stdout }\OperatorTok{=} \VariableTok{self}\NormalTok{._original_stdout}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Code in file autograd/two_layer_net_autograd.py}
\ImportTok{import}\NormalTok{ torch}
\NormalTok{device }\OperatorTok{=}\NormalTok{ torch.device(}\StringTok{'cpu'}\NormalTok{)}
\CommentTok{# device = torch.device('cuda') # Uncomment this to run on GPU}

\NormalTok{torch.manual_seed(}\DecValTok{0}\NormalTok{)}

\CommentTok{# N is batch size; D_in is input dimension;}
\CommentTok{# H is hidden dimension; D_out is output dimension.}
\CommentTok{#> <torch._C.Generator object at 0x7feb342abb90>}
\NormalTok{N, D_in, H, D_out }\OperatorTok{=} \DecValTok{64}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{10}

\CommentTok{# Create random Tensors to hold input and outputs}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.randn(N, D_in, device}\OperatorTok{=}\NormalTok{device)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ torch.randn(N, D_out, device}\OperatorTok{=}\NormalTok{device)}

\CommentTok{# Create random Tensors for weights; setting requires_grad=True means that we}
\CommentTok{# want to compute gradients for these Tensors during the backward pass.}
\NormalTok{w1 }\OperatorTok{=}\NormalTok{ torch.randn(D_in, H, device}\OperatorTok{=}\NormalTok{device, requires_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{w2 }\OperatorTok{=}\NormalTok{ torch.randn(H, D_out, device}\OperatorTok{=}\NormalTok{device, requires_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{learning_rate }\OperatorTok{=} \FloatTok{1e-6}

\ControlFlowTok{for}\NormalTok{ t }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{5}\NormalTok{):}
  \CommentTok{# Forward pass: compute predicted y using operations on Tensors. Since w1 and}
  \CommentTok{# w2 have requires_grad=True, operations involving these Tensors will cause}
  \CommentTok{# PyTorch to build a computational graph, allowing automatic computation of}
  \CommentTok{# gradients. Since we are no longer implementing the backward pass by hand we}
  \CommentTok{# don't need to keep references to intermediate values.}
\NormalTok{  y_pred }\OperatorTok{=}\NormalTok{ x.mm(w1).clamp(}\BuiltInTok{min}\OperatorTok{=}\DecValTok{0}\NormalTok{).mm(w2)}
  
  \CommentTok{# Compute and print loss. Loss is a Tensor of shape (), and loss.item()}
  \CommentTok{# is a Python number giving its value.}
\NormalTok{  loss }\OperatorTok{=}\NormalTok{ (y_pred }\OperatorTok{-}\NormalTok{ y).}\BuiltInTok{pow}\NormalTok{(}\DecValTok{2}\NormalTok{).}\BuiltInTok{sum}\NormalTok{()}
  \BuiltInTok{print}\NormalTok{(t, loss.item())}

  \CommentTok{# Use autograd to compute the backward pass. This call will compute the}
  \CommentTok{# gradient of loss with respect to all Tensors with requires_grad=True.}
  \CommentTok{# After this call w1.grad and w2.grad will be Tensors holding the gradient}
  \CommentTok{# of the loss with respect to w1 and w2 respectively.}
\NormalTok{  loss.backward()}

  \CommentTok{# Update weights using gradient descent. For this step we just want to mutate}
  \CommentTok{# the values of w1 and w2 in-place; we don't want to build up a computational}
  \CommentTok{# graph for the update steps, so we use the torch.no_grad() context manager}
  \CommentTok{# to prevent PyTorch from building a computational graph for the updates}
  \ControlFlowTok{with}\NormalTok{ torch.no_grad():}
\NormalTok{    w1 }\OperatorTok{-=}\NormalTok{ learning_rate }\OperatorTok{*}\NormalTok{ w1.grad}
\NormalTok{    w2 }\OperatorTok{-=}\NormalTok{ learning_rate }\OperatorTok{*}\NormalTok{ w2.grad}

    \CommentTok{# Manually zero the gradients after running the backward pass}
    \ControlFlowTok{with}\NormalTok{ HiddenPrints():   }\CommentTok{# this would be the equivalent of invisible() in R}
\NormalTok{      w1.grad.zero_()}
\NormalTok{      w2.grad.zero_()}
\CommentTok{#> 0 29428666.0}
\CommentTok{#> 1 22739450.0}
\CommentTok{#> 2 20605262.0}
\CommentTok{#> 3 19520376.0}
\CommentTok{#> 4 17810228.0}
\end{Highlighting}
\end{Shaded}

\hypertarget{r-code}{%
\section{R code}\label{r-code}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# library(reticulate) # originally qwe used reticulate}
\KeywordTok{library}\NormalTok{(rTorch)}

\NormalTok{torch  =}\StringTok{ }\KeywordTok{import}\NormalTok{(}\StringTok{"torch"}\NormalTok{)}
\NormalTok{device =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{device}\NormalTok{(}\StringTok{'cpu'}\NormalTok{)}
\CommentTok{# device = torch.device('cuda') # Uncomment this to run on GPU}

\NormalTok{torch}\OperatorTok{$}\KeywordTok{manual_seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\CommentTok{#> <torch._C.Generator>}

\CommentTok{# N is batch size; D_in is input dimension;}
\CommentTok{# H is hidden dimension; D_out is output dimension.}
\NormalTok{N <-}\StringTok{ }\NormalTok{64L; D_in <-}\StringTok{ }\NormalTok{1000L; H <-}\StringTok{ }\NormalTok{100L; D_out <-}\StringTok{ }\NormalTok{10L}

\CommentTok{# Create random Tensors to hold inputs and outputs}
\NormalTok{x =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{randn}\NormalTok{(N, D_in, }\DataTypeTok{device=}\NormalTok{device)}
\NormalTok{y =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{randn}\NormalTok{(N, D_out, }\DataTypeTok{device=}\NormalTok{device)}

\CommentTok{# Create random Tensors for weights; setting requires_grad=True means that we}
\CommentTok{# want to compute gradients for these Tensors during the backward pass.}
\NormalTok{w1 =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{randn}\NormalTok{(D_in, H, }\DataTypeTok{device=}\NormalTok{device, }\DataTypeTok{requires_grad=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{w2 =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{randn}\NormalTok{(H, D_out, }\DataTypeTok{device=}\NormalTok{device, }\DataTypeTok{requires_grad=}\OtherTok{TRUE}\NormalTok{)}

\NormalTok{learning_rate =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{scalar_tensor}\NormalTok{(}\FloatTok{1e-6}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{) \{}
  \CommentTok{# Forward pass: compute predicted y using operations on Tensors. Since w1 and}
  \CommentTok{# w2 have requires_grad=True, operations involving these Tensors will cause}
  \CommentTok{# PyTorch to build a computational graph, allowing automatic computation of}
  \CommentTok{# gradients. Since we are no longer implementing the backward pass by hand we}
  \CommentTok{# don't need to keep references to intermediate values.}
\NormalTok{  y_pred =}\StringTok{ }\NormalTok{x}\OperatorTok{$}\KeywordTok{mm}\NormalTok{(w1)}\OperatorTok{$}\KeywordTok{clamp}\NormalTok{(}\DataTypeTok{min=}\DecValTok{0}\NormalTok{)}\OperatorTok{$}\KeywordTok{mm}\NormalTok{(w2)}
  
  \CommentTok{# Compute and print loss. Loss is a Tensor of shape (), and loss.item()}
  \CommentTok{# is a Python number giving its value.}
\NormalTok{  loss =}\StringTok{ }\NormalTok{(torch}\OperatorTok{$}\KeywordTok{sub}\NormalTok{(y_pred, y))}\OperatorTok{$}\KeywordTok{pow}\NormalTok{(}\DecValTok{2}\NormalTok{)}\OperatorTok{$}\KeywordTok{sum}\NormalTok{()}
  \KeywordTok{cat}\NormalTok{(t, }\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{, loss}\OperatorTok{$}\KeywordTok{item}\NormalTok{(), }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}

  \CommentTok{# Use autograd to compute the backward pass. This call will compute the}
  \CommentTok{# gradient of loss with respect to all Tensors with requires_grad=True.}
  \CommentTok{# After this call w1.grad and w2.grad will be Tensors holding the gradient}
  \CommentTok{# of the loss with respect to w1 and w2 respectively.}
\NormalTok{  loss}\OperatorTok{$}\KeywordTok{backward}\NormalTok{()}

  \CommentTok{# Update weights using gradient descent. For this step we just want to mutate}
  \CommentTok{# the values of w1 and w2 in-place; we don't want to build up a computational}
  \CommentTok{# graph for the update steps, so we use the torch.no_grad() context manager}
  \CommentTok{# to prevent PyTorch from building a computational graph for the updates}
  \KeywordTok{with}\NormalTok{(torch}\OperatorTok{$}\KeywordTok{no_grad}\NormalTok{(), \{}
\NormalTok{    w1}\OperatorTok{$}\NormalTok{data =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{sub}\NormalTok{(w1}\OperatorTok{$}\NormalTok{data, torch}\OperatorTok{$}\KeywordTok{mul}\NormalTok{(w1}\OperatorTok{$}\NormalTok{grad, learning_rate))}
\NormalTok{    w2}\OperatorTok{$}\NormalTok{data =}\StringTok{ }\NormalTok{torch}\OperatorTok{$}\KeywordTok{sub}\NormalTok{(w2}\OperatorTok{$}\NormalTok{data, torch}\OperatorTok{$}\KeywordTok{mul}\NormalTok{(w2}\OperatorTok{$}\NormalTok{grad, learning_rate))}

    \CommentTok{# Manually zero the gradients after running the backward pass}
\NormalTok{    w1}\OperatorTok{$}\NormalTok{grad}\OperatorTok{$}\KeywordTok{zero_}\NormalTok{()}
\NormalTok{    w2}\OperatorTok{$}\NormalTok{grad}\OperatorTok{$}\KeywordTok{zero_}\NormalTok{()}
\NormalTok{  \})}
\NormalTok{\}    }
\CommentTok{#> 1     29428666 }
\CommentTok{#> 2     22739450 }
\CommentTok{#> 3     20605262 }
\CommentTok{#> 4     19520376 }
\CommentTok{#> 5     17810228}
\end{Highlighting}
\end{Shaded}

\hypertarget{observations}{%
\section{Observations}\label{observations}}

If the seeds worked the same in Python and R, we should see similar results in the output.

\bibliography{book.bib,packages.bib}


\end{document}
