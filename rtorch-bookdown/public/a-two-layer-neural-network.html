<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 A two-layer neural network | A Minimal rTorch Tutorial</title>
  <meta name="description" content="This is a minimal tutorial of using the rTorch package to have fun while doing machine learning. This book was produced with bookdown." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 A two-layer neural network | A Minimal rTorch Tutorial" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal tutorial of using the rTorch package to have fun while doing machine learning. This book was produced with bookdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 A two-layer neural network | A Minimal rTorch Tutorial" />
  
  <meta name="twitter:description" content="This is a minimal tutorial of using the rTorch package to have fun while doing machine learning. This book was produced with bookdown." />
  

<meta name="author" content="Alfonso R. Reyes" />


<meta name="date" content="2019-09-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="rainfall-linear-regression.html"/>
<link rel="next" href="a-very-simple-neural-network.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal rTorch Tutorial</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#installation"><i class="fa fa-check"></i>Installation</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#python-anaconda"><i class="fa fa-check"></i>Python Anaconda</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#example"><i class="fa fa-check"></i>Example</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#automatic-installation"><i class="fa fa-check"></i>Automatic installation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Getting Started</b></span></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#motivation"><i class="fa fa-check"></i><b>1.1</b> Motivation</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#how-do-we-start-using-rtorch"><i class="fa fa-check"></i><b>1.2</b> How do we start using <code>rTorch</code></a><ul>
<li class="chapter" data-level="1.2.1" data-path="intro.html"><a href="intro.html#getting-the-pytorch-version"><i class="fa fa-check"></i><b>1.2.1</b> Getting the PyTorch version</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro.html"><a href="intro.html#pytorch-configuration"><i class="fa fa-check"></i><b>1.2.2</b> PyTorch configuration</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#what-can-you-do-with-rtorch"><i class="fa fa-check"></i><b>1.3</b> What can you do with <code>rTorch</code></a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#callable-pytorch-modules"><i class="fa fa-check"></i><b>1.4</b> Callable PyTorch modules</a><ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#the-torchvision-module"><i class="fa fa-check"></i><b>1.4.1</b> The <code>torchvision</code> module</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#np-the-numpy-module"><i class="fa fa-check"></i><b>1.4.2</b> <code>np</code>: the <code>numpy</code> module</a></li>
<li class="chapter" data-level="1.4.3" data-path="intro.html"><a href="intro.html#python-built-in-functions"><i class="fa fa-check"></i><b>1.4.3</b> Python built-in functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html"><i class="fa fa-check"></i><b>2</b> rTorch vs PyTorch: Whatâ€™s different</a><ul>
<li class="chapter" data-level="2.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#calling-objects-from-pytorch"><i class="fa fa-check"></i><b>2.1</b> Calling objects from PyTorch</a></li>
<li class="chapter" data-level="2.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#call-a-module-from-pytorch"><i class="fa fa-check"></i><b>2.2</b> Call a module from PyTorch</a></li>
<li class="chapter" data-level="2.3" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#show-the-attributes-methods-of-a-class-or-pytorch-object"><i class="fa fa-check"></i><b>2.3</b> Show the attributes (methods) of a class or PyTorch object</a></li>
<li class="chapter" data-level="2.4" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#enumeration"><i class="fa fa-check"></i><b>2.4</b> Enumeration</a></li>
<li class="chapter" data-level="2.5" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#how-to-iterate"><i class="fa fa-check"></i><b>2.5</b> How to iterate</a><ul>
<li class="chapter" data-level="2.5.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#using-enumerate-and-iterate"><i class="fa fa-check"></i><b>2.5.1</b> Using <code>enumerate</code> and <code>iterate</code></a></li>
<li class="chapter" data-level="2.5.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#using-a-for-loop-to-iterate"><i class="fa fa-check"></i><b>2.5.2</b> Using a <code>for-loop</code> to iterate</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#zero-gradient"><i class="fa fa-check"></i><b>2.6</b> Zero gradient</a><ul>
<li class="chapter" data-level="2.6.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#version-in-python"><i class="fa fa-check"></i><b>2.6.1</b> Version in Python</a></li>
<li class="chapter" data-level="2.6.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#version-in-r"><i class="fa fa-check"></i><b>2.6.2</b> Version in R</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#transform-a-tensor"><i class="fa fa-check"></i><b>2.7</b> Transform a tensor</a></li>
<li class="chapter" data-level="2.8" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#build-a-model-class"><i class="fa fa-check"></i><b>2.8</b> Build a model class</a><ul>
<li class="chapter" data-level="2.8.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#example-1"><i class="fa fa-check"></i><b>2.8.1</b> Example 1</a></li>
<li class="chapter" data-level="2.8.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#example-2-logistic-regression"><i class="fa fa-check"></i><b>2.8.2</b> Example 2: Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#convert-a-tensor-to-numpy-object"><i class="fa fa-check"></i><b>2.9</b> Convert a tensor to <code>numpy</code> object</a></li>
<li class="chapter" data-level="2.10" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#convert-a-numpy-object-to-an-r-object"><i class="fa fa-check"></i><b>2.10</b> Convert a <code>numpy</code> object to an <code>R</code> object</a></li>
</ul></li>
<li class="part"><span><b>II Basic Tensor Operations</b></span></li>
<li class="chapter" data-level="3" data-path="tensors.html"><a href="tensors.html"><i class="fa fa-check"></i><b>3</b> Tensors</a><ul>
<li class="chapter" data-level="3.1" data-path="tensors.html"><a href="tensors.html#tensor-data-types"><i class="fa fa-check"></i><b>3.1</b> Tensor data types</a></li>
<li class="chapter" data-level="3.2" data-path="tensors.html"><a href="tensors.html#arithmetic-of-tensors"><i class="fa fa-check"></i><b>3.2</b> Arithmetic of tensors</a><ul>
<li class="chapter" data-level="3.2.1" data-path="tensors.html"><a href="tensors.html#add-tensors"><i class="fa fa-check"></i><b>3.2.1</b> Add tensors</a></li>
<li class="chapter" data-level="3.2.2" data-path="tensors.html"><a href="tensors.html#multiply-a-tensor-by-a-scalar"><i class="fa fa-check"></i><b>3.2.2</b> Multiply a tensor by a scalar</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="tensors.html"><a href="tensors.html#numpy-and-pytorch"><i class="fa fa-check"></i><b>3.3</b> NumPy and PyTorch</a><ul>
<li class="chapter" data-level="3.3.1" data-path="tensors.html"><a href="tensors.html#tuples-python-and-vectors-r"><i class="fa fa-check"></i><b>3.3.1</b> Tuples (Python) and vectors (R)</a></li>
<li class="chapter" data-level="3.3.2" data-path="tensors.html"><a href="tensors.html#make-a-numpy-array-a-tensor-with-as_tensor"><i class="fa fa-check"></i><b>3.3.2</b> Make a numpy array a tensor with <code>as_tensor()</code></a></li>
<li class="chapter" data-level="3.3.3" data-path="tensors.html"><a href="tensors.html#tensor-to-array-and-viceversa"><i class="fa fa-check"></i><b>3.3.3</b> Tensor to array, and viceversa</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="tensors.html"><a href="tensors.html#create-tensors"><i class="fa fa-check"></i><b>3.4</b> Create tensors</a></li>
<li class="chapter" data-level="3.5" data-path="tensors.html"><a href="tensors.html#tensor-resizing"><i class="fa fa-check"></i><b>3.5</b> Tensor resizing</a><ul>
<li class="chapter" data-level="3.5.1" data-path="tensors.html"><a href="tensors.html#concatenate-tensors"><i class="fa fa-check"></i><b>3.5.1</b> Concatenate tensors</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="tensors.html"><a href="tensors.html#reshape-tensors"><i class="fa fa-check"></i><b>3.6</b> Reshape tensors</a><ul>
<li class="chapter" data-level="3.6.1" data-path="tensors.html"><a href="tensors.html#with-function-chunk"><i class="fa fa-check"></i><b>3.6.1</b> With function <code>chunk()</code>:</a></li>
<li class="chapter" data-level="3.6.2" data-path="tensors.html"><a href="tensors.html#with-index_select"><i class="fa fa-check"></i><b>3.6.2</b> With <code>index_select()</code>:</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="tensors.html"><a href="tensors.html#special-tensors"><i class="fa fa-check"></i><b>3.7</b> Special tensors</a><ul>
<li class="chapter" data-level="3.7.1" data-path="tensors.html"><a href="tensors.html#identity-matrix"><i class="fa fa-check"></i><b>3.7.1</b> Identity matrix</a></li>
<li class="chapter" data-level="3.7.2" data-path="tensors.html"><a href="tensors.html#ones"><i class="fa fa-check"></i><b>3.7.2</b> Ones</a></li>
<li class="chapter" data-level="3.7.3" data-path="tensors.html"><a href="tensors.html#zeros"><i class="fa fa-check"></i><b>3.7.3</b> Zeros</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="tensors.html"><a href="tensors.html#tensor-fill"><i class="fa fa-check"></i><b>3.8</b> Tensor fill</a><ul>
<li class="chapter" data-level="3.8.1" data-path="tensors.html"><a href="tensors.html#initialize-a-linear-or-log-scale-tensor"><i class="fa fa-check"></i><b>3.8.1</b> Initialize a linear or log scale Tensor</a></li>
<li class="chapter" data-level="3.8.2" data-path="tensors.html"><a href="tensors.html#inplace-out-of-place"><i class="fa fa-check"></i><b>3.8.2</b> Inplace / Out-of-place</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="tensors.html"><a href="tensors.html#access-to-tensor-elements"><i class="fa fa-check"></i><b>3.9</b> Access to tensor elements</a><ul>
<li class="chapter" data-level="3.9.1" data-path="tensors.html"><a href="tensors.html#using-indices-to-access-elements"><i class="fa fa-check"></i><b>3.9.1</b> Using indices to access elements</a></li>
<li class="chapter" data-level="3.9.2" data-path="tensors.html"><a href="tensors.html#using-the-take-function"><i class="fa fa-check"></i><b>3.9.2</b> Using the <code>take</code> function</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="tensors.html"><a href="tensors.html#other-tensor-operations"><i class="fa fa-check"></i><b>3.10</b> Other tensor operations</a><ul>
<li class="chapter" data-level="3.10.1" data-path="tensors.html"><a href="tensors.html#cross-product"><i class="fa fa-check"></i><b>3.10.1</b> Cross product</a></li>
<li class="chapter" data-level="3.10.2" data-path="tensors.html"><a href="tensors.html#dot-product"><i class="fa fa-check"></i><b>3.10.2</b> Dot product</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="tensors.html"><a href="tensors.html#logical-operations"><i class="fa fa-check"></i><b>3.11</b> Logical operations</a><ul>
<li class="chapter" data-level="3.11.1" data-path="tensors.html"><a href="tensors.html#logical-not"><i class="fa fa-check"></i><b>3.11.1</b> Logical NOT</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="tensors.html"><a href="tensors.html#distributions"><i class="fa fa-check"></i><b>3.12</b> Distributions</a><ul>
<li class="chapter" data-level="3.12.1" data-path="tensors.html"><a href="tensors.html#uniform-matrix"><i class="fa fa-check"></i><b>3.12.1</b> Uniform matrix</a></li>
<li class="chapter" data-level="3.12.2" data-path="tensors.html"><a href="tensors.html#binomial-distribution"><i class="fa fa-check"></i><b>3.12.2</b> Binomial distribution</a></li>
<li class="chapter" data-level="3.12.3" data-path="tensors.html"><a href="tensors.html#exponential-distribution"><i class="fa fa-check"></i><b>3.12.3</b> Exponential distribution</a></li>
<li class="chapter" data-level="3.12.4" data-path="tensors.html"><a href="tensors.html#weibull-distribution"><i class="fa fa-check"></i><b>3.12.4</b> Weibull distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linearalgebra.html"><a href="linearalgebra.html"><i class="fa fa-check"></i><b>4</b> Linear Algebra with Torch</a><ul>
<li class="chapter" data-level="4.1" data-path="linearalgebra.html"><a href="linearalgebra.html#scalars"><i class="fa fa-check"></i><b>4.1</b> Scalars</a></li>
<li class="chapter" data-level="4.2" data-path="linearalgebra.html"><a href="linearalgebra.html#vectors"><i class="fa fa-check"></i><b>4.2</b> Vectors</a></li>
<li class="chapter" data-level="4.3" data-path="linearalgebra.html"><a href="linearalgebra.html#matrices"><i class="fa fa-check"></i><b>4.3</b> Matrices</a></li>
<li class="chapter" data-level="4.4" data-path="linearalgebra.html"><a href="linearalgebra.html#d-tensors"><i class="fa fa-check"></i><b>4.4</b> 3D+ tensors</a></li>
<li class="chapter" data-level="4.5" data-path="linearalgebra.html"><a href="linearalgebra.html#transpose-of-a-matrix"><i class="fa fa-check"></i><b>4.5</b> Transpose of a matrix</a></li>
<li class="chapter" data-level="4.6" data-path="linearalgebra.html"><a href="linearalgebra.html#vectors-special-case-of-a-matrix"><i class="fa fa-check"></i><b>4.6</b> Vectors, special case of a matrix</a></li>
<li class="chapter" data-level="4.7" data-path="linearalgebra.html"><a href="linearalgebra.html#tensor-arithmetic"><i class="fa fa-check"></i><b>4.7</b> Tensor arithmetic</a></li>
<li class="chapter" data-level="4.8" data-path="linearalgebra.html"><a href="linearalgebra.html#add-a-scalar-to-a-tensor"><i class="fa fa-check"></i><b>4.8</b> Add a scalar to a tensor</a></li>
<li class="chapter" data-level="4.9" data-path="linearalgebra.html"><a href="linearalgebra.html#multiplying-tensors"><i class="fa fa-check"></i><b>4.9</b> Multiplying tensors</a></li>
<li class="chapter" data-level="4.10" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product-1"><i class="fa fa-check"></i><b>4.10</b> Dot product</a></li>
</ul></li>
<li class="part"><span><b>III Logistic Regression</b></span></li>
<li class="chapter" data-level="5" data-path="mnistdigits.html"><a href="mnistdigits.html"><i class="fa fa-check"></i><b>5</b> Example 1: MNIST handwritten digits</a><ul>
<li class="chapter" data-level="5.1" data-path="mnistdigits.html"><a href="mnistdigits.html#hyperparameters"><i class="fa fa-check"></i><b>5.1</b> Hyperparameters</a></li>
<li class="chapter" data-level="5.2" data-path="mnistdigits.html"><a href="mnistdigits.html#read-datasets"><i class="fa fa-check"></i><b>5.2</b> Read datasets</a></li>
<li class="chapter" data-level="5.3" data-path="mnistdigits.html"><a href="mnistdigits.html#define-the-model"><i class="fa fa-check"></i><b>5.3</b> Define the model</a></li>
<li class="chapter" data-level="5.4" data-path="mnistdigits.html"><a href="mnistdigits.html#training"><i class="fa fa-check"></i><b>5.4</b> Training</a></li>
<li class="chapter" data-level="5.5" data-path="mnistdigits.html"><a href="mnistdigits.html#prediction"><i class="fa fa-check"></i><b>5.5</b> Prediction</a></li>
<li class="chapter" data-level="5.6" data-path="mnistdigits.html"><a href="mnistdigits.html#save-the-model"><i class="fa fa-check"></i><b>5.6</b> Save the model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="the-sigmoid-function.html"><a href="the-sigmoid-function.html"><i class="fa fa-check"></i><b>6</b> The Sigmoid function</a></li>
<li class="chapter" data-level="7" data-path="a-classic-classification-problem.html"><a href="a-classic-classification-problem.html"><i class="fa fa-check"></i><b>7</b> A classic classification problem</a></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>8</b> Simple linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#generate-the-dataset"><i class="fa fa-check"></i><b>8.2</b> Generate the dataset</a></li>
<li class="chapter" data-level="8.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#convert-arrays-to-tensors"><i class="fa fa-check"></i><b>8.3</b> Convert arrays to tensors</a></li>
<li class="chapter" data-level="8.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#converting-from-numpy-to-tensor"><i class="fa fa-check"></i><b>8.4</b> Converting from numpy to tensor</a></li>
<li class="chapter" data-level="8.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#creating-the-network-model"><i class="fa fa-check"></i><b>8.5</b> Creating the network model</a></li>
<li class="chapter" data-level="8.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#optimizer-and-loss"><i class="fa fa-check"></i><b>8.6</b> Optimizer and Loss</a></li>
<li class="chapter" data-level="8.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#training-1"><i class="fa fa-check"></i><b>8.7</b> Training</a></li>
<li class="chapter" data-level="8.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#results"><i class="fa fa-check"></i><b>8.8</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html"><i class="fa fa-check"></i><b>9</b> Rainfall. Linear Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#training-data"><i class="fa fa-check"></i><b>9.1</b> Training data</a></li>
<li class="chapter" data-level="9.2" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#convert-arrays-to-tensors-1"><i class="fa fa-check"></i><b>9.2</b> Convert arrays to tensors</a></li>
<li class="chapter" data-level="9.3" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#build-the-model"><i class="fa fa-check"></i><b>9.3</b> Build the model</a></li>
<li class="chapter" data-level="9.4" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#generate-predictions"><i class="fa fa-check"></i><b>9.4</b> Generate predictions</a></li>
<li class="chapter" data-level="9.5" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#loss-function"><i class="fa fa-check"></i><b>9.5</b> Loss Function</a></li>
<li class="chapter" data-level="9.6" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#step-by-step-process"><i class="fa fa-check"></i><b>9.6</b> Step by step process</a><ul>
<li class="chapter" data-level="9.6.1" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#compute-the-losses"><i class="fa fa-check"></i><b>9.6.1</b> Compute the losses</a></li>
<li class="chapter" data-level="9.6.2" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#compute-gradients"><i class="fa fa-check"></i><b>9.6.2</b> Compute Gradients</a></li>
<li class="chapter" data-level="9.6.3" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#reset-the-gradients"><i class="fa fa-check"></i><b>9.6.3</b> Reset the gradients</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#all-together-train-for-multiple-epochs"><i class="fa fa-check"></i><b>9.7</b> All together: train for multiple epochs</a></li>
</ul></li>
<li class="part"><span><b>V Neural Networks</b></span></li>
<li class="chapter" data-level="10" data-path="a-two-layer-neural-network.html"><a href="a-two-layer-neural-network.html"><i class="fa fa-check"></i><b>10</b> A two-layer neural network</a><ul>
<li class="chapter" data-level="10.1" data-path="a-two-layer-neural-network.html"><a href="a-two-layer-neural-network.html#load-the-libraries"><i class="fa fa-check"></i><b>10.1</b> Load the libraries</a></li>
<li class="chapter" data-level="10.2" data-path="a-two-layer-neural-network.html"><a href="a-two-layer-neural-network.html#dataset"><i class="fa fa-check"></i><b>10.2</b> Dataset</a></li>
<li class="chapter" data-level="10.3" data-path="a-two-layer-neural-network.html"><a href="a-two-layer-neural-network.html#run-the-model-for-50-iterations"><i class="fa fa-check"></i><b>10.3</b> Run the model for 50 iterations</a></li>
<li class="chapter" data-level="10.4" data-path="a-two-layer-neural-network.html"><a href="a-two-layer-neural-network.html#run-it-at-100-iterations"><i class="fa fa-check"></i><b>10.4</b> Run it at 100 iterations</a></li>
<li class="chapter" data-level="10.5" data-path="a-two-layer-neural-network.html"><a href="a-two-layer-neural-network.html#original-pytorch-code"><i class="fa fa-check"></i><b>10.5</b> Original PyTorch code</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html"><i class="fa fa-check"></i><b>11</b> A very simple neural network</a><ul>
<li class="chapter" data-level="11.1" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#introduction-1"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#select-device"><i class="fa fa-check"></i><b>11.2</b> Select device</a></li>
<li class="chapter" data-level="11.3" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#create-the-dataset"><i class="fa fa-check"></i><b>11.3</b> Create the dataset</a></li>
<li class="chapter" data-level="11.4" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#define-the-model-1"><i class="fa fa-check"></i><b>11.4</b> Define the model</a></li>
<li class="chapter" data-level="11.5" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#loss-function-1"><i class="fa fa-check"></i><b>11.5</b> Loss function</a></li>
<li class="chapter" data-level="11.6" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#iterate-through-batches"><i class="fa fa-check"></i><b>11.6</b> Iterate through batches</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="neural-networks-2.html"><a href="neural-networks-2.html"><i class="fa fa-check"></i><b>12</b> Neural Networks 2</a><ul>
<li class="chapter" data-level="12.1" data-path="neural-networks-2.html"><a href="neural-networks-2.html#nn2-1"><i class="fa fa-check"></i><b>12.1</b> nn2 1</a></li>
<li class="chapter" data-level="12.2" data-path="neural-networks-2.html"><a href="neural-networks-2.html#nn2-2"><i class="fa fa-check"></i><b>12.2</b> nn2 2</a></li>
</ul></li>
<li class="part"><span><b>VI PyTorch and R data structures</b></span></li>
<li class="chapter" data-level="13" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html"><i class="fa fa-check"></i><b>13</b> Working with data.frame</a><ul>
<li class="chapter" data-level="13.1" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#load-pytorch-libraries"><i class="fa fa-check"></i><b>13.1</b> Load PyTorch libraries</a></li>
<li class="chapter" data-level="13.2" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#load-dataset"><i class="fa fa-check"></i><b>13.2</b> Load dataset</a></li>
<li class="chapter" data-level="13.3" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#summary-statistics-for-tensors"><i class="fa fa-check"></i><b>13.3</b> Summary statistics for tensors</a><ul>
<li class="chapter" data-level="13.3.1" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#using-data.frame"><i class="fa fa-check"></i><b>13.3.1</b> using <code>data.frame</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="working-with-data-table.html"><a href="working-with-data-table.html"><i class="fa fa-check"></i><b>14</b> Working with data.table</a><ul>
<li class="chapter" data-level="14.1" data-path="working-with-data-table.html"><a href="working-with-data-table.html#load-pytorch-libraries-1"><i class="fa fa-check"></i><b>14.1</b> Load PyTorch libraries</a></li>
<li class="chapter" data-level="14.2" data-path="working-with-data-table.html"><a href="working-with-data-table.html#load-dataset-1"><i class="fa fa-check"></i><b>14.2</b> Load dataset</a></li>
<li class="chapter" data-level="14.3" data-path="working-with-data-table.html"><a href="working-with-data-table.html#read-the-datasets-without-normalization"><i class="fa fa-check"></i><b>14.3</b> Read the datasets without normalization</a></li>
<li class="chapter" data-level="14.4" data-path="working-with-data-table.html"><a href="working-with-data-table.html#using-data.table"><i class="fa fa-check"></i><b>14.4</b> Using <code>data.table</code></a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixA.html"><a href="appendixA.html"><i class="fa fa-check"></i><b>A</b> Statistical Background</a><ul>
<li class="chapter" data-level="A.1" data-path="appendixA.html"><a href="appendixA.html#basic-statistical-terms"><i class="fa fa-check"></i><b>A.1</b> Basic statistical terms</a><ul>
<li class="chapter" data-level="A.1.1" data-path="appendixA.html"><a href="appendixA.html#five-number-summary"><i class="fa fa-check"></i><b>A.1.1</b> Five-number summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Minimal rTorch Tutorial</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="a-two-layer-neural-network" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> A two-layer neural network</h1>
<p>The following example was converted from PyTorch to rTorch to show differences and similarities of both approaches. The original source can be found here:</p>
<p>Source: <a href="https://github.com/jcjohnson/pytorch-examples#pytorch-tensors" class="uri">https://github.com/jcjohnson/pytorch-examples#pytorch-tensors</a></p>
<div id="load-the-libraries" class="section level2">
<h2><span class="header-section-number">10.1</span> Load the libraries</h2>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb212-1" data-line-number="1"><span class="kw">library</span>(rTorch)</a>
<a class="sourceLine" id="cb212-2" data-line-number="2"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb212-3" data-line-number="3"></a>
<a class="sourceLine" id="cb212-4" data-line-number="4">device =<span class="st"> </span>torch<span class="op">$</span><span class="kw">device</span>(<span class="st">&#39;cpu&#39;</span>)</a>
<a class="sourceLine" id="cb212-5" data-line-number="5"><span class="co"># device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU</span></a>
<a class="sourceLine" id="cb212-6" data-line-number="6"></a>
<a class="sourceLine" id="cb212-7" data-line-number="7">torch<span class="op">$</span><span class="kw">manual_seed</span>(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb212-8" data-line-number="8"><span class="co">#&gt; &lt;torch._C.Generator&gt;</span></a></code></pre></div>
<ul>
<li><code>N</code> is batch size;</li>
<li><code>D_in</code> is input dimension;</li>
<li><code>H</code> is hidden dimension;</li>
<li><code>D_out</code> is output dimension.</li>
</ul>
</div>
<div id="dataset" class="section level2">
<h2><span class="header-section-number">10.2</span> Dataset</h2>
<p>We will create a random dataset for a <strong>two layer neural network</strong>.</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb213-1" data-line-number="1">N &lt;-<span class="st"> </span>64L; D_in &lt;-<span class="st"> </span>1000L; H &lt;-<span class="st"> </span>100L; D_out &lt;-<span class="st"> </span>10L</a>
<a class="sourceLine" id="cb213-2" data-line-number="2"></a>
<a class="sourceLine" id="cb213-3" data-line-number="3"><span class="co"># Create random Tensors to hold inputs and outputs</span></a>
<a class="sourceLine" id="cb213-4" data-line-number="4">x &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(N, D_in, <span class="dt">device=</span>device)</a>
<a class="sourceLine" id="cb213-5" data-line-number="5">y &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(N, D_out, <span class="dt">device=</span>device)</a>
<a class="sourceLine" id="cb213-6" data-line-number="6"></a>
<a class="sourceLine" id="cb213-7" data-line-number="7"><span class="kw">dim</span>(x)</a>
<a class="sourceLine" id="cb213-8" data-line-number="8"><span class="co">#&gt; [1]   64 1000</span></a>
<a class="sourceLine" id="cb213-9" data-line-number="9"><span class="kw">dim</span>(y)</a>
<a class="sourceLine" id="cb213-10" data-line-number="10"><span class="co">#&gt; [1] 64 10</span></a></code></pre></div>
</div>
<div id="run-the-model-for-50-iterations" class="section level2">
<h2><span class="header-section-number">10.3</span> Run the model for 50 iterations</h2>
<p>Letâ€™s say that for the sake of time we select to run only 50 iterations of the loop doing the training.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb214-1" data-line-number="1"><span class="co"># Randomly initialize weights</span></a>
<a class="sourceLine" id="cb214-2" data-line-number="2">w1 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(D_in, H, <span class="dt">device=</span>device)   <span class="co"># layer 1</span></a>
<a class="sourceLine" id="cb214-3" data-line-number="3">w2 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(H, D_out, <span class="dt">device=</span>device)  <span class="co"># layer 2</span></a></code></pre></div>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb215-1" data-line-number="1">learning_rate =<span class="st"> </span><span class="fl">1e-6</span></a>
<a class="sourceLine" id="cb215-2" data-line-number="2"></a>
<a class="sourceLine" id="cb215-3" data-line-number="3"><span class="co"># loop</span></a>
<a class="sourceLine" id="cb215-4" data-line-number="4"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">50</span>) {</a>
<a class="sourceLine" id="cb215-5" data-line-number="5">  <span class="co"># Forward pass: compute predicted y, y_pred</span></a>
<a class="sourceLine" id="cb215-6" data-line-number="6">  h &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">mm</span>(w1)              <span class="co"># matrix multiplication, x*w1</span></a>
<a class="sourceLine" id="cb215-7" data-line-number="7">  h_relu &lt;-<span class="st"> </span>h<span class="op">$</span><span class="kw">clamp</span>(<span class="dt">min=</span><span class="dv">0</span>)   <span class="co"># make elements greater than zero</span></a>
<a class="sourceLine" id="cb215-8" data-line-number="8">  y_pred &lt;-<span class="st"> </span>h_relu<span class="op">$</span><span class="kw">mm</span>(w2)    <span class="co"># matrix multiplication, h_relu*w2</span></a>
<a class="sourceLine" id="cb215-9" data-line-number="9"></a>
<a class="sourceLine" id="cb215-10" data-line-number="10">  <span class="co"># Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor</span></a>
<a class="sourceLine" id="cb215-11" data-line-number="11">  <span class="co"># of shape (); we can get its value as a Python number with loss.item().</span></a>
<a class="sourceLine" id="cb215-12" data-line-number="12">  loss &lt;-<span class="st"> </span>(torch<span class="op">$</span><span class="kw">sub</span>(y_pred, y))<span class="op">$</span><span class="kw">pow</span>(<span class="dv">2</span>)<span class="op">$</span><span class="kw">sum</span>()   <span class="co"># sum((y_pred-y)^2)</span></a>
<a class="sourceLine" id="cb215-13" data-line-number="13">  <span class="co"># cat(t, &quot;\t&quot;)</span></a>
<a class="sourceLine" id="cb215-14" data-line-number="14">  <span class="co"># cat(loss$item(), &quot;\n&quot;)</span></a>
<a class="sourceLine" id="cb215-15" data-line-number="15"></a>
<a class="sourceLine" id="cb215-16" data-line-number="16">  <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></a>
<a class="sourceLine" id="cb215-17" data-line-number="17">  grad_y_pred &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">mul</span>(torch<span class="op">$</span><span class="kw">scalar_tensor</span>(<span class="fl">2.0</span>), torch<span class="op">$</span><span class="kw">sub</span>(y_pred, y))</a>
<a class="sourceLine" id="cb215-18" data-line-number="18">  grad_w2 &lt;-<span class="st"> </span>h_relu<span class="op">$</span><span class="kw">t</span>()<span class="op">$</span><span class="kw">mm</span>(grad_y_pred)        <span class="co"># compute gradient of w2</span></a>
<a class="sourceLine" id="cb215-19" data-line-number="19">  grad_h_relu &lt;-<span class="st"> </span>grad_y_pred<span class="op">$</span><span class="kw">mm</span>(w2<span class="op">$</span><span class="kw">t</span>())</a>
<a class="sourceLine" id="cb215-20" data-line-number="20">  grad_h &lt;-<span class="st"> </span>grad_h_relu<span class="op">$</span><span class="kw">clone</span>()</a>
<a class="sourceLine" id="cb215-21" data-line-number="21">  mask &lt;-<span class="st"> </span>grad_h<span class="op">$</span><span class="kw">lt</span>(<span class="dv">0</span>)                         <span class="co"># filter values lower than zero </span></a>
<a class="sourceLine" id="cb215-22" data-line-number="22">  torch<span class="op">$</span><span class="kw">masked_select</span>(grad_h, mask)<span class="op">$</span><span class="kw">fill_</span>(<span class="fl">0.0</span>) <span class="co"># make them equal to zero</span></a>
<a class="sourceLine" id="cb215-23" data-line-number="23">  grad_w1 &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">t</span>()<span class="op">$</span><span class="kw">mm</span>(grad_h)                  <span class="co"># compute gradient of w1</span></a>
<a class="sourceLine" id="cb215-24" data-line-number="24">   </a>
<a class="sourceLine" id="cb215-25" data-line-number="25">  <span class="co"># Update weights using gradient descent</span></a>
<a class="sourceLine" id="cb215-26" data-line-number="26">  w1 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(w1, torch<span class="op">$</span><span class="kw">mul</span>(learning_rate, grad_w1))</a>
<a class="sourceLine" id="cb215-27" data-line-number="27">  w2 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(w2, torch<span class="op">$</span><span class="kw">mul</span>(learning_rate, grad_w2))</a>
<a class="sourceLine" id="cb215-28" data-line-number="28">}</a>
<a class="sourceLine" id="cb215-29" data-line-number="29"></a>
<a class="sourceLine" id="cb215-30" data-line-number="30">df_<span class="dv">50</span> &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> y<span class="op">$</span><span class="kw">flatten</span>()<span class="op">$</span><span class="kw">numpy</span>(), </a>
<a class="sourceLine" id="cb215-31" data-line-number="31">                    <span class="dt">y_pred =</span> y_pred<span class="op">$</span><span class="kw">flatten</span>()<span class="op">$</span><span class="kw">numpy</span>(), <span class="dt">iter =</span> <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb215-32" data-line-number="32"></a>
<a class="sourceLine" id="cb215-33" data-line-number="33"><span class="kw">ggplot</span>(df_<span class="dv">50</span>, <span class="kw">aes</span>(<span class="dt">x =</span> y, <span class="dt">y =</span> y_pred)) <span class="op">+</span></a>
<a class="sourceLine" id="cb215-34" data-line-number="34"><span class="st">    </span><span class="kw">geom_point</span>()</a></code></pre></div>
<p><img src="0501-neural_networks_files/figure-html/run-model-1.png" width="70%" style="display: block; margin: auto;" />
We see a lot of dispersion between the predicted values, <span class="math inline">\(y_{pred}\)</span> and the real values, <span class="math inline">\(y\)</span>. We are far from our goal.</p>
</div>
<div id="run-it-at-100-iterations" class="section level2">
<h2><span class="header-section-number">10.4</span> Run it at 100 iterations</h2>
<p>Now, we convert the script above to a function, so we could reuse it several times. We want to study the effect of the iteration on the performance of rthe algorithm.</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb216-1" data-line-number="1">train &lt;-<span class="st"> </span><span class="cf">function</span>(iterations) {</a>
<a class="sourceLine" id="cb216-2" data-line-number="2"></a>
<a class="sourceLine" id="cb216-3" data-line-number="3">    <span class="co"># Randomly initialize weights</span></a>
<a class="sourceLine" id="cb216-4" data-line-number="4">    w1 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(D_in, H, <span class="dt">device=</span>device)   <span class="co"># layer 1</span></a>
<a class="sourceLine" id="cb216-5" data-line-number="5">    w2 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(H, D_out, <span class="dt">device=</span>device)  <span class="co"># layer 2</span></a>
<a class="sourceLine" id="cb216-6" data-line-number="6">    </a>
<a class="sourceLine" id="cb216-7" data-line-number="7">    learning_rate =<span class="st"> </span><span class="fl">1e-6</span></a>
<a class="sourceLine" id="cb216-8" data-line-number="8">    </a>
<a class="sourceLine" id="cb216-9" data-line-number="9">    <span class="co"># loop</span></a>
<a class="sourceLine" id="cb216-10" data-line-number="10">    <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>iterations) {</a>
<a class="sourceLine" id="cb216-11" data-line-number="11">      <span class="co"># Forward pass: compute predicted y</span></a>
<a class="sourceLine" id="cb216-12" data-line-number="12">      h &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">mm</span>(w1)</a>
<a class="sourceLine" id="cb216-13" data-line-number="13">      h_relu &lt;-<span class="st"> </span>h<span class="op">$</span><span class="kw">clamp</span>(<span class="dt">min=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb216-14" data-line-number="14">      y_pred &lt;-<span class="st"> </span>h_relu<span class="op">$</span><span class="kw">mm</span>(w2)</a>
<a class="sourceLine" id="cb216-15" data-line-number="15">    </a>
<a class="sourceLine" id="cb216-16" data-line-number="16">      <span class="co"># Compute and print loss; loss is a scalar stored in a PyTorch Tensor</span></a>
<a class="sourceLine" id="cb216-17" data-line-number="17">      <span class="co"># of shape (); we can get its value as a Python number with loss.item().</span></a>
<a class="sourceLine" id="cb216-18" data-line-number="18">      loss &lt;-<span class="st"> </span>(torch<span class="op">$</span><span class="kw">sub</span>(y_pred, y))<span class="op">$</span><span class="kw">pow</span>(<span class="dv">2</span>)<span class="op">$</span><span class="kw">sum</span>()</a>
<a class="sourceLine" id="cb216-19" data-line-number="19">      <span class="co"># cat(t, &quot;\t&quot;); cat(loss$item(), &quot;\n&quot;)</span></a>
<a class="sourceLine" id="cb216-20" data-line-number="20">    </a>
<a class="sourceLine" id="cb216-21" data-line-number="21">      <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></a>
<a class="sourceLine" id="cb216-22" data-line-number="22">      grad_y_pred &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">mul</span>(torch<span class="op">$</span><span class="kw">scalar_tensor</span>(<span class="fl">2.0</span>), torch<span class="op">$</span><span class="kw">sub</span>(y_pred, y))</a>
<a class="sourceLine" id="cb216-23" data-line-number="23">      grad_w2 &lt;-<span class="st"> </span>h_relu<span class="op">$</span><span class="kw">t</span>()<span class="op">$</span><span class="kw">mm</span>(grad_y_pred)</a>
<a class="sourceLine" id="cb216-24" data-line-number="24">      grad_h_relu &lt;-<span class="st"> </span>grad_y_pred<span class="op">$</span><span class="kw">mm</span>(w2<span class="op">$</span><span class="kw">t</span>())</a>
<a class="sourceLine" id="cb216-25" data-line-number="25">      grad_h &lt;-<span class="st"> </span>grad_h_relu<span class="op">$</span><span class="kw">clone</span>()</a>
<a class="sourceLine" id="cb216-26" data-line-number="26">      mask &lt;-<span class="st"> </span>grad_h<span class="op">$</span><span class="kw">lt</span>(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb216-27" data-line-number="27">      torch<span class="op">$</span><span class="kw">masked_select</span>(grad_h, mask)<span class="op">$</span><span class="kw">fill_</span>(<span class="fl">0.0</span>)</a>
<a class="sourceLine" id="cb216-28" data-line-number="28">      grad_w1 &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">t</span>()<span class="op">$</span><span class="kw">mm</span>(grad_h)</a>
<a class="sourceLine" id="cb216-29" data-line-number="29">       </a>
<a class="sourceLine" id="cb216-30" data-line-number="30">      <span class="co"># Update weights using gradient descent</span></a>
<a class="sourceLine" id="cb216-31" data-line-number="31">      w1 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(w1, torch<span class="op">$</span><span class="kw">mul</span>(learning_rate, grad_w1))</a>
<a class="sourceLine" id="cb216-32" data-line-number="32">      w2 &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(w2, torch<span class="op">$</span><span class="kw">mul</span>(learning_rate, grad_w2))</a>
<a class="sourceLine" id="cb216-33" data-line-number="33">    }</a>
<a class="sourceLine" id="cb216-34" data-line-number="34">    </a>
<a class="sourceLine" id="cb216-35" data-line-number="35">    <span class="kw">data.frame</span>(<span class="dt">y =</span> y<span class="op">$</span><span class="kw">flatten</span>()<span class="op">$</span><span class="kw">numpy</span>(), </a>
<a class="sourceLine" id="cb216-36" data-line-number="36">                        <span class="dt">y_pred =</span> y_pred<span class="op">$</span><span class="kw">flatten</span>()<span class="op">$</span><span class="kw">numpy</span>(), <span class="dt">iter =</span> iterations)</a>
<a class="sourceLine" id="cb216-37" data-line-number="37"></a>
<a class="sourceLine" id="cb216-38" data-line-number="38">}</a>
<a class="sourceLine" id="cb216-39" data-line-number="39"></a>
<a class="sourceLine" id="cb216-40" data-line-number="40">df_<span class="dv">100</span> &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">iterations =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb216-41" data-line-number="41"><span class="kw">ggplot</span>(df_<span class="dv">100</span>, <span class="kw">aes</span>(<span class="dt">x =</span> y_pred, <span class="dt">y =</span> y)) <span class="op">+</span></a>
<a class="sourceLine" id="cb216-42" data-line-number="42"><span class="st">    </span><span class="kw">geom_point</span>()</a></code></pre></div>
<p><img src="0501-neural_networks_files/figure-html/run-model-100-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Still there are differences between the value and the prediction. Letâ€™s try with more iterations, like 250:</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb217-1" data-line-number="1">df_<span class="dv">250</span> &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">iterations =</span> <span class="dv">200</span>)</a>
<a class="sourceLine" id="cb217-2" data-line-number="2"></a>
<a class="sourceLine" id="cb217-3" data-line-number="3"><span class="kw">ggplot</span>(df_<span class="dv">250</span>, <span class="kw">aes</span>(<span class="dt">x =</span> y_pred, <span class="dt">y =</span> y)) <span class="op">+</span></a>
<a class="sourceLine" id="cb217-4" data-line-number="4"><span class="st">    </span><span class="kw">geom_point</span>()</a></code></pre></div>
<p><img src="0501-neural_networks_files/figure-html/unnamed-chunk-2-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We see the formation of a line between the values and prediction, which means we are getting closer at finding the right algorithm, in this particular case, weights and bias.</p>
<p>Letâ€™s try one more time with 500 iterations:</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb218-1" data-line-number="1">df_<span class="dv">500</span> &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">iterations =</span> <span class="dv">500</span>)</a>
<a class="sourceLine" id="cb218-2" data-line-number="2"></a>
<a class="sourceLine" id="cb218-3" data-line-number="3"><span class="kw">ggplot</span>(df_<span class="dv">500</span>, <span class="kw">aes</span>(<span class="dt">x =</span> y_pred, <span class="dt">y =</span> y)) <span class="op">+</span></a>
<a class="sourceLine" id="cb218-4" data-line-number="4"><span class="st">    </span><span class="kw">geom_point</span>()</a></code></pre></div>
<p><img src="0501-neural_networks_files/figure-html/unnamed-chunk-3-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="original-pytorch-code" class="section level2">
<h2><span class="header-section-number">10.5</span> Original PyTorch code</h2>
<p>This code will not execute. It is shown here for reference. The running code will be written in <strong>rTorch</strong>.</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb219-1" data-line-number="1"><span class="co"># Code in file tensor/two_layer_net_tensor.py</span></a>
<a class="sourceLine" id="cb219-2" data-line-number="2"><span class="im">import</span> torch</a>
<a class="sourceLine" id="cb219-3" data-line-number="3"></a>
<a class="sourceLine" id="cb219-4" data-line-number="4">device <span class="op">=</span> torch.device(<span class="st">&#39;cpu&#39;</span>)</a>
<a class="sourceLine" id="cb219-5" data-line-number="5"><span class="co"># device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU</span></a>
<a class="sourceLine" id="cb219-6" data-line-number="6"></a>
<a class="sourceLine" id="cb219-7" data-line-number="7"><span class="co"># N is batch size; D_in is input dimension;</span></a>
<a class="sourceLine" id="cb219-8" data-line-number="8"><span class="co"># H is hidden dimension; D_out is output dimension.</span></a>
<a class="sourceLine" id="cb219-9" data-line-number="9">N, D_in, H, D_out <span class="op">=</span> <span class="dv">64</span>, <span class="dv">1000</span>, <span class="dv">100</span>, <span class="dv">10</span></a>
<a class="sourceLine" id="cb219-10" data-line-number="10"></a>
<a class="sourceLine" id="cb219-11" data-line-number="11"><span class="co"># Create random input and output data</span></a>
<a class="sourceLine" id="cb219-12" data-line-number="12">x <span class="op">=</span> torch.randn(N, D_in, device<span class="op">=</span>device)</a>
<a class="sourceLine" id="cb219-13" data-line-number="13">y <span class="op">=</span> torch.randn(N, D_out, device<span class="op">=</span>device)</a>
<a class="sourceLine" id="cb219-14" data-line-number="14"></a>
<a class="sourceLine" id="cb219-15" data-line-number="15"><span class="co"># Randomly initialize weights</span></a>
<a class="sourceLine" id="cb219-16" data-line-number="16">w1 <span class="op">=</span> torch.randn(D_in, H, device<span class="op">=</span>device)</a>
<a class="sourceLine" id="cb219-17" data-line-number="17">w2 <span class="op">=</span> torch.randn(H, D_out, device<span class="op">=</span>device)</a>
<a class="sourceLine" id="cb219-18" data-line-number="18"></a>
<a class="sourceLine" id="cb219-19" data-line-number="19">learning_rate <span class="op">=</span> <span class="fl">1e-6</span></a>
<a class="sourceLine" id="cb219-20" data-line-number="20"><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</a>
<a class="sourceLine" id="cb219-21" data-line-number="21">  <span class="co"># Forward pass: compute predicted y</span></a>
<a class="sourceLine" id="cb219-22" data-line-number="22">  h <span class="op">=</span> x.mm(w1)</a>
<a class="sourceLine" id="cb219-23" data-line-number="23">  h_relu <span class="op">=</span> h.clamp(<span class="bu">min</span><span class="op">=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb219-24" data-line-number="24">  y_pred <span class="op">=</span> h_relu.mm(w2)</a>
<a class="sourceLine" id="cb219-25" data-line-number="25"></a>
<a class="sourceLine" id="cb219-26" data-line-number="26">  <span class="co"># Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor</span></a>
<a class="sourceLine" id="cb219-27" data-line-number="27">  <span class="co"># of shape (); we can get its value as a Python number with loss.item().</span></a>
<a class="sourceLine" id="cb219-28" data-line-number="28">  loss <span class="op">=</span> (y_pred <span class="op">-</span> y).<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>()</a>
<a class="sourceLine" id="cb219-29" data-line-number="29">  <span class="bu">print</span>(t, loss.item())</a>
<a class="sourceLine" id="cb219-30" data-line-number="30"></a>
<a class="sourceLine" id="cb219-31" data-line-number="31">  <span class="co"># Backprop to compute gradients of w1 and w2 with respect to loss</span></a>
<a class="sourceLine" id="cb219-32" data-line-number="32">  grad_y_pred <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> (y_pred <span class="op">-</span> y)</a>
<a class="sourceLine" id="cb219-33" data-line-number="33">  grad_w2 <span class="op">=</span> h_relu.t().mm(grad_y_pred)</a>
<a class="sourceLine" id="cb219-34" data-line-number="34">  grad_h_relu <span class="op">=</span> grad_y_pred.mm(w2.t())</a>
<a class="sourceLine" id="cb219-35" data-line-number="35">  grad_h <span class="op">=</span> grad_h_relu.clone()</a>
<a class="sourceLine" id="cb219-36" data-line-number="36">  grad_h[h <span class="op">&lt;</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb219-37" data-line-number="37">  grad_w1 <span class="op">=</span> x.t().mm(grad_h)</a>
<a class="sourceLine" id="cb219-38" data-line-number="38"></a>
<a class="sourceLine" id="cb219-39" data-line-number="39">  <span class="co"># Update weights using gradient descent</span></a>
<a class="sourceLine" id="cb219-40" data-line-number="40">  w1 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_w1</a>
<a class="sourceLine" id="cb219-41" data-line-number="41">  w2 <span class="op">-=</span> learning_rate <span class="op">*</span> grad_w2____</a></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="rainfall-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-very-simple-neural-network.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["rtorch-book.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
