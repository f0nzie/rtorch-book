<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Rainfall. Linear Regression | A Minimal rTorch Tutorial</title>
  <meta name="description" content="This is a minimal tutorial of using the rTorch package to have fun while doing machine learning. This book was produced with bookdown." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Rainfall. Linear Regression | A Minimal rTorch Tutorial" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal tutorial of using the rTorch package to have fun while doing machine learning. This book was produced with bookdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Rainfall. Linear Regression | A Minimal rTorch Tutorial" />
  
  <meta name="twitter:description" content="This is a minimal tutorial of using the rTorch package to have fun while doing machine learning. This book was produced with bookdown." />
  

<meta name="author" content="Alfonso R. Reyes" />


<meta name="date" content="2019-09-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-linear-regression.html"/>
<link rel="next" href="a-two-layer-neural-network.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal rTorch Tutorial</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prerequisites</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#installation"><i class="fa fa-check"></i>Installation</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#python-anaconda"><i class="fa fa-check"></i>Python Anaconda</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#example"><i class="fa fa-check"></i>Example</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#automatic-installation"><i class="fa fa-check"></i>Automatic installation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>I Getting Started</b></span></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#motivation"><i class="fa fa-check"></i><b>1.1</b> Motivation</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#how-do-we-start-using-rtorch"><i class="fa fa-check"></i><b>1.2</b> How do we start using <code>rTorch</code></a><ul>
<li class="chapter" data-level="1.2.1" data-path="intro.html"><a href="intro.html#getting-the-pytorch-version"><i class="fa fa-check"></i><b>1.2.1</b> Getting the PyTorch version</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro.html"><a href="intro.html#pytorch-configuration"><i class="fa fa-check"></i><b>1.2.2</b> PyTorch configuration</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#what-can-you-do-with-rtorch"><i class="fa fa-check"></i><b>1.3</b> What can you do with <code>rTorch</code></a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#the-torchvision-module"><i class="fa fa-check"></i><b>1.3.1</b> The <code>torchvision</code> module</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#np-the-numpy-module"><i class="fa fa-check"></i><b>1.3.2</b> <code>np</code>: the <code>numpy</code> module</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#python-built-in-functions"><i class="fa fa-check"></i><b>1.3.3</b> Python built-in functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html"><i class="fa fa-check"></i><b>2</b> rTorch vs PyTorch: Whatâ€™s different</a><ul>
<li class="chapter" data-level="2.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#calling-objects-from-pytorch"><i class="fa fa-check"></i><b>2.1</b> Calling objects from PyTorch</a></li>
<li class="chapter" data-level="2.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#call-a-module-from-pytorch"><i class="fa fa-check"></i><b>2.2</b> Call a module from PyTorch</a></li>
<li class="chapter" data-level="2.3" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#show-the-attributes-methods-of-a-class-or-pytorch-object"><i class="fa fa-check"></i><b>2.3</b> Show the attributes (methods) of a class or PyTorch object</a></li>
<li class="chapter" data-level="2.4" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#enumeration"><i class="fa fa-check"></i><b>2.4</b> Enumeration</a></li>
<li class="chapter" data-level="2.5" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#how-to-iterate"><i class="fa fa-check"></i><b>2.5</b> How to iterate</a><ul>
<li class="chapter" data-level="2.5.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#using-enumerate-and-iterate"><i class="fa fa-check"></i><b>2.5.1</b> Using <code>enumerate</code> and <code>iterate</code></a></li>
<li class="chapter" data-level="2.5.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#using-a-for-loop-to-iterate"><i class="fa fa-check"></i><b>2.5.2</b> Using a <code>for-loop</code> to iterate</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#zero-gradient"><i class="fa fa-check"></i><b>2.6</b> Zero gradient</a><ul>
<li class="chapter" data-level="2.6.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#version-in-python"><i class="fa fa-check"></i><b>2.6.1</b> Version in Python</a></li>
<li class="chapter" data-level="2.6.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#version-in-r"><i class="fa fa-check"></i><b>2.6.2</b> Version in R</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#transform-a-tensor"><i class="fa fa-check"></i><b>2.7</b> Transform a tensor</a></li>
<li class="chapter" data-level="2.8" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#build-a-model-class"><i class="fa fa-check"></i><b>2.8</b> Build a model class</a><ul>
<li class="chapter" data-level="2.8.1" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#example-1"><i class="fa fa-check"></i><b>2.8.1</b> Example 1</a></li>
<li class="chapter" data-level="2.8.2" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#example-2"><i class="fa fa-check"></i><b>2.8.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#convert-a-tensor-to-numpy-object"><i class="fa fa-check"></i><b>2.9</b> Convert a tensor to <code>numpy</code> object</a></li>
<li class="chapter" data-level="2.10" data-path="rtorch-vs-pytorch-whats-different.html"><a href="rtorch-vs-pytorch-whats-different.html#convert-a-numpy-object-to-an-r-object"><i class="fa fa-check"></i><b>2.10</b> Convert a <code>numpy</code> object to an <code>R</code> object</a></li>
</ul></li>
<li class="part"><span><b>II Basic Tensor Operations</b></span></li>
<li class="chapter" data-level="3" data-path="tensors.html"><a href="tensors.html"><i class="fa fa-check"></i><b>3</b> Tensors</a><ul>
<li class="chapter" data-level="3.1" data-path="tensors.html"><a href="tensors.html#tensor-data-types"><i class="fa fa-check"></i><b>3.1</b> Tensor data types</a></li>
<li class="chapter" data-level="3.2" data-path="tensors.html"><a href="tensors.html#arithmetic-of-tensors"><i class="fa fa-check"></i><b>3.2</b> Arithmetic of tensors</a><ul>
<li class="chapter" data-level="3.2.1" data-path="tensors.html"><a href="tensors.html#add-tensors"><i class="fa fa-check"></i><b>3.2.1</b> Add tensors</a></li>
<li class="chapter" data-level="3.2.2" data-path="tensors.html"><a href="tensors.html#multiply-a-tensor-by-a-scalar"><i class="fa fa-check"></i><b>3.2.2</b> Multiply a tensor by a scalar</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="tensors.html"><a href="tensors.html#numpy-and-pytorch"><i class="fa fa-check"></i><b>3.3</b> NumPy and PyTorch</a><ul>
<li class="chapter" data-level="3.3.1" data-path="tensors.html"><a href="tensors.html#tuples-python-and-vectors-r"><i class="fa fa-check"></i><b>3.3.1</b> Tuples (Python) and vectors (R)</a></li>
<li class="chapter" data-level="3.3.2" data-path="tensors.html"><a href="tensors.html#make-a-numpy-array-a-tensor-with-as_tensor"><i class="fa fa-check"></i><b>3.3.2</b> Make a numpy array a tensor with <code>as_tensor()</code></a></li>
<li class="chapter" data-level="3.3.3" data-path="tensors.html"><a href="tensors.html#tensor-to-array-and-viceversa"><i class="fa fa-check"></i><b>3.3.3</b> Tensor to array, and viceversa</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="tensors.html"><a href="tensors.html#create-tensors"><i class="fa fa-check"></i><b>3.4</b> Create tensors</a></li>
<li class="chapter" data-level="3.5" data-path="tensors.html"><a href="tensors.html#tensor-resizing"><i class="fa fa-check"></i><b>3.5</b> Tensor resizing</a><ul>
<li class="chapter" data-level="3.5.1" data-path="tensors.html"><a href="tensors.html#concatenate-tensors"><i class="fa fa-check"></i><b>3.5.1</b> Concatenate tensors</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="tensors.html"><a href="tensors.html#reshape-tensors"><i class="fa fa-check"></i><b>3.6</b> Reshape tensors</a><ul>
<li class="chapter" data-level="3.6.1" data-path="tensors.html"><a href="tensors.html#with-function-chunk"><i class="fa fa-check"></i><b>3.6.1</b> With function <code>chunk()</code>:</a></li>
<li class="chapter" data-level="3.6.2" data-path="tensors.html"><a href="tensors.html#with-index_select"><i class="fa fa-check"></i><b>3.6.2</b> With <code>index_select()</code>:</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="tensors.html"><a href="tensors.html#special-tensors"><i class="fa fa-check"></i><b>3.7</b> Special tensors</a><ul>
<li class="chapter" data-level="3.7.1" data-path="tensors.html"><a href="tensors.html#identity-matrix"><i class="fa fa-check"></i><b>3.7.1</b> Identity matrix</a></li>
<li class="chapter" data-level="3.7.2" data-path="tensors.html"><a href="tensors.html#ones"><i class="fa fa-check"></i><b>3.7.2</b> Ones</a></li>
<li class="chapter" data-level="3.7.3" data-path="tensors.html"><a href="tensors.html#zeros"><i class="fa fa-check"></i><b>3.7.3</b> Zeros</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="tensors.html"><a href="tensors.html#tensor-fill"><i class="fa fa-check"></i><b>3.8</b> Tensor fill</a><ul>
<li class="chapter" data-level="3.8.1" data-path="tensors.html"><a href="tensors.html#initialize-a-linear-or-log-scale-tensor"><i class="fa fa-check"></i><b>3.8.1</b> Initialize a linear or log scale Tensor</a></li>
<li class="chapter" data-level="3.8.2" data-path="tensors.html"><a href="tensors.html#inplace-out-of-place"><i class="fa fa-check"></i><b>3.8.2</b> Inplace / Out-of-place</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="tensors.html"><a href="tensors.html#access-to-tensor-elements"><i class="fa fa-check"></i><b>3.9</b> Access to tensor elements</a><ul>
<li class="chapter" data-level="3.9.1" data-path="tensors.html"><a href="tensors.html#using-indices-to-access-elements"><i class="fa fa-check"></i><b>3.9.1</b> Using indices to access elements</a></li>
<li class="chapter" data-level="3.9.2" data-path="tensors.html"><a href="tensors.html#using-the-take-function"><i class="fa fa-check"></i><b>3.9.2</b> Using the <code>take</code> function</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="tensors.html"><a href="tensors.html#other-tensor-operations"><i class="fa fa-check"></i><b>3.10</b> Other tensor operations</a><ul>
<li class="chapter" data-level="3.10.1" data-path="tensors.html"><a href="tensors.html#cross-product"><i class="fa fa-check"></i><b>3.10.1</b> Cross product</a></li>
<li class="chapter" data-level="3.10.2" data-path="tensors.html"><a href="tensors.html#dot-product"><i class="fa fa-check"></i><b>3.10.2</b> Dot product</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="tensors.html"><a href="tensors.html#logical-operations"><i class="fa fa-check"></i><b>3.11</b> Logical operations</a><ul>
<li class="chapter" data-level="3.11.1" data-path="tensors.html"><a href="tensors.html#logical-not"><i class="fa fa-check"></i><b>3.11.1</b> Logical NOT</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="tensors.html"><a href="tensors.html#distributions"><i class="fa fa-check"></i><b>3.12</b> Distributions</a><ul>
<li class="chapter" data-level="3.12.1" data-path="tensors.html"><a href="tensors.html#uniform-matrix"><i class="fa fa-check"></i><b>3.12.1</b> Uniform matrix</a></li>
<li class="chapter" data-level="3.12.2" data-path="tensors.html"><a href="tensors.html#binomial-distribution"><i class="fa fa-check"></i><b>3.12.2</b> Binomial distribution</a></li>
<li class="chapter" data-level="3.12.3" data-path="tensors.html"><a href="tensors.html#exponential-distribution"><i class="fa fa-check"></i><b>3.12.3</b> Exponential distribution</a></li>
<li class="chapter" data-level="3.12.4" data-path="tensors.html"><a href="tensors.html#weibull-distribution"><i class="fa fa-check"></i><b>3.12.4</b> Weibull distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linearalgebra.html"><a href="linearalgebra.html"><i class="fa fa-check"></i><b>4</b> Linear Algebra with Torch</a><ul>
<li class="chapter" data-level="4.1" data-path="linearalgebra.html"><a href="linearalgebra.html#scalars"><i class="fa fa-check"></i><b>4.1</b> Scalars</a></li>
<li class="chapter" data-level="4.2" data-path="linearalgebra.html"><a href="linearalgebra.html#vectors"><i class="fa fa-check"></i><b>4.2</b> Vectors</a></li>
<li class="chapter" data-level="4.3" data-path="linearalgebra.html"><a href="linearalgebra.html#matrices"><i class="fa fa-check"></i><b>4.3</b> Matrices</a></li>
<li class="chapter" data-level="4.4" data-path="linearalgebra.html"><a href="linearalgebra.html#d-tensors"><i class="fa fa-check"></i><b>4.4</b> 3D+ tensors</a></li>
<li class="chapter" data-level="4.5" data-path="linearalgebra.html"><a href="linearalgebra.html#transpose-of-a-matrix"><i class="fa fa-check"></i><b>4.5</b> Transpose of a matrix</a></li>
<li class="chapter" data-level="4.6" data-path="linearalgebra.html"><a href="linearalgebra.html#vectors-special-case-of-a-matrix"><i class="fa fa-check"></i><b>4.6</b> Vectors, special case of a matrix</a></li>
<li class="chapter" data-level="4.7" data-path="linearalgebra.html"><a href="linearalgebra.html#tensor-arithmetic"><i class="fa fa-check"></i><b>4.7</b> Tensor arithmetic</a></li>
<li class="chapter" data-level="4.8" data-path="linearalgebra.html"><a href="linearalgebra.html#add-a-scalar-to-a-tensor"><i class="fa fa-check"></i><b>4.8</b> Add a scalar to a tensor</a></li>
<li class="chapter" data-level="4.9" data-path="linearalgebra.html"><a href="linearalgebra.html#multiplying-tensors"><i class="fa fa-check"></i><b>4.9</b> Multiplying tensors</a></li>
<li class="chapter" data-level="4.10" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product-1"><i class="fa fa-check"></i><b>4.10</b> Dot product</a></li>
</ul></li>
<li class="part"><span><b>III Logistic Regression</b></span></li>
<li class="chapter" data-level="5" data-path="mnistdigits.html"><a href="mnistdigits.html"><i class="fa fa-check"></i><b>5</b> Example 1: MNIST handwritten digits</a><ul>
<li class="chapter" data-level="5.1" data-path="mnistdigits.html"><a href="mnistdigits.html#hyperparameters"><i class="fa fa-check"></i><b>5.1</b> Hyperparameters</a></li>
<li class="chapter" data-level="5.2" data-path="mnistdigits.html"><a href="mnistdigits.html#read-datasets"><i class="fa fa-check"></i><b>5.2</b> Read datasets</a></li>
<li class="chapter" data-level="5.3" data-path="mnistdigits.html"><a href="mnistdigits.html#define-the-model"><i class="fa fa-check"></i><b>5.3</b> Define the model</a></li>
<li class="chapter" data-level="5.4" data-path="mnistdigits.html"><a href="mnistdigits.html#training"><i class="fa fa-check"></i><b>5.4</b> Training</a></li>
<li class="chapter" data-level="5.5" data-path="mnistdigits.html"><a href="mnistdigits.html#prediction"><i class="fa fa-check"></i><b>5.5</b> Prediction</a></li>
<li class="chapter" data-level="5.6" data-path="mnistdigits.html"><a href="mnistdigits.html#save-the-model"><i class="fa fa-check"></i><b>5.6</b> Save the model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="the-sigmoid-function.html"><a href="the-sigmoid-function.html"><i class="fa fa-check"></i><b>6</b> The Sigmoid function</a></li>
<li class="chapter" data-level="7" data-path="a-classic-classification-problem.html"><a href="a-classic-classification-problem.html"><i class="fa fa-check"></i><b>7</b> A classic classification problem</a></li>
<li class="part"><span><b>IV Linear Regression</b></span></li>
<li class="chapter" data-level="8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>8</b> Simple linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#generate-the-dataset"><i class="fa fa-check"></i><b>8.2</b> Generate the dataset</a></li>
<li class="chapter" data-level="8.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#convert-arrays-to-tensors"><i class="fa fa-check"></i><b>8.3</b> Convert arrays to tensors</a></li>
<li class="chapter" data-level="8.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#converting-from-numpy-to-tensor"><i class="fa fa-check"></i><b>8.4</b> Converting from numpy to tensor</a></li>
<li class="chapter" data-level="8.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#creating-the-network-model"><i class="fa fa-check"></i><b>8.5</b> Creating the network model</a></li>
<li class="chapter" data-level="8.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#optimizer-and-loss"><i class="fa fa-check"></i><b>8.6</b> Optimizer and Loss</a></li>
<li class="chapter" data-level="8.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#training-1"><i class="fa fa-check"></i><b>8.7</b> Training</a></li>
<li class="chapter" data-level="8.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#results"><i class="fa fa-check"></i><b>8.8</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html"><i class="fa fa-check"></i><b>9</b> Rainfall. Linear Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#training-data"><i class="fa fa-check"></i><b>9.1</b> Training data</a></li>
<li class="chapter" data-level="9.2" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#convert-arrays-to-tensors-1"><i class="fa fa-check"></i><b>9.2</b> Convert arrays to tensors</a></li>
<li class="chapter" data-level="9.3" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#build-the-model"><i class="fa fa-check"></i><b>9.3</b> Build the model</a></li>
<li class="chapter" data-level="9.4" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#generate-predictions"><i class="fa fa-check"></i><b>9.4</b> Generate predictions</a></li>
<li class="chapter" data-level="9.5" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#loss-function"><i class="fa fa-check"></i><b>9.5</b> Loss Function</a></li>
<li class="chapter" data-level="9.6" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#step-by-step-process"><i class="fa fa-check"></i><b>9.6</b> Step by step process</a><ul>
<li class="chapter" data-level="9.6.1" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#compute-the-losses"><i class="fa fa-check"></i><b>9.6.1</b> Compute the losses</a></li>
<li class="chapter" data-level="9.6.2" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#compute-gradients"><i class="fa fa-check"></i><b>9.6.2</b> Compute Gradients</a></li>
<li class="chapter" data-level="9.6.3" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#reset-the-gradients"><i class="fa fa-check"></i><b>9.6.3</b> Reset the gradients</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="rainfall-linear-regression.html"><a href="rainfall-linear-regression.html#all-together-train-for-multiple-epochs"><i class="fa fa-check"></i><b>9.7</b> All together: train for multiple epochs</a></li>
</ul></li>
<li class="part"><span><b>V Neural Networks</b></span></li>
<li class="chapter" data-level="10" data-path="a-two-layer-neural-network.html"><a href="a-two-layer-neural-network.html"><i class="fa fa-check"></i><b>10</b> A two-layer neural network</a><ul>
<li class="chapter" data-level="10.1" data-path="a-two-layer-neural-network.html"><a href="a-two-layer-neural-network.html#load-the-libraries"><i class="fa fa-check"></i><b>10.1</b> Load the libraries</a></li>
<li class="chapter" data-level="10.2" data-path="a-two-layer-neural-network.html"><a href="a-two-layer-neural-network.html#dataset"><i class="fa fa-check"></i><b>10.2</b> Dataset</a></li>
<li class="chapter" data-level="10.3" data-path="a-two-layer-neural-network.html"><a href="a-two-layer-neural-network.html#run-the-model-for-50-iterations"><i class="fa fa-check"></i><b>10.3</b> Run the model for 50 iterations</a></li>
<li class="chapter" data-level="10.4" data-path="a-two-layer-neural-network.html"><a href="a-two-layer-neural-network.html#run-it-at-100-iterations"><i class="fa fa-check"></i><b>10.4</b> Run it at 100 iterations</a></li>
<li class="chapter" data-level="10.5" data-path="a-two-layer-neural-network.html"><a href="a-two-layer-neural-network.html#original-pytorch-code"><i class="fa fa-check"></i><b>10.5</b> Original PyTorch code</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html"><i class="fa fa-check"></i><b>11</b> A very simple neural network</a><ul>
<li class="chapter" data-level="11.1" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#introduction-1"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#select-device"><i class="fa fa-check"></i><b>11.2</b> Select device</a></li>
<li class="chapter" data-level="11.3" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#create-the-dataset"><i class="fa fa-check"></i><b>11.3</b> Create the dataset</a></li>
<li class="chapter" data-level="11.4" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#define-the-model-1"><i class="fa fa-check"></i><b>11.4</b> Define the model</a></li>
<li class="chapter" data-level="11.5" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#loss-function-1"><i class="fa fa-check"></i><b>11.5</b> Loss function</a></li>
<li class="chapter" data-level="11.6" data-path="a-very-simple-neural-network.html"><a href="a-very-simple-neural-network.html#iterate-through-batches"><i class="fa fa-check"></i><b>11.6</b> Iterate through batches</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="neural-networks-2.html"><a href="neural-networks-2.html"><i class="fa fa-check"></i><b>12</b> Neural Networks 2</a><ul>
<li class="chapter" data-level="12.1" data-path="neural-networks-2.html"><a href="neural-networks-2.html#nn2-1"><i class="fa fa-check"></i><b>12.1</b> nn2 1</a></li>
<li class="chapter" data-level="12.2" data-path="neural-networks-2.html"><a href="neural-networks-2.html#nn2-2"><i class="fa fa-check"></i><b>12.2</b> nn2 2</a></li>
</ul></li>
<li class="part"><span><b>VI PyTorch and R data structures</b></span></li>
<li class="chapter" data-level="13" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html"><i class="fa fa-check"></i><b>13</b> Working with data.frame</a><ul>
<li class="chapter" data-level="13.1" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#load-pytorch-libraries"><i class="fa fa-check"></i><b>13.1</b> Load PyTorch libraries</a></li>
<li class="chapter" data-level="13.2" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#load-dataset"><i class="fa fa-check"></i><b>13.2</b> Load dataset</a></li>
<li class="chapter" data-level="13.3" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#summary-statistics-for-tensors"><i class="fa fa-check"></i><b>13.3</b> Summary statistics for tensors</a><ul>
<li class="chapter" data-level="13.3.1" data-path="working-with-data-frame.html"><a href="working-with-data-frame.html#using-data.frame"><i class="fa fa-check"></i><b>13.3.1</b> using <code>data.frame</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="working-with-data-table.html"><a href="working-with-data-table.html"><i class="fa fa-check"></i><b>14</b> Working with data.table</a><ul>
<li class="chapter" data-level="14.1" data-path="working-with-data-table.html"><a href="working-with-data-table.html#load-pytorch-libraries-1"><i class="fa fa-check"></i><b>14.1</b> Load PyTorch libraries</a></li>
<li class="chapter" data-level="14.2" data-path="working-with-data-table.html"><a href="working-with-data-table.html#load-dataset-1"><i class="fa fa-check"></i><b>14.2</b> Load dataset</a></li>
<li class="chapter" data-level="14.3" data-path="working-with-data-table.html"><a href="working-with-data-table.html#read-the-datasets-without-normalization"><i class="fa fa-check"></i><b>14.3</b> Read the datasets without normalization</a></li>
<li class="chapter" data-level="14.4" data-path="working-with-data-table.html"><a href="working-with-data-table.html#using-data.table"><i class="fa fa-check"></i><b>14.4</b> Using <code>data.table</code></a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendixA.html"><a href="appendixA.html"><i class="fa fa-check"></i><b>A</b> Statistical Background</a><ul>
<li class="chapter" data-level="A.1" data-path="appendixA.html"><a href="appendixA.html#basic-statistical-terms"><i class="fa fa-check"></i><b>A.1</b> Basic statistical terms</a><ul>
<li class="chapter" data-level="A.1.1" data-path="appendixA.html"><a href="appendixA.html#five-number-summary"><i class="fa fa-check"></i><b>A.1.1</b> Five-number summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Minimal rTorch Tutorial</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="rainfall.-linear-regression" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Rainfall. Linear Regression</h1>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb187-1" data-line-number="1"><span class="kw">library</span>(rTorch)</a></code></pre></div>
<p>Select the device: CPU or GPU</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb188-1" data-line-number="1">torch<span class="op">$</span><span class="kw">manual_seed</span>(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb188-2" data-line-number="2"><span class="co">#&gt; &lt;torch._C.Generator&gt;</span></a>
<a class="sourceLine" id="cb188-3" data-line-number="3"></a>
<a class="sourceLine" id="cb188-4" data-line-number="4">device =<span class="st"> </span>torch<span class="op">$</span><span class="kw">device</span>(<span class="st">&#39;cpu&#39;</span>)</a></code></pre></div>
<div id="training-data" class="section level2">
<h2><span class="header-section-number">9.1</span> Training data</h2>
<p>The training data can be represented using 2 matrices (inputs and targets), each with one row per observation, and one column per variable.</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb189-1" data-line-number="1"><span class="co"># Input (temp, rainfall, humidity)</span></a>
<a class="sourceLine" id="cb189-2" data-line-number="2">inputs =<span class="st"> </span>np<span class="op">$</span><span class="kw">array</span>(<span class="kw">list</span>(<span class="kw">list</span>(<span class="dv">73</span>, <span class="dv">67</span>, <span class="dv">43</span>),</a>
<a class="sourceLine" id="cb189-3" data-line-number="3">                   <span class="kw">list</span>(<span class="dv">91</span>, <span class="dv">88</span>, <span class="dv">64</span>),</a>
<a class="sourceLine" id="cb189-4" data-line-number="4">                   <span class="kw">list</span>(<span class="dv">87</span>, <span class="dv">134</span>, <span class="dv">58</span>),</a>
<a class="sourceLine" id="cb189-5" data-line-number="5">                   <span class="kw">list</span>(<span class="dv">102</span>, <span class="dv">43</span>, <span class="dv">37</span>),</a>
<a class="sourceLine" id="cb189-6" data-line-number="6">                   <span class="kw">list</span>(<span class="dv">69</span>, <span class="dv">96</span>, <span class="dv">70</span>)), <span class="dt">dtype=</span><span class="st">&#39;float32&#39;</span>)</a>
<a class="sourceLine" id="cb189-7" data-line-number="7"></a>
<a class="sourceLine" id="cb189-8" data-line-number="8"><span class="co"># Targets (apples, oranges)</span></a>
<a class="sourceLine" id="cb189-9" data-line-number="9">targets =<span class="st"> </span>np<span class="op">$</span><span class="kw">array</span>(<span class="kw">list</span>(<span class="kw">list</span>(<span class="dv">56</span>, <span class="dv">70</span>), </a>
<a class="sourceLine" id="cb189-10" data-line-number="10">                    <span class="kw">list</span>(<span class="dv">81</span>, <span class="dv">101</span>),</a>
<a class="sourceLine" id="cb189-11" data-line-number="11">                    <span class="kw">list</span>(<span class="dv">119</span>, <span class="dv">133</span>),</a>
<a class="sourceLine" id="cb189-12" data-line-number="12">                    <span class="kw">list</span>(<span class="dv">22</span>, <span class="dv">37</span>), </a>
<a class="sourceLine" id="cb189-13" data-line-number="13">                    <span class="kw">list</span>(<span class="dv">103</span>, <span class="dv">119</span>)), <span class="dt">dtype=</span><span class="st">&#39;float32&#39;</span>)</a></code></pre></div>
</div>
<div id="convert-arrays-to-tensors-1" class="section level2">
<h2><span class="header-section-number">9.2</span> Convert arrays to tensors</h2>
<p>Before we build a model, we need to convert inputs and targets to PyTorch tensors.</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb190-1" data-line-number="1"><span class="co"># Convert inputs and targets to tensors</span></a>
<a class="sourceLine" id="cb190-2" data-line-number="2">inputs =<span class="st"> </span>torch<span class="op">$</span><span class="kw">from_numpy</span>(inputs)</a>
<a class="sourceLine" id="cb190-3" data-line-number="3">targets =<span class="st"> </span>torch<span class="op">$</span><span class="kw">from_numpy</span>(targets)</a>
<a class="sourceLine" id="cb190-4" data-line-number="4"></a>
<a class="sourceLine" id="cb190-5" data-line-number="5"><span class="kw">print</span>(inputs)</a>
<a class="sourceLine" id="cb190-6" data-line-number="6"><span class="co">#&gt; tensor([[ 73.,  67.,  43.],</span></a>
<a class="sourceLine" id="cb190-7" data-line-number="7"><span class="co">#&gt;         [ 91.,  88.,  64.],</span></a>
<a class="sourceLine" id="cb190-8" data-line-number="8"><span class="co">#&gt;         [ 87., 134.,  58.],</span></a>
<a class="sourceLine" id="cb190-9" data-line-number="9"><span class="co">#&gt;         [102.,  43.,  37.],</span></a>
<a class="sourceLine" id="cb190-10" data-line-number="10"><span class="co">#&gt;         [ 69.,  96.,  70.]], dtype=torch.float64)</span></a>
<a class="sourceLine" id="cb190-11" data-line-number="11"><span class="kw">print</span>(targets)</a>
<a class="sourceLine" id="cb190-12" data-line-number="12"><span class="co">#&gt; tensor([[ 56.,  70.],</span></a>
<a class="sourceLine" id="cb190-13" data-line-number="13"><span class="co">#&gt;         [ 81., 101.],</span></a>
<a class="sourceLine" id="cb190-14" data-line-number="14"><span class="co">#&gt;         [119., 133.],</span></a>
<a class="sourceLine" id="cb190-15" data-line-number="15"><span class="co">#&gt;         [ 22.,  37.],</span></a>
<a class="sourceLine" id="cb190-16" data-line-number="16"><span class="co">#&gt;         [103., 119.]], dtype=torch.float64)</span></a></code></pre></div>
<p>The weights and biases can also be represented as matrices, initialized with random values. The first row of <span class="math inline">\(w\)</span> and the first element of <span class="math inline">\(b\)</span> are used to predict the first target variable, i.e.Â yield for apples, and, similarly, the second for oranges.</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb191-1" data-line-number="1"><span class="co"># random numbers for weights and biases. Then convert to double()</span></a>
<a class="sourceLine" id="cb191-2" data-line-number="2">torch<span class="op">$</span><span class="kw">set_default_dtype</span>(torch<span class="op">$</span>double)</a>
<a class="sourceLine" id="cb191-3" data-line-number="3"></a>
<a class="sourceLine" id="cb191-4" data-line-number="4">w =<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(2L, 3L, <span class="dt">requires_grad=</span><span class="ot">TRUE</span>)  <span class="co">#$double()</span></a>
<a class="sourceLine" id="cb191-5" data-line-number="5">b =<span class="st"> </span>torch<span class="op">$</span><span class="kw">randn</span>(2L, <span class="dt">requires_grad=</span><span class="ot">TRUE</span>)      <span class="co">#$double()</span></a>
<a class="sourceLine" id="cb191-6" data-line-number="6"></a>
<a class="sourceLine" id="cb191-7" data-line-number="7"><span class="kw">print</span>(w)</a>
<a class="sourceLine" id="cb191-8" data-line-number="8"><span class="co">#&gt; tensor([[ 1.5410, -0.2934, -2.1788],</span></a>
<a class="sourceLine" id="cb191-9" data-line-number="9"><span class="co">#&gt;         [ 0.5684, -1.0845, -1.3986]], requires_grad=True)</span></a>
<a class="sourceLine" id="cb191-10" data-line-number="10"><span class="kw">print</span>(b)</a>
<a class="sourceLine" id="cb191-11" data-line-number="11"><span class="co">#&gt; tensor([0.4033, 0.8380], requires_grad=True)</span></a></code></pre></div>
</div>
<div id="build-the-model" class="section level2">
<h2><span class="header-section-number">9.3</span> Build the model</h2>
<p>The model is simply a function that performs a matrix multiplication of the input <span class="math inline">\(x\)</span> and the weights <span class="math inline">\(w\)</span> (transposed), and adds the bias <span class="math inline">\(b\)</span> (replicated for each observation).</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb192-1" data-line-number="1">model &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb192-2" data-line-number="2">  wt &lt;-<span class="st"> </span>w<span class="op">$</span><span class="kw">t</span>()</a>
<a class="sourceLine" id="cb192-3" data-line-number="3">  <span class="kw">return</span>(torch<span class="op">$</span><span class="kw">add</span>(torch<span class="op">$</span><span class="kw">mm</span>(x, wt), b))</a>
<a class="sourceLine" id="cb192-4" data-line-number="4">}</a></code></pre></div>
</div>
<div id="generate-predictions" class="section level2">
<h2><span class="header-section-number">9.4</span> Generate predictions</h2>
<p>The matrix obtained by passing the input data to the model is a set of predictions for the target variables.</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb193-1" data-line-number="1"><span class="co"># Generate predictions</span></a>
<a class="sourceLine" id="cb193-2" data-line-number="2">preds =<span class="st"> </span><span class="kw">model</span>(inputs)</a>
<a class="sourceLine" id="cb193-3" data-line-number="3"><span class="kw">print</span>(preds)</a>
<a class="sourceLine" id="cb193-4" data-line-number="4"><span class="co">#&gt; tensor([[  -0.4516,  -90.4691],</span></a>
<a class="sourceLine" id="cb193-5" data-line-number="5"><span class="co">#&gt;         [ -24.6303, -132.3828],</span></a>
<a class="sourceLine" id="cb193-6" data-line-number="6"><span class="co">#&gt;         [ -31.2192, -176.1530],</span></a>
<a class="sourceLine" id="cb193-7" data-line-number="7"><span class="co">#&gt;         [  64.3523,  -39.5645],</span></a>
<a class="sourceLine" id="cb193-8" data-line-number="8"><span class="co">#&gt;         [ -73.9524, -161.9560]], grad_fn=&lt;AddBackward0&gt;)</span></a></code></pre></div>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb194-1" data-line-number="1"><span class="co"># Compare with targets</span></a>
<a class="sourceLine" id="cb194-2" data-line-number="2"><span class="kw">print</span>(targets)</a>
<a class="sourceLine" id="cb194-3" data-line-number="3"><span class="co">#&gt; tensor([[ 56.,  70.],</span></a>
<a class="sourceLine" id="cb194-4" data-line-number="4"><span class="co">#&gt;         [ 81., 101.],</span></a>
<a class="sourceLine" id="cb194-5" data-line-number="5"><span class="co">#&gt;         [119., 133.],</span></a>
<a class="sourceLine" id="cb194-6" data-line-number="6"><span class="co">#&gt;         [ 22.,  37.],</span></a>
<a class="sourceLine" id="cb194-7" data-line-number="7"><span class="co">#&gt;         [103., 119.]])</span></a></code></pre></div>
<p>Because weâ€™ve started with random weights and biases, the model does not a very good job of predicting the target variables.</p>
</div>
<div id="loss-function" class="section level2">
<h2><span class="header-section-number">9.5</span> Loss Function</h2>
<p>We can compare the predictions with the actual targets, using the following method:</p>
<ul>
<li>Calculate the difference between the two matrices (preds and targets).</li>
<li>Square all elements of the difference matrix to remove negative values.</li>
<li>Calculate the average of the elements in the resulting matrix.</li>
</ul>
<p>The result is a single number, known as the mean squared error (MSE).</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb195-1" data-line-number="1"><span class="co"># MSE loss</span></a>
<a class="sourceLine" id="cb195-2" data-line-number="2">mse =<span class="st"> </span><span class="cf">function</span>(t1, t2) {</a>
<a class="sourceLine" id="cb195-3" data-line-number="3">  diff &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(t1, t2)</a>
<a class="sourceLine" id="cb195-4" data-line-number="4">  mul &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">sum</span>(torch<span class="op">$</span><span class="kw">mul</span>(diff, diff))</a>
<a class="sourceLine" id="cb195-5" data-line-number="5">  <span class="kw">return</span>(torch<span class="op">$</span><span class="kw">div</span>(mul, diff<span class="op">$</span><span class="kw">numel</span>()))</a>
<a class="sourceLine" id="cb195-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb195-7" data-line-number="7"></a>
<a class="sourceLine" id="cb195-8" data-line-number="8"><span class="kw">print</span>(mse)</a>
<a class="sourceLine" id="cb195-9" data-line-number="9"><span class="co">#&gt; function(t1, t2) {</span></a>
<a class="sourceLine" id="cb195-10" data-line-number="10"><span class="co">#&gt;   diff &lt;- torch$sub(t1, t2)</span></a>
<a class="sourceLine" id="cb195-11" data-line-number="11"><span class="co">#&gt;   mul &lt;- torch$sum(torch$mul(diff, diff))</span></a>
<a class="sourceLine" id="cb195-12" data-line-number="12"><span class="co">#&gt;   return(torch$div(mul, diff$numel()))</span></a>
<a class="sourceLine" id="cb195-13" data-line-number="13"><span class="co">#&gt; }</span></a></code></pre></div>
</div>
<div id="step-by-step-process" class="section level2">
<h2><span class="header-section-number">9.6</span> Step by step process</h2>
<div id="compute-the-losses" class="section level3">
<h3><span class="header-section-number">9.6.1</span> Compute the losses</h3>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb196-1" data-line-number="1"><span class="co"># Compute loss</span></a>
<a class="sourceLine" id="cb196-2" data-line-number="2">loss =<span class="st"> </span><span class="kw">mse</span>(preds, targets)</a>
<a class="sourceLine" id="cb196-3" data-line-number="3"><span class="kw">print</span>(loss)</a>
<a class="sourceLine" id="cb196-4" data-line-number="4"><span class="co">#&gt; tensor(33060.8053, grad_fn=&lt;DivBackward0&gt;)</span></a>
<a class="sourceLine" id="cb196-5" data-line-number="5"><span class="co"># 46194</span></a>
<a class="sourceLine" id="cb196-6" data-line-number="6"><span class="co"># 33060.8070</span></a></code></pre></div>
<p>The resulting number is called the <strong>loss</strong>, because it indicates how bad the model is at predicting the target variables. Lower the loss, better the model.</p>
</div>
<div id="compute-gradients" class="section level3">
<h3><span class="header-section-number">9.6.2</span> Compute Gradients</h3>
<p>With PyTorch, we can automatically compute the gradient or derivative of the loss w.r.t. to the weights and biases, because they have <code>requires_grad</code> set to True.</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb197-1" data-line-number="1"><span class="co"># Compute gradients</span></a>
<a class="sourceLine" id="cb197-2" data-line-number="2">loss<span class="op">$</span><span class="kw">backward</span>()</a></code></pre></div>
<p>The gradients are stored in the .grad property of the respective tensors.</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb198-1" data-line-number="1"><span class="co"># Gradients for weights</span></a>
<a class="sourceLine" id="cb198-2" data-line-number="2"><span class="kw">print</span>(w)</a>
<a class="sourceLine" id="cb198-3" data-line-number="3"><span class="co">#&gt; tensor([[ 1.5410, -0.2934, -2.1788],</span></a>
<a class="sourceLine" id="cb198-4" data-line-number="4"><span class="co">#&gt;         [ 0.5684, -1.0845, -1.3986]], requires_grad=True)</span></a>
<a class="sourceLine" id="cb198-5" data-line-number="5"><span class="kw">print</span>(w<span class="op">$</span>grad)</a>
<a class="sourceLine" id="cb198-6" data-line-number="6"><span class="co">#&gt; tensor([[ -6938.4351,  -9674.6757,  -5744.0206],</span></a>
<a class="sourceLine" id="cb198-7" data-line-number="7"><span class="co">#&gt;         [-17408.7861, -20595.9333, -12453.4702]])</span></a></code></pre></div>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb199-1" data-line-number="1"><span class="co"># Gradients for bias</span></a>
<a class="sourceLine" id="cb199-2" data-line-number="2"><span class="kw">print</span>(b)</a>
<a class="sourceLine" id="cb199-3" data-line-number="3"><span class="co">#&gt; tensor([0.4033, 0.8380], requires_grad=True)</span></a>
<a class="sourceLine" id="cb199-4" data-line-number="4"><span class="kw">print</span>(b<span class="op">$</span>grad)</a>
<a class="sourceLine" id="cb199-5" data-line-number="5"><span class="co">#&gt; tensor([ -89.3802, -212.1051])</span></a></code></pre></div>
<p>A key insight from calculus is that the gradient indicates the rate of change of the loss, or the slope of the loss function w.r.t. the weights and biases.</p>
<ul>
<li>If a gradient element is positive:
<ul>
<li>increasing the elementâ€™s value slightly will increase the loss.</li>
<li>decreasing the elementâ€™s value slightly will decrease the loss.</li>
</ul></li>
<li>If a gradient element is negative,
<ul>
<li>increasing the elementâ€™s value slightly will decrease the loss.</li>
<li>decreasing the elementâ€™s value slightly will increase the loss.</li>
</ul></li>
</ul>
<p>The increase or decrease is proportional to the value of the gradient.</p>
</div>
<div id="reset-the-gradients" class="section level3">
<h3><span class="header-section-number">9.6.3</span> Reset the gradients</h3>
<p>Finally, weâ€™ll reset the gradients to zero before moving forward, because PyTorch accumulates gradients.</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb200-1" data-line-number="1"><span class="co"># Reset the gradients</span></a>
<a class="sourceLine" id="cb200-2" data-line-number="2">w<span class="op">$</span>grad<span class="op">$</span><span class="kw">zero_</span>()</a>
<a class="sourceLine" id="cb200-3" data-line-number="3"><span class="co">#&gt; tensor([[0., 0., 0.],</span></a>
<a class="sourceLine" id="cb200-4" data-line-number="4"><span class="co">#&gt;         [0., 0., 0.]])</span></a>
<a class="sourceLine" id="cb200-5" data-line-number="5">b<span class="op">$</span>grad<span class="op">$</span><span class="kw">zero_</span>()</a>
<a class="sourceLine" id="cb200-6" data-line-number="6"><span class="co">#&gt; tensor([0., 0.])</span></a>
<a class="sourceLine" id="cb200-7" data-line-number="7"></a>
<a class="sourceLine" id="cb200-8" data-line-number="8"><span class="kw">print</span>(w<span class="op">$</span>grad)</a>
<a class="sourceLine" id="cb200-9" data-line-number="9"><span class="co">#&gt; tensor([[0., 0., 0.],</span></a>
<a class="sourceLine" id="cb200-10" data-line-number="10"><span class="co">#&gt;         [0., 0., 0.]])</span></a>
<a class="sourceLine" id="cb200-11" data-line-number="11"><span class="kw">print</span>(b<span class="op">$</span>grad)</a>
<a class="sourceLine" id="cb200-12" data-line-number="12"><span class="co">#&gt; tensor([0., 0.])</span></a></code></pre></div>
<div id="adjust-weights-and-biases-using-gradient-descent" class="section level4">
<h4><span class="header-section-number">9.6.3.1</span> Adjust weights and biases using gradient descent</h4>
<p>Weâ€™ll reduce the loss and improve our model using the gradient descent algorithm, which has the following steps:</p>
<ol style="list-style-type: decimal">
<li>Generate predictions</li>
<li>Calculate the loss</li>
<li>Compute gradients w.r.t the weights and biases</li>
<li>Adjust the weights by subtracting a small quantity proportional to the gradient</li>
<li>Reset the gradients to zero</li>
</ol>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb201-1" data-line-number="1"><span class="co"># Generate predictions</span></a>
<a class="sourceLine" id="cb201-2" data-line-number="2">preds =<span class="st"> </span><span class="kw">model</span>(inputs)</a>
<a class="sourceLine" id="cb201-3" data-line-number="3"><span class="kw">print</span>(preds)</a>
<a class="sourceLine" id="cb201-4" data-line-number="4"><span class="co">#&gt; tensor([[  -0.4516,  -90.4691],</span></a>
<a class="sourceLine" id="cb201-5" data-line-number="5"><span class="co">#&gt;         [ -24.6303, -132.3828],</span></a>
<a class="sourceLine" id="cb201-6" data-line-number="6"><span class="co">#&gt;         [ -31.2192, -176.1530],</span></a>
<a class="sourceLine" id="cb201-7" data-line-number="7"><span class="co">#&gt;         [  64.3523,  -39.5645],</span></a>
<a class="sourceLine" id="cb201-8" data-line-number="8"><span class="co">#&gt;         [ -73.9524, -161.9560]], grad_fn=&lt;AddBackward0&gt;)</span></a></code></pre></div>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb202-1" data-line-number="1"><span class="co"># Calculate the loss</span></a>
<a class="sourceLine" id="cb202-2" data-line-number="2">loss =<span class="st"> </span><span class="kw">mse</span>(preds, targets)</a>
<a class="sourceLine" id="cb202-3" data-line-number="3"><span class="kw">print</span>(loss)</a>
<a class="sourceLine" id="cb202-4" data-line-number="4"><span class="co">#&gt; tensor(33060.8053, grad_fn=&lt;DivBackward0&gt;)</span></a></code></pre></div>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb203-1" data-line-number="1"><span class="co"># Compute gradients</span></a>
<a class="sourceLine" id="cb203-2" data-line-number="2">loss<span class="op">$</span><span class="kw">backward</span>()</a>
<a class="sourceLine" id="cb203-3" data-line-number="3"></a>
<a class="sourceLine" id="cb203-4" data-line-number="4"><span class="kw">print</span>(w<span class="op">$</span>grad)</a>
<a class="sourceLine" id="cb203-5" data-line-number="5"><span class="co">#&gt; tensor([[ -6938.4351,  -9674.6757,  -5744.0206],</span></a>
<a class="sourceLine" id="cb203-6" data-line-number="6"><span class="co">#&gt;         [-17408.7861, -20595.9333, -12453.4702]])</span></a>
<a class="sourceLine" id="cb203-7" data-line-number="7"><span class="kw">print</span>(b<span class="op">$</span>grad)</a>
<a class="sourceLine" id="cb203-8" data-line-number="8"><span class="co">#&gt; tensor([ -89.3802, -212.1051])</span></a></code></pre></div>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb204-1" data-line-number="1"><span class="co"># Adjust weights and reset gradients</span></a>
<a class="sourceLine" id="cb204-2" data-line-number="2"><span class="kw">with</span>(torch<span class="op">$</span><span class="kw">no_grad</span>(), {</a>
<a class="sourceLine" id="cb204-3" data-line-number="3">  <span class="kw">print</span>(w); <span class="kw">print</span>(b)    <span class="co"># requires_grad attribute remains</span></a>
<a class="sourceLine" id="cb204-4" data-line-number="4">  w<span class="op">$</span>data &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(w<span class="op">$</span>data, torch<span class="op">$</span><span class="kw">mul</span>(w<span class="op">$</span>grad<span class="op">$</span>data, torch<span class="op">$</span><span class="kw">scalar_tensor</span>(<span class="fl">1e-5</span>)))</a>
<a class="sourceLine" id="cb204-5" data-line-number="5">  b<span class="op">$</span>data &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(b<span class="op">$</span>data, torch<span class="op">$</span><span class="kw">mul</span>(b<span class="op">$</span>grad<span class="op">$</span>data, torch<span class="op">$</span><span class="kw">scalar_tensor</span>(<span class="fl">1e-5</span>)))</a>
<a class="sourceLine" id="cb204-6" data-line-number="6"></a>
<a class="sourceLine" id="cb204-7" data-line-number="7">  <span class="kw">print</span>(w<span class="op">$</span>grad<span class="op">$</span>data<span class="op">$</span><span class="kw">zero_</span>())</a>
<a class="sourceLine" id="cb204-8" data-line-number="8">  <span class="kw">print</span>(b<span class="op">$</span>grad<span class="op">$</span>data<span class="op">$</span><span class="kw">zero_</span>())</a>
<a class="sourceLine" id="cb204-9" data-line-number="9">})</a>
<a class="sourceLine" id="cb204-10" data-line-number="10"><span class="co">#&gt; tensor([[ 1.5410, -0.2934, -2.1788],</span></a>
<a class="sourceLine" id="cb204-11" data-line-number="11"><span class="co">#&gt;         [ 0.5684, -1.0845, -1.3986]], requires_grad=True)</span></a>
<a class="sourceLine" id="cb204-12" data-line-number="12"><span class="co">#&gt; tensor([0.4033, 0.8380], requires_grad=True)</span></a>
<a class="sourceLine" id="cb204-13" data-line-number="13"><span class="co">#&gt; tensor([[0., 0., 0.],</span></a>
<a class="sourceLine" id="cb204-14" data-line-number="14"><span class="co">#&gt;         [0., 0., 0.]])</span></a>
<a class="sourceLine" id="cb204-15" data-line-number="15"><span class="co">#&gt; tensor([0., 0.])</span></a>
<a class="sourceLine" id="cb204-16" data-line-number="16"></a>
<a class="sourceLine" id="cb204-17" data-line-number="17"><span class="kw">print</span>(w)</a>
<a class="sourceLine" id="cb204-18" data-line-number="18"><span class="co">#&gt; tensor([[ 1.6104, -0.1967, -2.1213],</span></a>
<a class="sourceLine" id="cb204-19" data-line-number="19"><span class="co">#&gt;         [ 0.7425, -0.8786, -1.2741]], requires_grad=True)</span></a>
<a class="sourceLine" id="cb204-20" data-line-number="20"><span class="kw">print</span>(b)</a>
<a class="sourceLine" id="cb204-21" data-line-number="21"><span class="co">#&gt; tensor([0.4042, 0.8401], requires_grad=True)</span></a></code></pre></div>
<p>With the new weights and biases, the model should have a lower loss.</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb205-1" data-line-number="1"><span class="co"># Calculate loss</span></a>
<a class="sourceLine" id="cb205-2" data-line-number="2">preds =<span class="st"> </span><span class="kw">model</span>(inputs)</a>
<a class="sourceLine" id="cb205-3" data-line-number="3">loss =<span class="st"> </span><span class="kw">mse</span>(preds, targets)</a>
<a class="sourceLine" id="cb205-4" data-line-number="4"><span class="kw">print</span>(loss)</a>
<a class="sourceLine" id="cb205-5" data-line-number="5"><span class="co">#&gt; tensor(23432.4894, grad_fn=&lt;DivBackward0&gt;)</span></a></code></pre></div>
</div>
</div>
</div>
<div id="all-together-train-for-multiple-epochs" class="section level2">
<h2><span class="header-section-number">9.7</span> All together: train for multiple epochs</h2>
<p>To reduce the loss further, we repeat the process of adjusting the weights and biases using the gradients multiple times. Each iteration is called an <strong>epoch</strong>.</p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb206-1" data-line-number="1"><span class="co"># Running all together</span></a>
<a class="sourceLine" id="cb206-2" data-line-number="2"><span class="co"># Adjust weights and reset gradients</span></a>
<a class="sourceLine" id="cb206-3" data-line-number="3">num_epochs &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb206-4" data-line-number="4"></a>
<a class="sourceLine" id="cb206-5" data-line-number="5"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>num_epochs) {</a>
<a class="sourceLine" id="cb206-6" data-line-number="6">  preds =<span class="st"> </span><span class="kw">model</span>(inputs)</a>
<a class="sourceLine" id="cb206-7" data-line-number="7">  loss =<span class="st"> </span><span class="kw">mse</span>(preds, targets)</a>
<a class="sourceLine" id="cb206-8" data-line-number="8">  loss<span class="op">$</span><span class="kw">backward</span>()</a>
<a class="sourceLine" id="cb206-9" data-line-number="9">  <span class="kw">with</span>(torch<span class="op">$</span><span class="kw">no_grad</span>(), {</a>
<a class="sourceLine" id="cb206-10" data-line-number="10">    w<span class="op">$</span>data &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(w<span class="op">$</span>data, torch<span class="op">$</span><span class="kw">mul</span>(w<span class="op">$</span>grad, torch<span class="op">$</span><span class="kw">scalar_tensor</span>(<span class="fl">1e-5</span>)))</a>
<a class="sourceLine" id="cb206-11" data-line-number="11">    b<span class="op">$</span>data &lt;-<span class="st"> </span>torch<span class="op">$</span><span class="kw">sub</span>(b<span class="op">$</span>data, torch<span class="op">$</span><span class="kw">mul</span>(b<span class="op">$</span>grad, torch<span class="op">$</span><span class="kw">scalar_tensor</span>(<span class="fl">1e-5</span>)))</a>
<a class="sourceLine" id="cb206-12" data-line-number="12">    </a>
<a class="sourceLine" id="cb206-13" data-line-number="13">    w<span class="op">$</span>grad<span class="op">$</span><span class="kw">zero_</span>()</a>
<a class="sourceLine" id="cb206-14" data-line-number="14">    b<span class="op">$</span>grad<span class="op">$</span><span class="kw">zero_</span>()</a>
<a class="sourceLine" id="cb206-15" data-line-number="15">  })</a>
<a class="sourceLine" id="cb206-16" data-line-number="16">}</a>
<a class="sourceLine" id="cb206-17" data-line-number="17"></a>
<a class="sourceLine" id="cb206-18" data-line-number="18"><span class="co"># Calculate loss</span></a>
<a class="sourceLine" id="cb206-19" data-line-number="19">preds =<span class="st"> </span><span class="kw">model</span>(inputs)</a>
<a class="sourceLine" id="cb206-20" data-line-number="20">loss =<span class="st"> </span><span class="kw">mse</span>(preds, targets)</a>
<a class="sourceLine" id="cb206-21" data-line-number="21"><span class="kw">print</span>(loss)</a>
<a class="sourceLine" id="cb206-22" data-line-number="22"><span class="co">#&gt; tensor(1258.0216, grad_fn=&lt;DivBackward0&gt;)</span></a>
<a class="sourceLine" id="cb206-23" data-line-number="23"></a>
<a class="sourceLine" id="cb206-24" data-line-number="24"><span class="co"># predictions</span></a>
<a class="sourceLine" id="cb206-25" data-line-number="25">preds</a>
<a class="sourceLine" id="cb206-26" data-line-number="26"><span class="co">#&gt; tensor([[ 69.2462,  80.2082],</span></a>
<a class="sourceLine" id="cb206-27" data-line-number="27"><span class="co">#&gt;         [ 73.7183,  97.2052],</span></a>
<a class="sourceLine" id="cb206-28" data-line-number="28"><span class="co">#&gt;         [118.5780, 124.9272],</span></a>
<a class="sourceLine" id="cb206-29" data-line-number="29"><span class="co">#&gt;         [ 89.2282,  92.7052],</span></a>
<a class="sourceLine" id="cb206-30" data-line-number="30"><span class="co">#&gt;         [ 47.4648,  80.7782]], grad_fn=&lt;AddBackward0&gt;)</span></a>
<a class="sourceLine" id="cb206-31" data-line-number="31"></a>
<a class="sourceLine" id="cb206-32" data-line-number="32"><span class="co"># Targets</span></a>
<a class="sourceLine" id="cb206-33" data-line-number="33">targets</a>
<a class="sourceLine" id="cb206-34" data-line-number="34"><span class="co">#&gt; tensor([[ 56.,  70.],</span></a>
<a class="sourceLine" id="cb206-35" data-line-number="35"><span class="co">#&gt;         [ 81., 101.],</span></a>
<a class="sourceLine" id="cb206-36" data-line-number="36"><span class="co">#&gt;         [119., 133.],</span></a>
<a class="sourceLine" id="cb206-37" data-line-number="37"><span class="co">#&gt;         [ 22.,  37.],</span></a>
<a class="sourceLine" id="cb206-38" data-line-number="38"><span class="co">#&gt;         [103., 119.]])</span></a></code></pre></div>

</div>
</div>



</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-two-layer-neural-network.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["rtorch-book.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
