[
["index.html", "A Minimal rTorch Tutorial Prerequisites Installation Python Anaconda", " A Minimal rTorch Tutorial Alfonso R. Reyes 2019-09-24 Prerequisites You need two things to get rTorch working: Install Python Anaconda. Preferrably, for 64-bits, and above Python 3.6+. Install R, Rtools and RStudio. Install rTorch from CRAN or GitHub. Note. It is not mandatory to have a previously created Python environment with Anaconda, where PyTorch and TorchVision have already been installed. This step is optional. You could also get it installed directly from the R console, in very similar fashion as in R-TensorFlow using the function install_pytorch. This book is available online via GitHub Pages, or you can also build it from source from its repository. Installation rTorch is available via CRAN or GitHub. The rTorch package can be installed from CRAN or Github. From CRAN: install.packages(&quot;rTorch&quot;) From GitHub, install rTorch with: devtools::install_github(&quot;f0nzie/rTorch&quot;) Python Anaconda Before start running rTorch, install a Python Anaconda environment first. Example Create a conda environment from the terminal with conda create -n myenv python=3.7 Activate the new environment with conda activate myenv Install the PyTorch related packages with: conda install python=3.6.6 pytorch-cpu torchvision-cpu matplotlib pandas -c pytorch The last part -c pytorch specifies the conda channel to download the PyTorch packages. Your installation may not work if you don’t indicate the channel. Now, you can load rTorch in R or RStudio. Automatic installation I use the idea from automatic installation in r-tensorflow, to create the function rTorch::install_pytorch(). This function will allow you to install a conda environment complete with all PyTorch requirements. Note. matplotlib and pandas are not really necessary for rTorch to work, but I was asked if matplotlib or pandas would work with PyTorch. So, I decided to install them for testing and experimentation. They both work. "],
["intro.html", "Chapter 1 Introduction 1.1 Motivation 1.2 How do we start using rTorch 1.3 What can you do with rTorch 1.4 Callable PyTorch modules", " Chapter 1 Introduction 1.1 Motivation Why do we want a package of something that is already working well, such as PyTorch? There are several reasons, but the main one is to bring another machine learning framework to R. Probably it just me but I feel PyTorch very comfortable to work with. Feels pretty much like everything else in Python. I have tried other frameworks in R. The closest that matches a natural language like PyTorch, is MXnet. Unfortunately, it is the hardest to install and maintain after updates. Yes. I could have worked directly with PyTorch in a native Python environment, such as Jupyter or PyCharm but it very hard to quit RMarkdown once you get used to it. It is the real thing in regards to literate programming. It does not only contributes to improving the quality of the code but establishes a workflow for a better understanding of the subject by your intended readers (Knuth 1983), in what is been called the literate programming paradigm (Cordes and Brown 1991). This has the additional benefit of giving the ability to write combination of Python and R code together in the same document. There will times when it is better to create a class in Python; and other times where R will be more convenient to handle a data structure. 1.2 How do we start using rTorch Start using rTorch is very simple. After installing the minimum system requirements, you just call it with: library(rTorch) There are several way of testing that rTorch is up and running. Let’s see some of them: 1.2.1 Getting the PyTorch version rTorch::torch_version() #&gt; [1] &quot;1.1&quot; 1.2.2 PyTorch configuration This will show the PyTorch version and the current version of Python installed, as well as the paths where they reside. rTorch::torch_config() #&gt; PyTorch v1.1.0 (~/anaconda3/envs/r-torch/lib/python3.6/site-packages/torch) #&gt; Python v3.6 (~/anaconda3/envs/r-torch/bin/python) 1.3 What can you do with rTorch Practically, you can do everything you could with PyTorch within the R ecosystem. Additionally to the rTorch module, from where you can extract methods, functions and classes, there are available two more modules: torchvision and np, which is short for numpy. 1.4 Callable PyTorch modules 1.4.1 The torchvision module This is an example of using the torchvision module. With torchvision we could download any of the datasets made available by PyTorch. In this example, we will be downloading the training dataset of the MNIST handwritten digits. There are 60,000 images in the training set and 10,000 images in the test set. transforms &lt;- torchvision$transforms # this is the folder where the datasets will be downloaded local_folder &lt;- &#39;./datasets/mnist_digits&#39; train_dataset = torchvision$datasets$MNIST(root = local_folder, train = TRUE, transform = transforms$ToTensor(), download = TRUE) train_dataset #&gt; Dataset MNIST #&gt; Number of datapoints: 60000 #&gt; Root location: ./datasets/mnist_digits #&gt; Split: Train You can do similarly for the test dataset if you set the flag train = FALSE. The test dataset has only 10,000 images. test_dataset = torchvision$datasets$MNIST(root = local_folder, train = FALSE, transform = transforms$ToTensor()) test_dataset #&gt; Dataset MNIST #&gt; Number of datapoints: 10000 #&gt; Root location: ./datasets/mnist_digits #&gt; Split: Test 1.4.2 np: the numpy module numpy is automaticaly installed when PyTorch is. There is some interdependence between both. Anytime that we need to do some transformation that is not available in PyTorch, we will use numpy. There are several operations that we could perform with numpy: Create an array # do some array manipulations with NumPy a &lt;- np$array(c(1:4)) a #&gt; [1] 1 2 3 4 np$reshape(np$arange(0, 9), c(3L, 3L)) #&gt; [,1] [,2] [,3] #&gt; [1,] 0 1 2 #&gt; [2,] 3 4 5 #&gt; [3,] 6 7 8 np$array(list( list(73, 67, 43), list(87, 134, 58), list(102, 43, 37), list(73, 67, 43), list(91, 88, 64), list(102, 43, 37), list(69, 96, 70), list(91, 88, 64), list(102, 43, 37), list(69, 96, 70) ), dtype=&#39;float32&#39;) #&gt; [,1] [,2] [,3] #&gt; [1,] 73 67 43 #&gt; [2,] 87 134 58 #&gt; [3,] 102 43 37 #&gt; [4,] 73 67 43 #&gt; [5,] 91 88 64 #&gt; [6,] 102 43 37 #&gt; [7,] 69 96 70 #&gt; [8,] 91 88 64 #&gt; [9,] 102 43 37 #&gt; [10,] 69 96 70 Reshape an array For the same test dataset that we loaded above, we will show the image of the handwritten digit and its label or class. Before plotting the image, we need to: Extract the image and label from the dataset Convert the tensor to a numpy array Reshape the tensor as a 2D array Plot the digit and its label rotate &lt;- function(x) t(apply(x, 2, rev)) # function to rotate the matrix # label for the image label &lt;- test_dataset[0][[2]] label #&gt; [1] 7 # convert tensor to numpy array .show_img &lt;- test_dataset[0][[1]]$numpy() dim(.show_img) #&gt; [1] 1 28 28 # reshape 3D array to 2D show_img &lt;- np$reshape(.show_img, c(28L, 28L)) dim(show_img) #&gt; [1] 28 28 # show in grays and rotate image(rotate(show_img), col = gray.colors(64)) title(label) Generate a random array # set the seed np$random$seed(123L) # generate a random array x = np$random$rand(100L) # calculate the y array y = np$sin(x) * np$power(x, 3L) + 3L * x + np$random$rand(100L) * 0.8 plot(x, y) Convert a numpy array to a PyTorch tensor This is a very common operation that I have seen in examples using PyTorch. Creating the array in numpy. and then convert it to a tensor. # input array x = np$array(rbind( c(0,0,1), c(0,1,1), c(1,0,1), c(1,1,1))) # the numpy array x #&gt; [,1] [,2] [,3] #&gt; [1,] 0 0 1 #&gt; [2,] 0 1 1 #&gt; [3,] 1 0 1 #&gt; [4,] 1 1 1 # convert the numpy array to float X &lt;- np$float32(x) # convert the numpy array to a float tensor X &lt;- torch$FloatTensor(X) X #&gt; tensor([[0., 0., 1.], #&gt; [0., 1., 1.], #&gt; [1., 0., 1.], #&gt; [1., 1., 1.]]) 1.4.3 Python built-in functions To access the Python built-in functions we make use of the package reticulate and the function import_builtins(). py_bi &lt;- import_builtins() Length of a dataset py_bi$len(train_dataset) #&gt; [1] 60000 py_bi$len(test_dataset) #&gt; [1] 10000 Iterators # iterate through training dataset enum_train_dataset &lt;- py_bi$enumerate(train_dataset) cat(sprintf(&quot;%8s %8s \\n&quot;, &quot;index&quot;, &quot;label&quot;)) #&gt; index label for (i in 1:py_bi$len(train_dataset)) { obj &lt;- reticulate::iter_next(enum_train_dataset) idx &lt;- obj[[1]] # index number cat(sprintf(&quot;%8d %5d \\n&quot;, idx, obj[[2]][[2]])) if (i &gt;= 100) break # print only 100 labels } #&gt; 0 5 #&gt; 1 0 #&gt; 2 4 #&gt; 3 1 #&gt; 4 9 #&gt; 5 2 #&gt; 6 1 #&gt; 7 3 #&gt; 8 1 #&gt; 9 4 #&gt; 10 3 #&gt; 11 5 #&gt; 12 3 #&gt; 13 6 #&gt; 14 1 #&gt; 15 7 #&gt; 16 2 #&gt; 17 8 #&gt; 18 6 #&gt; 19 9 #&gt; 20 4 #&gt; 21 0 #&gt; 22 9 #&gt; 23 1 #&gt; 24 1 #&gt; 25 2 #&gt; 26 4 #&gt; 27 3 #&gt; 28 2 #&gt; 29 7 #&gt; 30 3 #&gt; 31 8 #&gt; 32 6 #&gt; 33 9 #&gt; 34 0 #&gt; 35 5 #&gt; 36 6 #&gt; 37 0 #&gt; 38 7 #&gt; 39 6 #&gt; 40 1 #&gt; 41 8 #&gt; 42 7 #&gt; 43 9 #&gt; 44 3 #&gt; 45 9 #&gt; 46 8 #&gt; 47 5 #&gt; 48 9 #&gt; 49 3 #&gt; 50 3 #&gt; 51 0 #&gt; 52 7 #&gt; 53 4 #&gt; 54 9 #&gt; 55 8 #&gt; 56 0 #&gt; 57 9 #&gt; 58 4 #&gt; 59 1 #&gt; 60 4 #&gt; 61 4 #&gt; 62 6 #&gt; 63 0 #&gt; 64 4 #&gt; 65 5 #&gt; 66 6 #&gt; 67 1 #&gt; 68 0 #&gt; 69 0 #&gt; 70 1 #&gt; 71 7 #&gt; 72 1 #&gt; 73 6 #&gt; 74 3 #&gt; 75 0 #&gt; 76 2 #&gt; 77 1 #&gt; 78 1 #&gt; 79 7 #&gt; 80 9 #&gt; 81 0 #&gt; 82 2 #&gt; 83 6 #&gt; 84 7 #&gt; 85 8 #&gt; 86 3 #&gt; 87 9 #&gt; 88 0 #&gt; 89 4 #&gt; 90 6 #&gt; 91 7 #&gt; 92 4 #&gt; 93 6 #&gt; 94 8 #&gt; 95 0 #&gt; 96 7 #&gt; 97 8 #&gt; 98 3 #&gt; 99 1 Types and instances # get the class of the object py_bi$type(train_dataset) #&gt; &lt;class &#39;torchvision.datasets.mnist.MNIST&#39;&gt; # is train_dataset a torchvision dataset class py_bi$isinstance(train_dataset, torchvision$datasets$mnist$MNIST) #&gt; [1] TRUE References "],
["rtorch-vs-pytorch-whats-different.html", "Chapter 2 rTorch vs PyTorch: What’s different 2.1 Calling objects from PyTorch 2.2 Call a module from PyTorch 2.3 Show the attributes (methods) of a class or PyTorch object 2.4 Enumeration 2.5 How to iterate 2.6 Zero gradient 2.7 Transform a tensor 2.8 Build a model class 2.9 Convert a tensor to numpy object 2.10 Convert a numpy object to an R object", " Chapter 2 rTorch vs PyTorch: What’s different This chapter will explain the main differences between PyTorch and rTorch. Most of the things work directly in PyTorch but we need to be aware of some minor differences when working with rTorch. Here is a review of existing methods. library(rTorch) 2.1 Calling objects from PyTorch We use the dollar sign or $ to call a class, function or method from the rTorch modules. In this case, from torch module: torch$tensor #&gt; &lt;built-in method tensor of type&gt; 2.2 Call a module from PyTorch # these are the equivalents of import module nn &lt;- torch$nn transforms &lt;- torchvision$transforms dsets &lt;- torchvision$datasets Then we can proceed to extract classes, methods and functions from the nn, transforms, and dsets objects. 2.3 Show the attributes (methods) of a class or PyTorch object Sometimes we are interested in knowing the internal components of a class. In that case, we use the reticulate function py_list_attributes(). local_folder &lt;- &#39;./datasets/mnist_digits&#39; train_dataset = torchvision$datasets$MNIST(root = local_folder, train = TRUE, transform = transforms$ToTensor(), download = TRUE) train_dataset #&gt; Dataset MNIST #&gt; Number of datapoints: 60000 #&gt; Root location: ./datasets/mnist_digits #&gt; Split: Train Show the attributes of train_dataset: reticulate::py_list_attributes(train_dataset) #&gt; [1] &quot;__add__&quot; &quot;__class__&quot; #&gt; [3] &quot;__delattr__&quot; &quot;__dict__&quot; #&gt; [5] &quot;__dir__&quot; &quot;__doc__&quot; #&gt; [7] &quot;__eq__&quot; &quot;__format__&quot; #&gt; [9] &quot;__ge__&quot; &quot;__getattribute__&quot; #&gt; [11] &quot;__getitem__&quot; &quot;__gt__&quot; #&gt; [13] &quot;__hash__&quot; &quot;__init__&quot; #&gt; [15] &quot;__init_subclass__&quot; &quot;__le__&quot; #&gt; [17] &quot;__len__&quot; &quot;__lt__&quot; #&gt; [19] &quot;__module__&quot; &quot;__ne__&quot; #&gt; [21] &quot;__new__&quot; &quot;__reduce__&quot; #&gt; [23] &quot;__reduce_ex__&quot; &quot;__repr__&quot; #&gt; [25] &quot;__setattr__&quot; &quot;__sizeof__&quot; #&gt; [27] &quot;__str__&quot; &quot;__subclasshook__&quot; #&gt; [29] &quot;__weakref__&quot; &quot;_check_exists&quot; #&gt; [31] &quot;_format_transform_repr&quot; &quot;_repr_indent&quot; #&gt; [33] &quot;class_to_idx&quot; &quot;classes&quot; #&gt; [35] &quot;data&quot; &quot;download&quot; #&gt; [37] &quot;extra_repr&quot; &quot;extract_gzip&quot; #&gt; [39] &quot;processed_folder&quot; &quot;raw_folder&quot; #&gt; [41] &quot;root&quot; &quot;target_transform&quot; #&gt; [43] &quot;targets&quot; &quot;test_data&quot; #&gt; [45] &quot;test_file&quot; &quot;test_labels&quot; #&gt; [47] &quot;train&quot; &quot;train_data&quot; #&gt; [49] &quot;train_labels&quot; &quot;training_file&quot; #&gt; [51] &quot;transform&quot; &quot;transforms&quot; #&gt; [53] &quot;urls&quot; Knowing the internal methods of a class could be useful when we want to refer to a specific property of such class. For example, from the list above, we know that the object train_dataset has an attribute __len__. We can call it like this: train_dataset$`__len__`() #&gt; [1] 60000 2.4 Enumeration x_train = array(c(3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167, 7.042, 10.791, 5.313, 7.997, 3.1), dim = c(15,1)) x_train &lt;- r_to_py(x_train) x_train &lt;- torch$from_numpy(x_train) # convert to tensor x_train &lt;- x_train$type(torch$FloatTensor) # make it a a FloatTensor x_train #&gt; tensor([[ 3.3000], #&gt; [ 4.4000], #&gt; [ 5.5000], #&gt; [ 6.7100], #&gt; [ 6.9300], #&gt; [ 4.1680], #&gt; [ 9.7790], #&gt; [ 6.1820], #&gt; [ 7.5900], #&gt; [ 2.1670], #&gt; [ 7.0420], #&gt; [10.7910], #&gt; [ 5.3130], #&gt; [ 7.9970], #&gt; [ 3.1000]]) x_train$nelement() # number of elements in the tensor #&gt; [1] 15 2.5 How to iterate 2.5.1 Using enumerate and iterate py = import_builtins() enum_x_train = py$enumerate(x_train) enum_x_train #&gt; &lt;enumerate&gt; py$len(x_train) #&gt; [1] 15 xit = iterate(enum_x_train, simplify = TRUE) xit #&gt; [[1]] #&gt; [[1]][[1]] #&gt; [1] 0 #&gt; #&gt; [[1]][[2]] #&gt; tensor([3.3000]) #&gt; #&gt; #&gt; [[2]] #&gt; [[2]][[1]] #&gt; [1] 1 #&gt; #&gt; [[2]][[2]] #&gt; tensor([4.4000]) #&gt; #&gt; #&gt; [[3]] #&gt; [[3]][[1]] #&gt; [1] 2 #&gt; #&gt; [[3]][[2]] #&gt; tensor([5.5000]) #&gt; #&gt; #&gt; [[4]] #&gt; [[4]][[1]] #&gt; [1] 3 #&gt; #&gt; [[4]][[2]] #&gt; tensor([6.7100]) #&gt; #&gt; #&gt; [[5]] #&gt; [[5]][[1]] #&gt; [1] 4 #&gt; #&gt; [[5]][[2]] #&gt; tensor([6.9300]) #&gt; #&gt; #&gt; [[6]] #&gt; [[6]][[1]] #&gt; [1] 5 #&gt; #&gt; [[6]][[2]] #&gt; tensor([4.1680]) #&gt; #&gt; #&gt; [[7]] #&gt; [[7]][[1]] #&gt; [1] 6 #&gt; #&gt; [[7]][[2]] #&gt; tensor([9.7790]) #&gt; #&gt; #&gt; [[8]] #&gt; [[8]][[1]] #&gt; [1] 7 #&gt; #&gt; [[8]][[2]] #&gt; tensor([6.1820]) #&gt; #&gt; #&gt; [[9]] #&gt; [[9]][[1]] #&gt; [1] 8 #&gt; #&gt; [[9]][[2]] #&gt; tensor([7.5900]) #&gt; #&gt; #&gt; [[10]] #&gt; [[10]][[1]] #&gt; [1] 9 #&gt; #&gt; [[10]][[2]] #&gt; tensor([2.1670]) #&gt; #&gt; #&gt; [[11]] #&gt; [[11]][[1]] #&gt; [1] 10 #&gt; #&gt; [[11]][[2]] #&gt; tensor([7.0420]) #&gt; #&gt; #&gt; [[12]] #&gt; [[12]][[1]] #&gt; [1] 11 #&gt; #&gt; [[12]][[2]] #&gt; tensor([10.7910]) #&gt; #&gt; #&gt; [[13]] #&gt; [[13]][[1]] #&gt; [1] 12 #&gt; #&gt; [[13]][[2]] #&gt; tensor([5.3130]) #&gt; #&gt; #&gt; [[14]] #&gt; [[14]][[1]] #&gt; [1] 13 #&gt; #&gt; [[14]][[2]] #&gt; tensor([7.9970]) #&gt; #&gt; #&gt; [[15]] #&gt; [[15]][[1]] #&gt; [1] 14 #&gt; #&gt; [[15]][[2]] #&gt; tensor([3.1000]) 2.5.2 Using a for-loop to iterate # reset the iterator enum_x_train = py$enumerate(x_train) for (i in 1:py$len(x_train)) { obj &lt;- iter_next(enum_x_train) # next item cat(obj[[1]], &quot;\\t&quot;) # 1st part or index print(obj[[2]]) # 2nd part or tensor } #&gt; 0 tensor([3.3000]) #&gt; 1 tensor([4.4000]) #&gt; 2 tensor([5.5000]) #&gt; 3 tensor([6.7100]) #&gt; 4 tensor([6.9300]) #&gt; 5 tensor([4.1680]) #&gt; 6 tensor([9.7790]) #&gt; 7 tensor([6.1820]) #&gt; 8 tensor([7.5900]) #&gt; 9 tensor([2.1670]) #&gt; 10 tensor([7.0420]) #&gt; 11 tensor([10.7910]) #&gt; 12 tensor([5.3130]) #&gt; 13 tensor([7.9970]) #&gt; 14 tensor([3.1000]) We will find very frequently this kind of iterators when we read a dataset using torchvision. There are different ways to iterate through these objects. 2.6 Zero gradient The zero gradient was one of the most difficult to implement in R if we don’t pay attention to the content of the objects carrying the weights and biases. This happens when the algorithm written in PyTorch is not immediately translatable to rTorch. This can be appreciated in this example. We are using the same seed in the PyTorch and rTorch versions, so, we could compare the results. 2.6.1 Version in Python import numpy as np import torch torch.manual_seed(0) # reproducible # Input (temp, rainfall, humidity) #&gt; &lt;torch._C.Generator object at 0x7f3e18c02e10&gt; inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70]], dtype=&#39;float32&#39;) # Targets (apples, oranges) targets = np.array([[56, 70], [81, 101], [119, 133], [22, 37], [103, 119]], dtype=&#39;float32&#39;) # Convert inputs and targets to tensors inputs = torch.from_numpy(inputs) targets = torch.from_numpy(targets) # random weights and biases w = torch.randn(2, 3, requires_grad=True) b = torch.randn(2, requires_grad=True) # function for the model def model(x): wt = w.t() mm = x @ w.t() return x @ w.t() + b # @ represents matrix multiplication in PyTorch # MSE loss function def mse(t1, t2): diff = t1 - t2 return torch.sum(diff * diff) / diff.numel() # Running all together # Train for 100 epochs for i in range(100): preds = model(inputs) loss = mse(preds, targets) loss.backward() with torch.no_grad(): w -= w.grad * 0.00001 b -= b.grad * 0.00001 w_gz = w.grad.zero_() b_gz = b.grad.zero_() # Calculate loss preds = model(inputs) loss = mse(preds, targets) print(&quot;Loss: &quot;, loss) # predictions #&gt; Loss: tensor(1270.1234, grad_fn=&lt;DivBackward0&gt;) print(&quot;\\nPredictions:&quot;) #&gt; #&gt; Predictions: preds # Targets #&gt; tensor([[ 69.3122, 80.2639], #&gt; [ 73.7528, 97.2381], #&gt; [118.3933, 124.7628], #&gt; [ 89.6111, 93.0286], #&gt; [ 47.3014, 80.6467]], grad_fn=&lt;AddBackward0&gt;) print(&quot;\\nTargets:&quot;) #&gt; #&gt; Targets: targets #&gt; tensor([[ 56., 70.], #&gt; [ 81., 101.], #&gt; [119., 133.], #&gt; [ 22., 37.], #&gt; [103., 119.]]) 2.6.2 Version in R library(rTorch) torch$manual_seed(0) #&gt; &lt;torch._C.Generator&gt; device = torch$device(&#39;cpu&#39;) # Input (temp, rainfall, humidity) inputs = np$array(list(list(73, 67, 43), list(91, 88, 64), list(87, 134, 58), list(102, 43, 37), list(69, 96, 70)), dtype=&#39;float32&#39;) # Targets (apples, oranges) targets = np$array(list(list(56, 70), list(81, 101), list(119, 133), list(22, 37), list(103, 119)), dtype=&#39;float32&#39;) # Convert inputs and targets to tensors inputs = torch$from_numpy(inputs) targets = torch$from_numpy(targets) # random numbers for weights and biases. Then convert to double() torch$set_default_dtype(torch$float64) w = torch$randn(2L, 3L, requires_grad=TRUE) #$double() b = torch$randn(2L, requires_grad=TRUE) #$double() model &lt;- function(x) { wt &lt;- w$t() return(torch$add(torch$mm(x, wt), b)) } # MSE loss mse = function(t1, t2) { diff &lt;- torch$sub(t1, t2) mul &lt;- torch$sum(torch$mul(diff, diff)) return(torch$div(mul, diff$numel())) } # Running all together # Adjust weights and reset gradients for (i in 1:100) { preds = model(inputs) loss = mse(preds, targets) loss$backward() with(torch$no_grad(), { w$data &lt;- torch$sub(w$data, torch$mul(w$grad, torch$scalar_tensor(1e-5))) b$data &lt;- torch$sub(b$data, torch$mul(b$grad, torch$scalar_tensor(1e-5))) w$grad$zero_() b$grad$zero_() }) } # Calculate loss preds = model(inputs) loss = mse(preds, targets) cat(&quot;Loss: &quot;); print(loss) #&gt; Loss: #&gt; tensor(1270.1237, grad_fn=&lt;DivBackward0&gt;) # predictions cat(&quot;\\nPredictions:\\n&quot;) #&gt; #&gt; Predictions: preds #&gt; tensor([[ 69.3122, 80.2639], #&gt; [ 73.7528, 97.2381], #&gt; [118.3933, 124.7628], #&gt; [ 89.6111, 93.0286], #&gt; [ 47.3013, 80.6467]], grad_fn=&lt;AddBackward0&gt;) # Targets cat(&quot;\\nTargets:\\n&quot;) #&gt; #&gt; Targets: targets #&gt; tensor([[ 56., 70.], #&gt; [ 81., 101.], #&gt; [119., 133.], #&gt; [ 22., 37.], #&gt; [103., 119.]]) Notice that while in Python, the tensor operation, gradient of the weights times the Learning Rate, is: \\[w = -w + \\nabla w \\; \\alpha\\] is a very straight forwward and clean code: w -= w.grad * 1e-5 In R shows a little bit more convoluted: w$data &lt;- torch$sub(w$data, torch$mul(w$grad, torch$scalar_tensor(1e-5))) Which is simpliefied when we use the generic methods from rTorch as: w$data &lt;- w$data - w$grad * 1e-5 These two expression are equivalent, with the first being the long version natural way of doing it in PyTorch. The second is using the generics in R for subtraction, multiplication and scalar conversion. param$data &lt;- torch$sub(param$data, torch$mul(param$grad$float(), torch$scalar_tensor(learning_rate))) } param$data &lt;- param$data - param$grad * learning_rate 2.7 Transform a tensor Explain how transform a tensor back and forth to numpy. Why is this important? 2.8 Build a model class PyTorch classes cannot not directly be instantiated from R. We need an intermediate step to create a class. For this, we use reticulate functions that will read the class implementation in Python code. 2.8.1 Example 1 py_run_string(&quot;import torch&quot;) main = py_run_string( &quot; import torch.nn as nn class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.layer = torch.nn.Linear(1, 1) def forward(self, x): x = self.layer(x) return x &quot;) # build a Linear Rgression model net &lt;- main$Net() 2.8.2 Example 2: Logistic Regression main &lt;- py_run_string( &quot; import torch.nn as nn class LogisticRegressionModel(nn.Module): def __init__(self, input_dim, output_dim): super(LogisticRegressionModel, self).__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): out = self.linear(x) return out &quot;) # build a Logistic Rgression model LogisticRegressionModel &lt;- main$LogisticRegressionModel 2.9 Convert a tensor to numpy object This is a frequent operation. I have found that this is necessary when a numpy function is not implemented in PyTorch We need to convert a tensor to R Perform a Boolean operation that is not directly available in PyTorch. 2.10 Convert a numpy object to an R object This is mainly required for these reasons: Create a data structure in R Plot using r-base or ggplot2 Perform an analysis on parts of a tensor Use R statistical functions that are not available in PyTorch. "],
["tensors.html", "Chapter 3 Tensors 3.1 Tensor data types 3.2 Arithmetic of tensors 3.3 NumPy and PyTorch 3.4 Create tensors 3.5 Tensor resizing 3.6 Reshape tensors 3.7 Special tensors 3.8 Tensor fill 3.9 Access to tensor elements 3.10 Other tensor operations 3.11 Logical operations 3.12 Distributions", " Chapter 3 Tensors We describe the most important PyTorch methods in this chapter. library(rTorch) 3.1 Tensor data types # Default data type torch$tensor(list(1.2, 3))$dtype # default for floating point is torch.float32 #&gt; torch.float32 # change default data type to float64 torch$set_default_dtype(torch$float64) torch$tensor(list(1.2, 3))$dtype # a new floating point tensor #&gt; torch.float64 There are five major type of Tensors in PyTorch library(rTorch) byte &lt;- torch$ByteTensor(3L, 3L) float &lt;- torch$FloatTensor(3L, 3L) double &lt;- torch$DoubleTensor(3L, 3L) long &lt;- torch$LongTensor(3L, 3L) boolean &lt;- torch$BoolTensor(5L, 5L) message(&quot;byte tensor&quot;) #&gt; byte tensor byte #&gt; tensor([[0, 0, 0], #&gt; [0, 0, 0], #&gt; [0, 0, 0]], dtype=torch.uint8) message(&quot;float tensor&quot;) #&gt; float tensor float #&gt; tensor([[0., 0., 0.], #&gt; [0., 0., 0.], #&gt; [0., 0., 0.]], dtype=torch.float32) message(&quot;double&quot;) #&gt; double double #&gt; tensor([[0., 0., 0.], #&gt; [0., 0., 0.], #&gt; [0., 0., 0.]]) message(&quot;long&quot;) #&gt; long long #&gt; tensor([[ 0, 0, 0], #&gt; [93923608032688, 0, 93923606400272], #&gt; [ 0, 0, 0]]) message(&quot;boolean&quot;) #&gt; boolean boolean #&gt; tensor([[ True, True, True, True, True], #&gt; [ True, False, False, True, True], #&gt; [ True, True, True, True, False], #&gt; [False, True, False, False, False], #&gt; [False, False, False, False, True]], dtype=torch.bool) A 4D tensor like in MNIST hand-written digits recognition dataset: mnist_4d &lt;- torch$FloatTensor(60000L, 3L, 28L, 28L) message(&quot;size&quot;) #&gt; size mnist_4d$size() #&gt; torch.Size([60000, 3, 28, 28]) message(&quot;length&quot;) #&gt; length length(mnist_4d) #&gt; [1] 141120000 message(&quot;shape, like in numpy&quot;) #&gt; shape, like in numpy mnist_4d$shape #&gt; torch.Size([60000, 3, 28, 28]) message(&quot;number of elements&quot;) #&gt; number of elements mnist_4d$numel() #&gt; [1] 141120000 A 3D tensor: ft3d &lt;- torch$FloatTensor(4L, 3L, 2L) ft3d #&gt; tensor([[[-3.3741e-10, 4.5654e-41], #&gt; [-3.3741e-10, 4.5654e-41], #&gt; [ 0.0000e+00, 0.0000e+00]], #&gt; #&gt; [[ 0.0000e+00, 0.0000e+00], #&gt; [ 0.0000e+00, 0.0000e+00], #&gt; [ 0.0000e+00, nan]], #&gt; #&gt; [[ 1.1444e-28, 1.2583e+00], #&gt; [ 9.2250e+36, 1.3410e+00], #&gt; [ 1.1444e-28, 1.2583e+00]], #&gt; #&gt; [[ 9.2250e+36, 1.3410e+00], #&gt; [ 1.4660e+13, 1.5417e+00], #&gt; [ 1.7109e-30, 1.6350e+00]]], dtype=torch.float32) 3.2 Arithmetic of tensors 3.2.1 Add tensors # add a scalar to a tensor # 3x5 matrix uniformly distributed between 0 and 1 mat0 &lt;- torch$FloatTensor(3L, 5L)$uniform_(0L, 1L) mat0 + 0.1 #&gt; tensor([[0.8394, 0.7823, 0.7397, 0.2066, 0.1630], #&gt; [0.4578, 0.9101, 0.1512, 0.2157, 0.1530], #&gt; [0.2091, 0.1006, 0.8996, 0.8848, 0.6962]], dtype=torch.float32) The expression tensor.index(m) is equivalent to tensor[m]. Add an element of tensor to a tensor: # fill a 3x5 matrix with 0.1 mat1 &lt;- torch$FloatTensor(3L, 5L)$uniform_(0.1, 0.1) # a vector with all ones mat2 &lt;- torch$FloatTensor(5L)$uniform_(1, 1) mat1[1, 1] + mat2 #&gt; tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000], dtype=torch.float32) # add two tensors mat1 + mat0 #&gt; tensor([[0.8394, 0.7823, 0.7397, 0.2066, 0.1630], #&gt; [0.4578, 0.9101, 0.1512, 0.2157, 0.1530], #&gt; [0.2091, 0.1006, 0.8996, 0.8848, 0.6962]], dtype=torch.float32) Add two tensors using the function add(): # PyTorch add two tensors x = torch$rand(5L, 4L) y = torch$rand(5L, 4L) print(x$add(y)) #&gt; tensor([[1.4858, 1.7484, 1.3578, 1.4873], #&gt; [0.9906, 1.0733, 0.2095, 1.4747], #&gt; [0.2601, 0.8075, 1.8880, 1.6219], #&gt; [1.3639, 1.4203, 1.4110, 0.5759], #&gt; [0.3801, 1.0272, 1.0236, 1.2996]]) Add two tensors using the generic +: print(x + y) #&gt; tensor([[1.4858, 1.7484, 1.3578, 1.4873], #&gt; [0.9906, 1.0733, 0.2095, 1.4747], #&gt; [0.2601, 0.8075, 1.8880, 1.6219], #&gt; [1.3639, 1.4203, 1.4110, 0.5759], #&gt; [0.3801, 1.0272, 1.0236, 1.2996]]) 3.2.2 Multiply a tensor by a scalar # Multiply tensor by scalar tensor = torch$ones(4L, dtype=torch$float64) scalar = np$float64(4.321) print(scalar) #&gt; [1] 4.32 print(torch$scalar_tensor(scalar)) #&gt; tensor(4.3210) Multiply two tensors using the function mul: (prod = torch$mul(tensor, torch$scalar_tensor(scalar))) #&gt; tensor([4.3210, 4.3210, 4.3210, 4.3210]) Short version using generics (prod = tensor * scalar) #&gt; tensor([4.3210, 4.3210, 4.3210, 4.3210]) 3.3 NumPy and PyTorch numpy has been made available as a module in rTorch. We can call functions from numpy refrerring to it as np$_a_function. Examples: # a 2D numpy array syn0 &lt;- np$random$rand(3L, 5L) print(syn0) #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 0.559 0.0208 0.0291 0.267 0.112 #&gt; [2,] 0.952 0.2273 0.3871 0.212 0.677 #&gt; [3,] 0.938 0.1541 0.4297 0.616 0.402 # numpy arrays of zeros syn1 &lt;- np$zeros(c(5L, 10L)) print(syn1) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 0 0 0 0 0 0 0 0 0 0 #&gt; [2,] 0 0 0 0 0 0 0 0 0 0 #&gt; [3,] 0 0 0 0 0 0 0 0 0 0 #&gt; [4,] 0 0 0 0 0 0 0 0 0 0 #&gt; [5,] 0 0 0 0 0 0 0 0 0 0 # add a scalar to a numpy array syn1 = syn1 + 0.1 print(syn1) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 #&gt; [2,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 #&gt; [3,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 #&gt; [4,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 #&gt; [5,] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 3.3.1 Tuples (Python) and vectors (R) In numpy a multidimensional array needs to be defined with a tuple in R we do it with a vector. In Python, we use a tuple, (5, 5) import numpy as np print(np.ones((5, 5))) #&gt; [[1. 1. 1. 1. 1.] #&gt; [1. 1. 1. 1. 1.] #&gt; [1. 1. 1. 1. 1.] #&gt; [1. 1. 1. 1. 1.] #&gt; [1. 1. 1. 1. 1.]] In R, we use a vector c(5L, 5L). The L indicates an integer. l1 &lt;- np$ones(c(5L, 5L)) print(l1) #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 1 1 1 1 1 #&gt; [2,] 1 1 1 1 1 #&gt; [3,] 1 1 1 1 1 #&gt; [4,] 1 1 1 1 1 #&gt; [5,] 1 1 1 1 1 Vector-matrix multiplication in numpy: np$dot(syn0, syn1) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 0.0988 0.0988 0.0988 0.0988 0.0988 0.0988 0.0988 0.0988 0.0988 0.0988 #&gt; [2,] 0.2456 0.2456 0.2456 0.2456 0.2456 0.2456 0.2456 0.2456 0.2456 0.2456 #&gt; [3,] 0.2540 0.2540 0.2540 0.2540 0.2540 0.2540 0.2540 0.2540 0.2540 0.2540 Build a numpy array from three R vectors: X &lt;- np$array(rbind(c(1,2,3), c(4,5,6), c(7,8,9))) print(X) #&gt; [,1] [,2] [,3] #&gt; [1,] 1 2 3 #&gt; [2,] 4 5 6 #&gt; [3,] 7 8 9 And transpose the array: np$transpose(X) #&gt; [,1] [,2] [,3] #&gt; [1,] 1 4 7 #&gt; [2,] 2 5 8 #&gt; [3,] 3 6 9 3.3.2 Make a numpy array a tensor with as_tensor() a = np$array(list(1, 2, 3)) # a numpy array t = torch$as_tensor(a) # convert it to tensor print(t) #&gt; tensor([1., 2., 3.]) We can create the tensor directly from R using tensor(): torch$tensor(list( 1, 2, 3)) # create a tensor #&gt; tensor([1., 2., 3.]) t[1L]$fill_(-1) # fill element with -1 #&gt; tensor(-1.) print(a) #&gt; [1] -1 2 3 3.3.3 Tensor to array, and viceversa This is a very common operation in machine learning: # convert tensor to a numpy array a = torch$rand(5L, 4L) b = a$numpy() print(b) #&gt; [,1] [,2] [,3] [,4] #&gt; [1,] 0.226 0.150 0.524059 0.387 #&gt; [2,] 0.611 0.819 0.000215 0.671 #&gt; [3,] 0.913 0.952 0.724134 0.756 #&gt; [4,] 0.720 0.461 0.482003 0.726 #&gt; [5,] 0.454 0.931 0.800009 0.483 # convert a numpy array to a tensor np_a = np$array(c(c(3, 4), c(3, 6))) t_a = torch$from_numpy(np_a) print(t_a) #&gt; tensor([3., 4., 3., 6.]) 3.4 Create tensors A random 1D tensor: ft1 &lt;- torch$FloatTensor(np$random$rand(5L)) print(ft1) #&gt; tensor([0.2879, 0.3993, 0.8096, 0.7798, 0.0298], dtype=torch.float32) Force a tensor as a float of 64-bits: ft2 &lt;- torch$as_tensor(np$random$rand(5L), dtype= torch$float64) print(ft2) #&gt; tensor([0.9916, 0.3176, 0.9034, 0.5072, 0.0209]) Convert the tensor to float 16-bits: ft2_dbl &lt;- torch$as_tensor(ft2, dtype = torch$float16) ft2_dbl #&gt; tensor([0.9917, 0.3176, 0.9033, 0.5073, 0.0209], dtype=torch.float16) Create a tensor of size (5 x 7) with uninitialized memory: a &lt;- torch$FloatTensor(5L, 7L) print(a) #&gt; tensor([[-3.3741e-10, 4.5654e-41, 3.8451e+02, 3.0644e-41, 1.4013e-45, #&gt; 0.0000e+00, 3.0514e+06], #&gt; [ 3.0644e-41, 0.0000e+00, 0.0000e+00, 4.2039e-45, 0.0000e+00, #&gt; 0.0000e+00, 0.0000e+00], #&gt; [ 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4906e+07, #&gt; 3.0644e-41, 0.0000e+00], #&gt; [ 0.0000e+00, 1.6064e+07, 3.0644e-41, 0.0000e+00, 0.0000e+00, #&gt; 1.4013e-45, 0.0000e+00], #&gt; [ 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, #&gt; 0.0000e+00, 1.4906e+07]], dtype=torch.float32) Using arange to create a tensor. Start from 0: v = torch$arange(9L) (v = v$view(3L, 3L)) #&gt; tensor([[0, 1, 2], #&gt; [3, 4, 5], #&gt; [6, 7, 8]]) 3.5 Tensor resizing x = torch$randn(2L, 3L) # Size 2x3 y = x$view(6L) # Resize x to size 6 z = x$view(-1L, 2L) # Size 3x2 print(y) #&gt; tensor([-1.0758, 0.6942, 1.3438, 0.3493, -0.8490, 0.8410]) print(z) #&gt; tensor([[-1.0758, 0.6942], #&gt; [ 1.3438, 0.3493], #&gt; [-0.8490, 0.8410]]) Reproduce this tensor: 0 1 2 3 4 5 6 7 8 v = torch$arange(9L) (v = v$view(3L, 3L)) #&gt; tensor([[0, 1, 2], #&gt; [3, 4, 5], #&gt; [6, 7, 8]]) 3.5.1 Concatenate tensors x = torch$randn(2L, 3L) print(x) #&gt; tensor([[ 1.6602, 1.1016, -0.1551], #&gt; [ 0.9467, -0.6704, -1.4392]]) Concatenate tensors by dim=0: torch$cat(list(x, x, x), 0L) #&gt; tensor([[ 1.6602, 1.1016, -0.1551], #&gt; [ 0.9467, -0.6704, -1.4392], #&gt; [ 1.6602, 1.1016, -0.1551], #&gt; [ 0.9467, -0.6704, -1.4392], #&gt; [ 1.6602, 1.1016, -0.1551], #&gt; [ 0.9467, -0.6704, -1.4392]]) Concatenate tensors by dim=1: torch$cat(list(x, x, x), 1L) #&gt; tensor([[ 1.6602, 1.1016, -0.1551, 1.6602, 1.1016, -0.1551, 1.6602, 1.1016, #&gt; -0.1551], #&gt; [ 0.9467, -0.6704, -1.4392, 0.9467, -0.6704, -1.4392, 0.9467, -0.6704, #&gt; -1.4392]]) 3.6 Reshape tensors 3.6.1 With function chunk(): Let’s say this is an image tensor with the 3-channels and 28x28 pixels # ----- Reshape tensors ----- img &lt;- torch$ones(3L, 28L, 28L) # Create the tensor of ones print(img$size()) #&gt; torch.Size([3, 28, 28]) On the first dimension dim = 0L, reshape the tensor: img_chunks &lt;- torch$chunk(img, chunks = 3L, dim = 0L) print(length(img_chunks)) #&gt; [1] 3 The first chunk member: # 1st chunk member img_chunk &lt;- img_chunks[[1]] print(img_chunk$size()) #&gt; torch.Size([1, 28, 28]) print(img_chunk$sum()) # if the tensor had all ones, what is the sum? #&gt; tensor(784.) The second chunk member: # 2nd chunk member img_chunk &lt;- img_chunks[[2]] print(img_chunk$size()) #&gt; torch.Size([1, 28, 28]) print(img_chunk$sum()) # if the tensor had all ones, what is the sum? #&gt; tensor(784.) # 3rd chunk member img_chunk &lt;- img_chunks[[3]] print(img_chunk$size()) #&gt; torch.Size([1, 28, 28]) print(img_chunk$sum()) # if the tensor had all ones, what is the sum? #&gt; tensor(784.) 3.6.2 With index_select(): img &lt;- torch$ones(3L, 28L, 28L) # Create the tensor of ones This is the layer 1: # index_select. get layer 1 indices = torch$tensor(c(0L)) img_layer &lt;- torch$index_select(img, dim = 0L, index = indices) The size of the layer: print(img_layer$size()) #&gt; torch.Size([1, 28, 28]) The sum of all elements in that layer: print(img_layer$sum()) #&gt; tensor(784.) This is the layer 2: # index_select. get layer 2 indices = torch$tensor(c(1L)) img_layer &lt;- torch$index_select(img, dim = 0L, index = indices) print(img_layer$size()) #&gt; torch.Size([1, 28, 28]) print(img_layer$sum()) #&gt; tensor(784.) This is the layer 3: # index_select. get layer 3 indices = torch$tensor(c(2L)) img_layer &lt;- torch$index_select(img, dim = 0L, index = indices) print(img_layer$size()) #&gt; torch.Size([1, 28, 28]) print(img_layer$sum()) #&gt; tensor(784.) 3.7 Special tensors 3.7.1 Identity matrix # identity matrix eye = torch$eye(3L) # Create an identity 3x3 tensor print(eye) #&gt; tensor([[1., 0., 0.], #&gt; [0., 1., 0.], #&gt; [0., 0., 1.]]) 3.7.2 Ones (v = torch$ones(10L)) # A tensor of size 10 containing all ones #&gt; tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) (v = torch$ones(2L, 1L, 2L, 1L)) # Size 2x1x2x1 #&gt; tensor([[[[1.], #&gt; [1.]]], #&gt; #&gt; #&gt; [[[1.], #&gt; [1.]]]]) v = torch$ones_like(eye) # A tensor with same shape as eye. Fill it with 1. v #&gt; tensor([[1., 1., 1.], #&gt; [1., 1., 1.], #&gt; [1., 1., 1.]]) 3.7.3 Zeros (z = torch$zeros(10L)) # A tensor of size 10 containing all zeros #&gt; tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) 3.8 Tensor fill On this tensor: (v = torch$ones(3L, 3L)) #&gt; tensor([[1., 1., 1.], #&gt; [1., 1., 1.], #&gt; [1., 1., 1.]]) Fill row 1 with 2s: v[1L, ]$fill_(2L) #&gt; tensor([2., 2., 2.]) print(v) #&gt; tensor([[2., 2., 2.], #&gt; [1., 1., 1.], #&gt; [1., 1., 1.]]) Fill row 2 with 3s: v[2L, ]$fill_(3L) #&gt; tensor([3., 3., 3.]) print(v) #&gt; tensor([[2., 2., 2.], #&gt; [3., 3., 3.], #&gt; [1., 1., 1.]]) # Initialize Tensor with a range of value v = torch$arange(10L) # similar to range(5) but creating a Tensor (v = torch$arange(0L, 10L, step = 1L)) # Size 5. Similar to range(0, 5, 1) #&gt; tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 3.8.1 Initialize a linear or log scale Tensor Create a tensor with 10 linear points for (1, 10) inclusive: (v = torch$linspace(1L, 10L, steps = 10L)) #&gt; tensor([ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.]) Create a tensor with 10 logarithmic points for (1, 10) inclusive: (v = torch$logspace(start=-10L, end = 10L, steps = 5L)) #&gt; tensor([1.0000e-10, 1.0000e-05, 1.0000e+00, 1.0000e+05, 1.0000e+10]) 3.8.2 Inplace / Out-of-place On this uninitialized tensor: (a &lt;- torch$FloatTensor(5L, 7L)) #&gt; tensor([[0.0000e+00, 0.0000e+00, 2.8292e+05, 3.0644e-41, 2.8292e+05, 3.0644e-41, #&gt; 0.0000e+00], #&gt; [0.0000e+00, 1.3733e+07, 3.0644e-41, 1.3733e+07, 3.0644e-41, 1.3733e+07, #&gt; 3.0644e-41], #&gt; [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, #&gt; 0.0000e+00], #&gt; [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3733e+07, 3.0644e-41, 1.3733e+07, #&gt; 3.0644e-41], #&gt; [1.3733e+07, 3.0644e-41, 1.4013e-45, 0.0000e+00, 0.0000e+00, 0.0000e+00, #&gt; 0.0000e+00]], dtype=torch.float32) Fill the tensor with the value 3.5: a$fill_(3.5) #&gt; tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]], #&gt; dtype=torch.float32) Add a scalar to the tensor: b &lt;- a$add(4.0) The tensor a is still filled with 3.5. A new tensor b is returned with values 3.5 + 4.0 = 7.5 print(a) #&gt; tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000], #&gt; [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]], #&gt; dtype=torch.float32) print(b) #&gt; tensor([[7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000], #&gt; [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000], #&gt; [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000], #&gt; [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000], #&gt; [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000]], #&gt; dtype=torch.float32) 3.9 Access to tensor elements # replace an element at position 0, 0 (new_tensor = torch$Tensor(list(list(1, 2), list(3, 4)))) #&gt; tensor([[1., 2.], #&gt; [3., 4.]]) Print element at position 1,1: print(new_tensor[1L, 1L]) #&gt; tensor(1.) Fill element at position 1,1 with 5: new_tensor[1L, 1L]$fill_(5) #&gt; tensor(5.) Show the modified tensor: print(new_tensor) # tensor([[ 5., 2.],[ 3., 4.]]) #&gt; tensor([[5., 2.], #&gt; [3., 4.]]) Access an element at position 1, 0: print(new_tensor[2L, 1L]) # tensor([ 3.]) #&gt; tensor(3.) print(new_tensor[2L, 1L]$item()) # 3. #&gt; [1] 3 3.9.1 Using indices to access elements On this tensor: x = torch$randn(3L, 4L) print(x) #&gt; tensor([[ 1.7255, 0.7821, -0.5988, 0.7119], #&gt; [ 0.9558, -1.1081, 1.5438, 0.7918], #&gt; [-0.8336, -1.0546, -0.2481, 0.3643]]) Select indices, dim=0: indices = torch$tensor(list(0L, 2L)) torch$index_select(x, 0L, indices) #&gt; tensor([[ 1.7255, 0.7821, -0.5988, 0.7119], #&gt; [-0.8336, -1.0546, -0.2481, 0.3643]]) Select indices, dim=1: torch$index_select(x, 1L, indices) #&gt; tensor([[ 1.7255, -0.5988], #&gt; [ 0.9558, 1.5438], #&gt; [-0.8336, -0.2481]]) 3.9.2 Using the take function # Take by indices src = torch$tensor(list(list(4, 3, 5), list(6, 7, 8)) ) print(src) #&gt; tensor([[4., 3., 5.], #&gt; [6., 7., 8.]]) print( torch$take(src, torch$tensor(list(0L, 2L, 5L))) ) #&gt; tensor([4., 5., 8.]) 3.10 Other tensor operations 3.10.1 Cross product m1 = torch$ones(3L, 5L) m2 = torch$ones(3L, 5L) v1 = torch$ones(3L) # Cross product # Size 3x5 (r = torch$cross(m1, m2)) #&gt; tensor([[0., 0., 0., 0., 0.], #&gt; [0., 0., 0., 0., 0.], #&gt; [0., 0., 0., 0., 0.]]) 3.10.2 Dot product # Dot product of 2 tensors # Dot product of 2 tensors p &lt;- torch$Tensor(list(4L, 2L)) q &lt;- torch$Tensor(list(3L, 1L)) (r = torch$dot(p, q)) # 14 #&gt; tensor(14.) (r &lt;- p %.*% q) #&gt; tensor(14.) 3.11 Logical operations m0 = torch$zeros(3L, 5L) m1 = torch$ones(3L, 5L) m2 = torch$eye(3L, 5L) print(m1 == m0) #&gt; tensor([[False, False, False, False, False], #&gt; [False, False, False, False, False], #&gt; [False, False, False, False, False]], dtype=torch.bool) print(m1 != m1) #&gt; tensor([[False, False, False, False, False], #&gt; [False, False, False, False, False], #&gt; [False, False, False, False, False]], dtype=torch.bool) print(m2 == m2) #&gt; tensor([[True, True, True, True, True], #&gt; [True, True, True, True, True], #&gt; [True, True, True, True, True]], dtype=torch.bool) # AND m1 &amp; m1 #&gt; tensor([[True, True, True, True, True], #&gt; [True, True, True, True, True], #&gt; [True, True, True, True, True]], dtype=torch.bool) # OR m0 | m2 #&gt; tensor([[ True, False, False, False, False], #&gt; [False, True, False, False, False], #&gt; [False, False, True, False, False]], dtype=torch.bool) # OR m1 | m2 #&gt; tensor([[True, True, True, True, True], #&gt; [True, True, True, True, True], #&gt; [True, True, True, True, True]], dtype=torch.bool) # all_boolean &lt;- function(x) { # # convert tensor of 1s and 0s to a unique boolean # as.logical(torch$all(x)$numpy()) # } # tensor is less than A &lt;- torch$ones(60000L, 1L, 28L, 28L) C &lt;- A * 0.5 # is C &lt; A all(torch$lt(C, A)) #&gt; tensor(1, dtype=torch.uint8) all(C &lt; A) #&gt; tensor(1, dtype=torch.uint8) # is A &lt; C all(A &lt; C) #&gt; tensor(0, dtype=torch.uint8) # tensor is greater than A &lt;- torch$ones(60000L, 1L, 28L, 28L) D &lt;- A * 2.0 all(torch$gt(D, A)) #&gt; tensor(1, dtype=torch.uint8) all(torch$gt(A, D)) #&gt; tensor(0, dtype=torch.uint8) # tensor is less than or equal A1 &lt;- torch$ones(60000L, 1L, 28L, 28L) all(torch$le(A1, A1)) #&gt; tensor(1, dtype=torch.uint8) all(A1 &lt;= A1) #&gt; tensor(1, dtype=torch.uint8) # tensor is greater than or equal A0 &lt;- torch$zeros(60000L, 1L, 28L, 28L) all(torch$ge(A0, A0)) #&gt; tensor(1, dtype=torch.uint8) all(A0 &gt;= A0) #&gt; tensor(1, dtype=torch.uint8) all(A1 &gt;= A0) #&gt; tensor(1, dtype=torch.uint8) all(A1 &lt;= A0) #&gt; tensor(0, dtype=torch.uint8) 3.11.1 Logical NOT all_true &lt;- torch$BoolTensor(list(TRUE, TRUE, TRUE, TRUE)) all_true #&gt; tensor([True, True, True, True], dtype=torch.bool) # logical NOT not_all_true &lt;- !all_true not_all_true #&gt; tensor([False, False, False, False], dtype=torch.bool) diag &lt;- torch$eye(5L) diag #&gt; tensor([[1., 0., 0., 0., 0.], #&gt; [0., 1., 0., 0., 0.], #&gt; [0., 0., 1., 0., 0.], #&gt; [0., 0., 0., 1., 0.], #&gt; [0., 0., 0., 0., 1.]]) # logical NOT not_diag &lt;- !diag # convert to integer not_diag$to(dtype=torch$uint8) #&gt; tensor([[0, 1, 1, 1, 1], #&gt; [1, 0, 1, 1, 1], #&gt; [1, 1, 0, 1, 1], #&gt; [1, 1, 1, 0, 1], #&gt; [1, 1, 1, 1, 0]], dtype=torch.uint8) 3.12 Distributions Initialize a tensor randomized with a normal distribution with mean=0, var=1: a &lt;- torch$randn(5L, 7L) print(a) #&gt; tensor([[-1.1013, 0.7958, 0.3010, -0.1767, -0.9521, -0.2378, -0.2243], #&gt; [-0.6484, 1.4482, -0.5799, 0.5762, 0.5901, 0.8748, 0.3789], #&gt; [ 2.2061, 1.4470, 1.9231, -3.0280, -0.1786, -0.4835, 0.2372], #&gt; [ 0.0514, 0.1188, 0.1995, 0.3786, 0.5452, 0.4128, -0.7809], #&gt; [-0.6479, 0.6762, 0.2314, -0.6185, 0.2549, -0.3033, 0.6026]]) print(a$size()) #&gt; torch.Size([5, 7]) 3.12.1 Uniform matrix library(rTorch) # 3x5 matrix uniformly distributed between 0 and 1 mat0 &lt;- torch$FloatTensor(3L, 5L)$uniform_(0L, 1L) # fill a 3x5 matrix with 0.1 mat1 &lt;- torch$FloatTensor(3L, 5L)$uniform_(0.1, 0.1) # a vector with all ones mat2 &lt;- torch$FloatTensor(5L)$uniform_(1, 1) mat0 #&gt; tensor([[0.3589, 0.4819, 0.5588, 0.3070, 0.4982], #&gt; [0.8788, 0.9769, 0.4635, 0.2740, 0.8148], #&gt; [0.3031, 0.2872, 0.2409, 0.9991, 0.6892]], dtype=torch.float32) mat1 #&gt; tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000], #&gt; [0.1000, 0.1000, 0.1000, 0.1000, 0.1000], #&gt; [0.1000, 0.1000, 0.1000, 0.1000, 0.1000]], dtype=torch.float32) 3.12.2 Binomial distribution Binomial &lt;- torch$distributions$binomial$Binomial m = Binomial(100, torch$tensor(list(0 , .2, .8, 1))) (x = m$sample()) #&gt; tensor([ 0., 24., 76., 100.]) m = Binomial(torch$tensor(list(list(5.), list(10.))), torch$tensor(list(0.5, 0.8))) (x = m$sample()) #&gt; tensor([[1., 3.], #&gt; [4., 8.]]) 3.12.3 Exponential distribution Exponential &lt;- torch$distributions$exponential$Exponential m = Exponential(torch$tensor(list(1.0))) m$sample() # Exponential distributed with rate=1 #&gt; tensor([0.2964]) 3.12.4 Weibull distribution Weibull &lt;- torch$distributions$weibull$Weibull m = Weibull(torch$tensor(list(1.0)), torch$tensor(list(1.0))) m$sample() # sample from a Weibull distribution with scale=1, concentration=1 #&gt; tensor([0.4213]) "],
["linearalgebra.html", "Chapter 4 Linear Algebra with Torch 4.1 Scalars 4.2 Vectors 4.3 Matrices 4.4 3D+ tensors 4.5 Transpose of a matrix 4.6 Vectors, special case of a matrix 4.7 Tensor arithmetic 4.8 Add a scalar to a tensor 4.9 Multiplying tensors 4.10 Dot product", " Chapter 4 Linear Algebra with Torch The following are basic operations of Linear Algebra using PyTorch. library(rTorch) 4.1 Scalars torch$scalar_tensor(2.78654) #&gt; tensor(2.7865) torch$scalar_tensor(0L) #&gt; tensor(0.) torch$scalar_tensor(1L) #&gt; tensor(1.) torch$scalar_tensor(TRUE) #&gt; tensor(1.) torch$scalar_tensor(FALSE) #&gt; tensor(0.) 4.2 Vectors v &lt;- c(0, 1, 2, 3, 4, 5) torch$as_tensor(v) #&gt; tensor([0., 1., 2., 3., 4., 5.]) # row-vector message(&quot;R matrix&quot;) #&gt; R matrix (mr &lt;- matrix(1:10, nrow=1)) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 1 2 3 4 5 6 7 8 9 10 message(&quot;as_tensor&quot;) #&gt; as_tensor torch$as_tensor(mr) #&gt; tensor([[ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], dtype=torch.int32) message(&quot;shape_of_tensor&quot;) #&gt; shape_of_tensor torch$as_tensor(mr)$shape #&gt; torch.Size([1, 10]) # column-vector message(&quot;R matrix, one column&quot;) #&gt; R matrix, one column (mc &lt;- matrix(1:10, ncol=1)) #&gt; [,1] #&gt; [1,] 1 #&gt; [2,] 2 #&gt; [3,] 3 #&gt; [4,] 4 #&gt; [5,] 5 #&gt; [6,] 6 #&gt; [7,] 7 #&gt; [8,] 8 #&gt; [9,] 9 #&gt; [10,] 10 message(&quot;as_tensor&quot;) #&gt; as_tensor torch$as_tensor(mc) #&gt; tensor([[ 1], #&gt; [ 2], #&gt; [ 3], #&gt; [ 4], #&gt; [ 5], #&gt; [ 6], #&gt; [ 7], #&gt; [ 8], #&gt; [ 9], #&gt; [10]], dtype=torch.int32) message(&quot;size of tensor&quot;) #&gt; size of tensor torch$as_tensor(mc)$shape #&gt; torch.Size([10, 1]) 4.3 Matrices message(&quot;R matrix&quot;) #&gt; R matrix (m1 &lt;- matrix(1:24, nrow = 3, byrow = TRUE)) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; [1,] 1 2 3 4 5 6 7 8 #&gt; [2,] 9 10 11 12 13 14 15 16 #&gt; [3,] 17 18 19 20 21 22 23 24 message(&quot;as_tensor&quot;) #&gt; as_tensor (t1 &lt;- torch$as_tensor(m1)) #&gt; tensor([[ 1, 2, 3, 4, 5, 6, 7, 8], #&gt; [ 9, 10, 11, 12, 13, 14, 15, 16], #&gt; [17, 18, 19, 20, 21, 22, 23, 24]], dtype=torch.int32) message(&quot;shape&quot;) #&gt; shape torch$as_tensor(m1)$shape #&gt; torch.Size([3, 8]) message(&quot;size&quot;) #&gt; size torch$as_tensor(m1)$size() #&gt; torch.Size([3, 8]) message(&quot;dim&quot;) #&gt; dim dim(torch$as_tensor(m1)) #&gt; [1] 3 8 message(&quot;length&quot;) #&gt; length length(torch$as_tensor(m1)) #&gt; [1] 24 message(&quot;R matrix&quot;) #&gt; R matrix (m2 &lt;- matrix(0:99, ncol = 10)) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] #&gt; [1,] 0 10 20 30 40 50 60 70 80 90 #&gt; [2,] 1 11 21 31 41 51 61 71 81 91 #&gt; [3,] 2 12 22 32 42 52 62 72 82 92 #&gt; [4,] 3 13 23 33 43 53 63 73 83 93 #&gt; [5,] 4 14 24 34 44 54 64 74 84 94 #&gt; [6,] 5 15 25 35 45 55 65 75 85 95 #&gt; [7,] 6 16 26 36 46 56 66 76 86 96 #&gt; [8,] 7 17 27 37 47 57 67 77 87 97 #&gt; [9,] 8 18 28 38 48 58 68 78 88 98 #&gt; [10,] 9 19 29 39 49 59 69 79 89 99 message(&quot;as_tensor&quot;) #&gt; as_tensor (t2 &lt;- torch$as_tensor(m2)) #&gt; tensor([[ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90], #&gt; [ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91], #&gt; [ 2, 12, 22, 32, 42, 52, 62, 72, 82, 92], #&gt; [ 3, 13, 23, 33, 43, 53, 63, 73, 83, 93], #&gt; [ 4, 14, 24, 34, 44, 54, 64, 74, 84, 94], #&gt; [ 5, 15, 25, 35, 45, 55, 65, 75, 85, 95], #&gt; [ 6, 16, 26, 36, 46, 56, 66, 76, 86, 96], #&gt; [ 7, 17, 27, 37, 47, 57, 67, 77, 87, 97], #&gt; [ 8, 18, 28, 38, 48, 58, 68, 78, 88, 98], #&gt; [ 9, 19, 29, 39, 49, 59, 69, 79, 89, 99]], dtype=torch.int32) message(&quot;shape&quot;) #&gt; shape t2$shape #&gt; torch.Size([10, 10]) message(&quot;dim&quot;) #&gt; dim dim(torch$as_tensor(m2)) #&gt; [1] 10 10 m1[1, 1] #&gt; [1] 1 m2[1, 1] #&gt; [1] 0 t1[1, 1] #&gt; tensor(1, dtype=torch.int32) t2[1, 1] #&gt; tensor(0, dtype=torch.int32) 4.4 3D+ tensors # RGB color image has three axes (img &lt;- torch$rand(3L, 28L, 28L)) #&gt; tensor([[[2.3195e-01, 2.7901e-01, 6.2459e-01, ..., 8.4426e-01, #&gt; 2.4758e-01, 4.3068e-01], #&gt; [3.2518e-01, 6.7801e-01, 7.2160e-01, ..., 9.2250e-01, #&gt; 6.9417e-01, 6.3049e-01], #&gt; [4.8502e-01, 3.1521e-01, 9.1004e-02, ..., 7.3325e-01, #&gt; 7.1481e-01, 4.9595e-01], #&gt; ..., #&gt; [4.5947e-01, 4.6499e-02, 3.6209e-01, ..., 4.9285e-01, #&gt; 5.3635e-01, 7.5423e-01], #&gt; [9.0498e-01, 5.0215e-01, 9.4646e-01, ..., 4.5080e-01, #&gt; 9.5163e-01, 1.0610e-01], #&gt; [1.7515e-01, 2.1939e-01, 8.6337e-01, ..., 5.6108e-01, #&gt; 7.1970e-01, 7.2553e-01]], #&gt; #&gt; [[8.7039e-01, 1.9537e-01, 2.5732e-01, ..., 5.6670e-01, #&gt; 2.7417e-01, 3.8252e-01], #&gt; [1.1267e-01, 9.8972e-01, 8.9118e-01, ..., 8.4189e-01, #&gt; 3.6573e-01, 7.6749e-01], #&gt; [8.9792e-01, 3.5984e-01, 8.3035e-01, ..., 7.3262e-01, #&gt; 5.9792e-01, 3.6532e-01], #&gt; ..., #&gt; [5.7248e-01, 7.5934e-01, 5.3420e-01, ..., 6.5942e-01, #&gt; 5.9810e-01, 4.0073e-01], #&gt; [4.6327e-01, 8.9703e-01, 1.5025e-02, ..., 4.7061e-01, #&gt; 3.6548e-01, 4.1729e-04], #&gt; [8.9610e-01, 2.6140e-01, 4.5616e-01, ..., 1.1952e-01, #&gt; 1.6670e-01, 5.9445e-02]], #&gt; #&gt; [[8.8381e-01, 9.7859e-01, 1.6928e-01, ..., 1.8248e-01, #&gt; 3.7392e-01, 7.0518e-01], #&gt; [2.4729e-01, 5.3682e-01, 5.0760e-01, ..., 8.5275e-01, #&gt; 6.6125e-01, 5.9745e-01], #&gt; [4.7013e-02, 1.2857e-01, 5.2290e-03, ..., 3.1945e-01, #&gt; 1.3336e-01, 6.9872e-01], #&gt; ..., #&gt; [4.9083e-01, 1.1735e-01, 8.1534e-01, ..., 3.4907e-01, #&gt; 3.5142e-01, 4.6827e-01], #&gt; [2.5672e-01, 2.5291e-01, 5.7967e-01, ..., 4.4226e-01, #&gt; 2.8406e-02, 5.6944e-01], #&gt; [5.7018e-01, 5.6662e-01, 3.6400e-03, ..., 3.4394e-01, #&gt; 5.0076e-01, 7.9066e-01]]]) img$shape #&gt; torch.Size([3, 28, 28]) img[1, 1, 1] #&gt; tensor(0.2320) img[3, 28, 28] #&gt; tensor(0.7907) 4.5 Transpose of a matrix (m3 &lt;- matrix(1:25, ncol = 5)) #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 1 6 11 16 21 #&gt; [2,] 2 7 12 17 22 #&gt; [3,] 3 8 13 18 23 #&gt; [4,] 4 9 14 19 24 #&gt; [5,] 5 10 15 20 25 # transpose message(&quot;transpose&quot;) #&gt; transpose tm3 &lt;- t(m3) tm3 #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 1 2 3 4 5 #&gt; [2,] 6 7 8 9 10 #&gt; [3,] 11 12 13 14 15 #&gt; [4,] 16 17 18 19 20 #&gt; [5,] 21 22 23 24 25 message(&quot;as_tensor&quot;) #&gt; as_tensor (t3 &lt;- torch$as_tensor(m3)) #&gt; tensor([[ 1, 6, 11, 16, 21], #&gt; [ 2, 7, 12, 17, 22], #&gt; [ 3, 8, 13, 18, 23], #&gt; [ 4, 9, 14, 19, 24], #&gt; [ 5, 10, 15, 20, 25]], dtype=torch.int32) message(&quot;transpose&quot;) #&gt; transpose tt3 &lt;- t3$transpose(dim0 = 0L, dim1 = 1L) tt3 #&gt; tensor([[ 1, 2, 3, 4, 5], #&gt; [ 6, 7, 8, 9, 10], #&gt; [11, 12, 13, 14, 15], #&gt; [16, 17, 18, 19, 20], #&gt; [21, 22, 23, 24, 25]], dtype=torch.int32) tm3 == tt3$numpy() # convert first the tensor to numpy #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] TRUE TRUE TRUE TRUE TRUE #&gt; [2,] TRUE TRUE TRUE TRUE TRUE #&gt; [3,] TRUE TRUE TRUE TRUE TRUE #&gt; [4,] TRUE TRUE TRUE TRUE TRUE #&gt; [5,] TRUE TRUE TRUE TRUE TRUE 4.6 Vectors, special case of a matrix message(&quot;R matrix&quot;) #&gt; R matrix m2 &lt;- matrix(0:99, ncol = 10) message(&quot;as_tensor&quot;) #&gt; as_tensor (t2 &lt;- torch$as_tensor(m2)) #&gt; tensor([[ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90], #&gt; [ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91], #&gt; [ 2, 12, 22, 32, 42, 52, 62, 72, 82, 92], #&gt; [ 3, 13, 23, 33, 43, 53, 63, 73, 83, 93], #&gt; [ 4, 14, 24, 34, 44, 54, 64, 74, 84, 94], #&gt; [ 5, 15, 25, 35, 45, 55, 65, 75, 85, 95], #&gt; [ 6, 16, 26, 36, 46, 56, 66, 76, 86, 96], #&gt; [ 7, 17, 27, 37, 47, 57, 67, 77, 87, 97], #&gt; [ 8, 18, 28, 38, 48, 58, 68, 78, 88, 98], #&gt; [ 9, 19, 29, 39, 49, 59, 69, 79, 89, 99]], dtype=torch.int32) # in R message(&quot;select column of matrix&quot;) #&gt; select column of matrix (v1 &lt;- m2[, 1]) #&gt; [1] 0 1 2 3 4 5 6 7 8 9 message(&quot;select row of matrix&quot;) #&gt; select row of matrix (v2 &lt;- m2[10, ]) #&gt; [1] 9 19 29 39 49 59 69 79 89 99 # PyTorch message() #&gt; t2c &lt;- t2[, 1] t2r &lt;- t2[10, ] t2c #&gt; tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32) t2r #&gt; tensor([ 9, 19, 29, 39, 49, 59, 69, 79, 89, 99], dtype=torch.int32) In vectors, the vector and its transpose are equal. tt2r &lt;- t2r$transpose(dim0 = 0L, dim1 = 0L) tt2r #&gt; tensor([ 9, 19, 29, 39, 49, 59, 69, 79, 89, 99], dtype=torch.int32) # a tensor of booleans. is vector equal to its transposed? t2r == tt2r #&gt; tensor([True, True, True, True, True, True, True, True, True, True], #&gt; dtype=torch.bool) 4.7 Tensor arithmetic message(&quot;x&quot;) #&gt; x (x = torch$ones(5L, 4L)) #&gt; tensor([[1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.]]) message(&quot;y&quot;) #&gt; y (y = torch$ones(5L, 4L)) #&gt; tensor([[1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.]]) message(&quot;x+y&quot;) #&gt; x+y x + y #&gt; tensor([[2., 2., 2., 2.], #&gt; [2., 2., 2., 2.], #&gt; [2., 2., 2., 2.], #&gt; [2., 2., 2., 2.], #&gt; [2., 2., 2., 2.]]) \\[A + B = B + A\\] x + y == y + x #&gt; tensor([[True, True, True, True], #&gt; [True, True, True, True], #&gt; [True, True, True, True], #&gt; [True, True, True, True], #&gt; [True, True, True, True]], dtype=torch.bool) 4.8 Add a scalar to a tensor s &lt;- 0.5 # scalar x + s #&gt; tensor([[1.5000, 1.5000, 1.5000, 1.5000], #&gt; [1.5000, 1.5000, 1.5000, 1.5000], #&gt; [1.5000, 1.5000, 1.5000, 1.5000], #&gt; [1.5000, 1.5000, 1.5000, 1.5000], #&gt; [1.5000, 1.5000, 1.5000, 1.5000]]) # scalar multiplying two tensors s * (x + y) #&gt; tensor([[1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.]]) 4.9 Multiplying tensors \\[A * B = B * A\\] message(&quot;x&quot;) #&gt; x (x = torch$ones(5L, 4L)) #&gt; tensor([[1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.]]) message(&quot;y&quot;) #&gt; y (y = torch$ones(5L, 4L)) #&gt; tensor([[1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.], #&gt; [1., 1., 1., 1.]]) message(&quot;2x+4y&quot;) #&gt; 2x+4y (z = 2 * x + 4 * y) #&gt; tensor([[6., 6., 6., 6.], #&gt; [6., 6., 6., 6.], #&gt; [6., 6., 6., 6.], #&gt; [6., 6., 6., 6.], #&gt; [6., 6., 6., 6.]]) x * y == y * x #&gt; tensor([[True, True, True, True], #&gt; [True, True, True, True], #&gt; [True, True, True, True], #&gt; [True, True, True, True], #&gt; [True, True, True, True]], dtype=torch.bool) 4.10 Dot product \\[dot(a,b)_{i,j,k,a,b,c} = \\sum_m a_{i,j,k,m}b_{a,b,m,c}\\] torch$dot(torch$tensor(c(2, 3)), torch$tensor(c(2, 1))) #&gt; tensor(7.) a &lt;- np$array(list(list(1, 2), list(3, 4))) a #&gt; [,1] [,2] #&gt; [1,] 1 2 #&gt; [2,] 3 4 b &lt;- np$array(list(list(1, 2), list(3, 4))) b #&gt; [,1] [,2] #&gt; [1,] 1 2 #&gt; [2,] 3 4 np$dot(a, b) #&gt; [,1] [,2] #&gt; [1,] 7 10 #&gt; [2,] 15 22 torch.dot() treats both a and b as 1D vectors (irrespective of their original shape) and computes their inner product. at &lt;- torch$as_tensor(a) bt &lt;- torch$as_tensor(b) # torch$dot(at, bt) &lt;- RuntimeError: dot: Expected 1-D argument self, but got 2-D # at %.*% bt If we perform the same dot product operation in Python, we get the same error: import torch import numpy as np a = np.array([[1, 2], [3, 4]]) a #&gt; array([[1, 2], #&gt; [3, 4]]) b = np.array([[1, 2], [3, 4]]) b #&gt; array([[1, 2], #&gt; [3, 4]]) np.dot(a, b) #&gt; array([[ 7, 10], #&gt; [15, 22]]) at = torch.as_tensor(a) bt = torch.as_tensor(b) at #&gt; tensor([[1, 2], #&gt; [3, 4]]) bt #&gt; tensor([[1, 2], #&gt; [3, 4]]) torch.dot(at, bt) #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: dot: Expected 1-D argument self, but got 2-D #&gt; #&gt; Detailed traceback: #&gt; File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; a &lt;- torch$Tensor(list(list(1, 2), list(3, 4))) b &lt;- torch$Tensor(c(c(1, 2), c(3, 4))) c &lt;- torch$Tensor(list(list(11, 12), list(13, 14))) a #&gt; tensor([[1., 2.], #&gt; [3., 4.]]) b #&gt; tensor([1., 2., 3., 4.]) torch$dot(a, b) #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: dot: Expected 1-D argument self, but got 2-D # this is another way of performing dot product in PyTorch # a$dot(a) o1 &lt;- torch$ones(2L, 2L) o2 &lt;- torch$ones(2L, 2L) o1 #&gt; tensor([[1., 1.], #&gt; [1., 1.]]) o2 #&gt; tensor([[1., 1.], #&gt; [1., 1.]]) torch$dot(o1, o2) #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: dot: Expected 1-D argument self, but got 2-D o1$dot(o2) #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: dot: Expected 1-D argument self, but got 2-D # 1D tensors work fine r = torch$dot(torch$Tensor(list(4L, 2L, 4L)), torch$Tensor(list(3L, 4L, 1L))) r #&gt; tensor(24.) ## mm and matmul seem to address the dot product we are looking for in tensors a = torch$randn(2L, 3L) b = torch$randn(3L, 4L) a$mm(b) #&gt; tensor([[ 0.0280, -2.1269, -0.3169, 0.1623], #&gt; [-0.2900, 2.1708, -2.3575, 0.5110]]) a$matmul(b) #&gt; tensor([[ 0.0280, -2.1269, -0.3169, 0.1623], #&gt; [-0.2900, 2.1708, -2.3575, 0.5110]]) Here is agood explanation: https://stackoverflow.com/a/44525687/5270873 abt &lt;- torch$mm(a, b)$transpose(dim0=0L, dim1=1L) abt #&gt; tensor([[ 0.0280, -0.2900], #&gt; [-2.1269, 2.1708], #&gt; [-0.3169, -2.3575], #&gt; [ 0.1623, 0.5110]]) at &lt;- a$transpose(dim0=0L, dim1=1L) bt &lt;- b$transpose(dim0=0L, dim1=1L) btat &lt;- torch$matmul(bt, at) btat #&gt; tensor([[ 0.0280, -0.2900], #&gt; [-2.1269, 2.1708], #&gt; [-0.3169, -2.3575], #&gt; [ 0.1623, 0.5110]]) \\[(A B)^T = B^T A^T\\] # tolerance torch$allclose(abt, btat, rtol=0.0001) #&gt; [1] TRUE "],
["mnistdigits.html", "Chapter 5 Example 1: MNIST handwritten digits 5.1 Hyperparameters 5.2 Read datasets 5.3 Define the model 5.4 Training 5.5 Prediction 5.6 Save the model", " Chapter 5 Example 1: MNIST handwritten digits Source: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/logistic_regression/main.py library(rTorch) nn &lt;- torch$nn transforms &lt;- torchvision$transforms torch$set_default_dtype(torch$float) 5.1 Hyperparameters # Hyper-parameters input_size &lt;- 784L num_classes &lt;- 10L num_epochs &lt;- 5L batch_size &lt;- 100L learning_rate &lt;- 0.001 5.2 Read datasets # MNIST dataset (images and labels) # IDX format local_folder &lt;- &#39;./datasets/raw_data&#39; train_dataset = torchvision$datasets$MNIST(root=local_folder, train=TRUE, transform=transforms$ToTensor(), download=TRUE) test_dataset = torchvision$datasets$MNIST(root=local_folder, train=FALSE, transform=transforms$ToTensor()) # Data loader (input pipeline). Make the datasets iteratble train_loader = torch$utils$data$DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=TRUE) test_loader = torch$utils$data$DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=FALSE) class(train_loader) #&gt; [1] &quot;torch.utils.data.dataloader.DataLoader&quot; #&gt; [2] &quot;python.builtin.object&quot; length(train_loader) #&gt; [1] 2 5.3 Define the model # Logistic regression model model = nn$Linear(input_size, num_classes) # Loss and optimizer # nn.CrossEntropyLoss() computes softmax internally criterion = nn$CrossEntropyLoss() optimizer = torch$optim$SGD(model$parameters(), lr=learning_rate) print(model) #&gt; Linear(in_features=784, out_features=10, bias=True) 5.4 Training # Train the model iter_train_loader &lt;- iterate(train_loader) total_step &lt;-length(iter_train_loader) for (epoch in 1:num_epochs) { i &lt;- 0 for (obj in iter_train_loader) { images &lt;- obj[[1]] # tensor torch.Size([64, 3, 28, 28]) labels &lt;- obj[[2]] # tensor torch.Size([64]), labels from 0 to 9 # cat(i, &quot;\\t&quot;); print(images$shape) # Reshape images to (batch_size, input_size) images &lt;- images$reshape(-1L, 28L*28L) # images &lt;- torch$as_tensor(images$reshape(-1L, 28L*28L), dtype=torch$double) # Forward pass outputs &lt;- model(images) loss &lt;- criterion(outputs, labels) # Backward and optimize optimizer$zero_grad() loss$backward() optimizer$step() if ((i+1) %% 100 == 0) { cat(sprintf(&#39;Epoch [%d/%d], Step [%d/%d], Loss: %f \\n&#39;, epoch+1, num_epochs, i+1, total_step, loss$item())) } i &lt;- i + 1 } } #&gt; Epoch [2/5], Step [100/600], Loss: 2.207937 #&gt; Epoch [2/5], Step [200/600], Loss: 2.129730 #&gt; Epoch [2/5], Step [300/600], Loss: 2.020010 #&gt; Epoch [2/5], Step [400/600], Loss: 1.921782 #&gt; Epoch [2/5], Step [500/600], Loss: 1.842267 #&gt; Epoch [2/5], Step [600/600], Loss: 1.849545 #&gt; Epoch [3/5], Step [100/600], Loss: 1.752690 #&gt; Epoch [3/5], Step [200/600], Loss: 1.671137 #&gt; Epoch [3/5], Step [300/600], Loss: 1.618696 #&gt; Epoch [3/5], Step [400/600], Loss: 1.548630 #&gt; Epoch [3/5], Step [500/600], Loss: 1.447902 #&gt; Epoch [3/5], Step [600/600], Loss: 1.540873 #&gt; Epoch [4/5], Step [100/600], Loss: 1.462256 #&gt; Epoch [4/5], Step [200/600], Loss: 1.385923 #&gt; Epoch [4/5], Step [300/600], Loss: 1.361354 #&gt; Epoch [4/5], Step [400/600], Loss: 1.314525 #&gt; Epoch [4/5], Step [500/600], Loss: 1.196100 #&gt; Epoch [4/5], Step [600/600], Loss: 1.338717 #&gt; Epoch [5/5], Step [100/600], Loss: 1.272773 #&gt; Epoch [5/5], Step [200/600], Loss: 1.201425 #&gt; Epoch [5/5], Step [300/600], Loss: 1.194291 #&gt; Epoch [5/5], Step [400/600], Loss: 1.157681 #&gt; Epoch [5/5], Step [500/600], Loss: 1.029743 #&gt; Epoch [5/5], Step [600/600], Loss: 1.199558 #&gt; Epoch [6/5], Step [100/600], Loss: 1.144258 #&gt; Epoch [6/5], Step [200/600], Loss: 1.076404 #&gt; Epoch [6/5], Step [300/600], Loss: 1.080425 #&gt; Epoch [6/5], Step [400/600], Loss: 1.046417 #&gt; Epoch [6/5], Step [500/600], Loss: 0.914617 #&gt; Epoch [6/5], Step [600/600], Loss: 1.098780 5.5 Prediction # Adjust weights and reset gradients iter_test_loader &lt;- iterate(test_loader) with(torch$no_grad(), { correct &lt;- 0 total &lt;- 0 for (obj in iter_test_loader) { images &lt;- obj[[1]] # tensor torch.Size([64, 3, 28, 28]) labels &lt;- obj[[2]] # tensor torch.Size([64]), labels from 0 to 9 images = images$reshape(-1L, 28L*28L) # images &lt;- torch$as_tensor(images$reshape(-1L, 28L*28L), dtype=torch$double) outputs = model(images) .predicted = torch$max(outputs$data, 1L) predicted &lt;- .predicted[1L] total = total + labels$size(0L) correct = correct + sum((predicted$numpy() == labels$numpy())) } cat(sprintf(&#39;Accuracy of the model on the 10000 test images: %f %%&#39;, (100 * correct / total))) }) #&gt; Accuracy of the model on the 10000 test images: 83.360000 % 5.6 Save the model # Save the model checkpoint torch$save(model$state_dict(), &#39;model.ckpt&#39;) "],
["a-classic-classification-problem.html", "Chapter 6 A classic classification problem", " Chapter 6 A classic classification problem I will combine here R and Python code just to show how easy is integrating R and Python. First thing we have to do is loading the package rTorch. We do that in a chunk: library(rTorch) Then, we proceed to copy the standard Python code but in their own Python chunks. This is a very nice example that I found in the web. It explains the classic challenge of classification. When rTorch is loaded, a number of Python libraries are also loaded, which enable us the immediate use of numpy, torch and matplotlib. # Logistic Regression # https://m-alcu.github.io/blog/2018/02/10/logit-pytorch/ import numpy as np import torch import torch.nn.functional as F from torch.autograd import Variable import matplotlib.pyplot as plt The next thing we do is setting a seed to make the example repeatable, in my machine and yours. np.random.seed(2048) Then we generate some random samples. N = 100 D = 2 X = np.random.randn(N, D) * 2 ctr = int(N/2) # center the first N/2 points at (-2,-2) X[:ctr,:] = X[:ctr,:] - 2 * np.ones((ctr, D)) # center the last N/2 points at (2, 2) X[ctr:,:] = X[ctr:,:] + 2 * np.ones((ctr, D)) # labels: first N/2 are 0, last N/2 are 1 # mark the first half with 0 and the sceond half with 1 T = np.array([0] * ctr + [1] * ctr).reshape(100, 1) And plot the original data for reference. # plot the data. color the dots using T plt.scatter(X[:,0], X[:,1], c=T.reshape(N), s=100, alpha=0.5) plt.xlabel(&#39;X(1)&#39;) plt.ylabel(&#39;X(2)&#39;) What follows is the definition of the model using a neural network and train the model. We set up the model: class Model(torch.nn.Module): def __init__(self): super(Model, self).__init__() self.linear = torch.nn.Linear(2, 1) # 2 in and 1 out def forward(self, x): y_pred = torch.sigmoid(self.linear(x)) return y_pred # Our model model = Model() criterion = torch.nn.BCELoss(reduction=&#39;mean&#39;) optimizer = torch.optim.SGD(model.parameters(), lr=0.01) Train the model: x_data = Variable(torch.Tensor(X)) y_data = Variable(torch.Tensor(T)) # Training loop for epoch in range(1000): # Forward pass: Compute predicted y by passing x to the model y_pred = model(x_data) # Compute and print loss loss = criterion(y_pred, y_data) # print(epoch, loss.data[0]) # Zero gradients, perform a backward pass, and update the weights. optimizer.zero_grad() loss.backward() optimizer.step() w = list(model.parameters()) w0 = w[0].data.numpy() w1 = w[1].data.numpy() Finally, we plot the results, by tracing the line that separates two classes, 0 and 1, which are both colored in the plot. print(&quot;Final gradient descend:&quot;, w) # plot the data and separating line #&gt; Final gradient descend: [Parameter containing: #&gt; tensor([[1.0932, 1.1070]], requires_grad=True), Parameter containing: #&gt; tensor([0.2259], requires_grad=True)] plt.scatter(X[:,0], X[:,1], c=T.reshape(N), s=100, alpha=0.5) x_axis = np.linspace(-6, 6, 100) y_axis = -(w1[0] + x_axis * w0[0][0]) / w0[0][1] line_up, = plt.plot(x_axis, y_axis,&#39;r--&#39;, label=&#39;gradient descent&#39;) plt.legend(handles=[line_up]) plt.xlabel(&#39;X(1)&#39;) plt.ylabel(&#39;X(2)&#39;) plt.show() "],
["simple-linear-regression.html", "Chapter 7 Simple linear regression 7.1 Introduction 7.2 Generate the dataset 7.3 Convert arrays to tensors 7.4 Converting from numpy to tensor 7.5 Creating the network model 7.6 Optimizer and Loss 7.7 Training 7.8 Results", " Chapter 7 Simple linear regression 7.1 Introduction Source: https://www.guru99.com/pytorch-tutorial.html library(rTorch) nn &lt;- torch$nn Variable &lt;- torch$autograd$Variable torch$manual_seed(123) #&gt; &lt;torch._C.Generator&gt; 7.2 Generate the dataset Before you start the training process, you need to know our data. You make a random function to test our model. \\(Y = x3 sin(x)+ 3x+0.8 rand(100)\\) np$random$seed(123L) x = np$random$rand(100L) y = np$sin(x) * np$power(x, 3L) + 3L * x + np$random$rand(100L) * 0.8 plot(x, y) 7.3 Convert arrays to tensors Before you start the training process, you need to convert the numpy array to Variables that supported by Torch and autograd. 7.4 Converting from numpy to tensor Notice that before converting to a Torch tensor, we need first to convert the R numeric vector to a numpy array: # convert numpy array to tensor in shape of input size x &lt;- r_to_py(x) y &lt;- r_to_py(y) x = torch$from_numpy(x$reshape(-1L, 1L))$float() y = torch$from_numpy(y$reshape(-1L, 1L))$float() print(x, y) #&gt; tensor([[0.6965], #&gt; [0.2861], #&gt; [0.2269], #&gt; [0.5513], #&gt; [0.7195], #&gt; [0.4231], #&gt; [0.9808], #&gt; [0.6848], #&gt; [0.4809], #&gt; [0.3921], #&gt; [0.3432], #&gt; [0.7290], #&gt; [0.4386], #&gt; [0.0597], #&gt; [0.3980], #&gt; [0.7380], #&gt; [0.1825], #&gt; [0.1755], #&gt; [0.5316], #&gt; [0.5318], #&gt; [0.6344], #&gt; [0.8494], #&gt; [0.7245], #&gt; [0.6110], #&gt; [0.7224], #&gt; [0.3230], #&gt; [0.3618], #&gt; [0.2283], #&gt; [0.2937], #&gt; [0.6310], #&gt; [0.0921], #&gt; [0.4337], #&gt; [0.4309], #&gt; [0.4937], #&gt; [0.4258], #&gt; [0.3123], #&gt; [0.4264], #&gt; [0.8934], #&gt; [0.9442], #&gt; [0.5018], #&gt; [0.6240], #&gt; [0.1156], #&gt; [0.3173], #&gt; [0.4148], #&gt; [0.8663], #&gt; [0.2505], #&gt; [0.4830], #&gt; [0.9856], #&gt; [0.5195], #&gt; [0.6129], #&gt; [0.1206], #&gt; [0.8263], #&gt; [0.6031], #&gt; [0.5451], #&gt; [0.3428], #&gt; [0.3041], #&gt; [0.4170], #&gt; [0.6813], #&gt; [0.8755], #&gt; [0.5104], #&gt; [0.6693], #&gt; [0.5859], #&gt; [0.6249], #&gt; [0.6747], #&gt; [0.8423], #&gt; [0.0832], #&gt; [0.7637], #&gt; [0.2437], #&gt; [0.1942], #&gt; [0.5725], #&gt; [0.0957], #&gt; [0.8853], #&gt; [0.6272], #&gt; [0.7234], #&gt; [0.0161], #&gt; [0.5944], #&gt; [0.5568], #&gt; [0.1590], #&gt; [0.1531], #&gt; [0.6955], #&gt; [0.3188], #&gt; [0.6920], #&gt; [0.5544], #&gt; [0.3890], #&gt; [0.9251], #&gt; [0.8417], #&gt; [0.3574], #&gt; [0.0436], #&gt; [0.3048], #&gt; [0.3982], #&gt; [0.7050], #&gt; [0.9954], #&gt; [0.3559], #&gt; [0.7625], #&gt; [0.5932], #&gt; [0.6917], #&gt; [0.1511], #&gt; [0.3989], #&gt; [0.2409], #&gt; [0.3435]]) 7.5 Creating the network model Our network model is a simple Linear layer with an input and an output shape of one. And the network output should be like this Net( (hidden): Linear(in_features=1, out_features=1, bias=True) ) py_run_string(&quot;import torch&quot;) main = py_run_string( &quot; import torch.nn as nn class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.layer = torch.nn.Linear(1, 1) def forward(self, x): x = self.layer(x) return x &quot;) # build a Linear Rgression model net &lt;- main$Net() print(net) #&gt; Net( #&gt; (layer): Linear(in_features=1, out_features=1, bias=True) #&gt; ) 7.6 Optimizer and Loss Next, you should define the Optimizer and the Loss Function for our training process. # Define Optimizer and Loss Function optimizer &lt;- torch$optim$SGD(net$parameters(), lr=0.2) loss_func &lt;- torch$nn$MSELoss() print(optimizer) #&gt; SGD ( #&gt; Parameter Group 0 #&gt; dampening: 0 #&gt; lr: 0.2 #&gt; momentum: 0 #&gt; nesterov: False #&gt; weight_decay: 0 #&gt; ) print(loss_func) #&gt; MSELoss() 7.7 Training Now let’s start our training process. With an epoch of 250, you will iterate our data to find the best value for our hyperparameters. # x = x$type(torch$float) # make it a a FloatTensor # y = y$type(torch$float) # x &lt;- torch$as_tensor(x, dtype = torch$float) # y &lt;- torch$as_tensor(y, dtype = torch$float) inputs = Variable(x) outputs = Variable(y) # base plot plot(x$data$numpy(), y$data$numpy(), col = &quot;blue&quot;) for (i in 1:250) { prediction = net(inputs) loss = loss_func(prediction, outputs) optimizer$zero_grad() loss$backward() optimizer$step() if (i &gt; 1) break if (i %% 10 == 0) { # plot and show learning process # points(x$data$numpy(), y$data$numpy()) points(x$data$numpy(), prediction$data$numpy(), col=&quot;red&quot;) # cat(i, loss$data$numpy(), &quot;\\n&quot;) } } 7.8 Results As you can see below, you successfully performed regression with a neural network. Actually, on every iteration, the red line in the plot will update and change its position to fit the data. But in this picture, you only show you the final result. "],
["rainfall-linear-regression.html", "Chapter 8 Rainfall. Linear Regression 8.1 Training data 8.2 Convert arrays to tensors 8.3 Build the model 8.4 Generate predictions 8.5 Loss Function 8.6 Step by step process 8.7 All together: train for multiple epochs", " Chapter 8 Rainfall. Linear Regression library(rTorch) Select the device: CPU or GPU torch$manual_seed(0) #&gt; &lt;torch._C.Generator&gt; device = torch$device(&#39;cpu&#39;) 8.1 Training data The training data can be represented using 2 matrices (inputs and targets), each with one row per observation, and one column per variable. # Input (temp, rainfall, humidity) inputs = np$array(list(list(73, 67, 43), list(91, 88, 64), list(87, 134, 58), list(102, 43, 37), list(69, 96, 70)), dtype=&#39;float32&#39;) # Targets (apples, oranges) targets = np$array(list(list(56, 70), list(81, 101), list(119, 133), list(22, 37), list(103, 119)), dtype=&#39;float32&#39;) 8.2 Convert arrays to tensors Before we build a model, we need to convert inputs and targets to PyTorch tensors. # Convert inputs and targets to tensors inputs = torch$from_numpy(inputs) targets = torch$from_numpy(targets) print(inputs) #&gt; tensor([[ 73., 67., 43.], #&gt; [ 91., 88., 64.], #&gt; [ 87., 134., 58.], #&gt; [102., 43., 37.], #&gt; [ 69., 96., 70.]], dtype=torch.float64) print(targets) #&gt; tensor([[ 56., 70.], #&gt; [ 81., 101.], #&gt; [119., 133.], #&gt; [ 22., 37.], #&gt; [103., 119.]], dtype=torch.float64) The weights and biases can also be represented as matrices, initialized with random values. The first row of \\(w\\) and the first element of \\(b\\) are used to predict the first target variable, i.e. yield for apples, and, similarly, the second for oranges. # random numbers for weights and biases. Then convert to double() torch$set_default_dtype(torch$double) w = torch$randn(2L, 3L, requires_grad=TRUE) #$double() b = torch$randn(2L, requires_grad=TRUE) #$double() print(w) #&gt; tensor([[ 1.5410, -0.2934, -2.1788], #&gt; [ 0.5684, -1.0845, -1.3986]], requires_grad=True) print(b) #&gt; tensor([0.4033, 0.8380], requires_grad=True) 8.3 Build the model The model is simply a function that performs a matrix multiplication of the input \\(x\\) and the weights \\(w\\) (transposed), and adds the bias \\(b\\) (replicated for each observation). model &lt;- function(x) { wt &lt;- w$t() return(torch$add(torch$mm(x, wt), b)) } 8.4 Generate predictions The matrix obtained by passing the input data to the model is a set of predictions for the target variables. # Generate predictions preds = model(inputs) print(preds) #&gt; tensor([[ -0.4516, -90.4691], #&gt; [ -24.6303, -132.3828], #&gt; [ -31.2192, -176.1530], #&gt; [ 64.3523, -39.5645], #&gt; [ -73.9524, -161.9560]], grad_fn=&lt;AddBackward0&gt;) # Compare with targets print(targets) #&gt; tensor([[ 56., 70.], #&gt; [ 81., 101.], #&gt; [119., 133.], #&gt; [ 22., 37.], #&gt; [103., 119.]]) Because we’ve started with random weights and biases, the model does not a very good job of predicting the target variables. 8.5 Loss Function We can compare the predictions with the actual targets, using the following method: Calculate the difference between the two matrices (preds and targets). Square all elements of the difference matrix to remove negative values. Calculate the average of the elements in the resulting matrix. The result is a single number, known as the mean squared error (MSE). # MSE loss mse = function(t1, t2) { diff &lt;- torch$sub(t1, t2) mul &lt;- torch$sum(torch$mul(diff, diff)) return(torch$div(mul, diff$numel())) } print(mse) #&gt; function(t1, t2) { #&gt; diff &lt;- torch$sub(t1, t2) #&gt; mul &lt;- torch$sum(torch$mul(diff, diff)) #&gt; return(torch$div(mul, diff$numel())) #&gt; } 8.6 Step by step process 8.6.1 Compute the losses # Compute loss loss = mse(preds, targets) print(loss) #&gt; tensor(33060.8053, grad_fn=&lt;DivBackward0&gt;) # 46194 # 33060.8070 The resulting number is called the loss, because it indicates how bad the model is at predicting the target variables. Lower the loss, better the model. 8.6.2 Compute Gradients With PyTorch, we can automatically compute the gradient or derivative of the loss w.r.t. to the weights and biases, because they have requires_grad set to True. # Compute gradients loss$backward() The gradients are stored in the .grad property of the respective tensors. # Gradients for weights print(w) #&gt; tensor([[ 1.5410, -0.2934, -2.1788], #&gt; [ 0.5684, -1.0845, -1.3986]], requires_grad=True) print(w$grad) #&gt; tensor([[ -6938.4351, -9674.6757, -5744.0206], #&gt; [-17408.7861, -20595.9333, -12453.4702]]) # Gradients for bias print(b) #&gt; tensor([0.4033, 0.8380], requires_grad=True) print(b$grad) #&gt; tensor([ -89.3802, -212.1051]) A key insight from calculus is that the gradient indicates the rate of change of the loss, or the slope of the loss function w.r.t. the weights and biases. If a gradient element is positive: increasing the element’s value slightly will increase the loss. decreasing the element’s value slightly will decrease the loss. If a gradient element is negative, increasing the element’s value slightly will decrease the loss. decreasing the element’s value slightly will increase the loss. The increase or decrease is proportional to the value of the gradient. 8.6.3 Reset the gradients Finally, we’ll reset the gradients to zero before moving forward, because PyTorch accumulates gradients. # Reset the gradients w$grad$zero_() #&gt; tensor([[0., 0., 0.], #&gt; [0., 0., 0.]]) b$grad$zero_() #&gt; tensor([0., 0.]) print(w$grad) #&gt; tensor([[0., 0., 0.], #&gt; [0., 0., 0.]]) print(b$grad) #&gt; tensor([0., 0.]) 8.6.3.1 Adjust weights and biases using gradient descent We’ll reduce the loss and improve our model using the gradient descent algorithm, which has the following steps: Generate predictions Calculate the loss Compute gradients w.r.t the weights and biases Adjust the weights by subtracting a small quantity proportional to the gradient Reset the gradients to zero # Generate predictions preds = model(inputs) print(preds) #&gt; tensor([[ -0.4516, -90.4691], #&gt; [ -24.6303, -132.3828], #&gt; [ -31.2192, -176.1530], #&gt; [ 64.3523, -39.5645], #&gt; [ -73.9524, -161.9560]], grad_fn=&lt;AddBackward0&gt;) # Calculate the loss loss = mse(preds, targets) print(loss) #&gt; tensor(33060.8053, grad_fn=&lt;DivBackward0&gt;) # Compute gradients loss$backward() print(w$grad) #&gt; tensor([[ -6938.4351, -9674.6757, -5744.0206], #&gt; [-17408.7861, -20595.9333, -12453.4702]]) print(b$grad) #&gt; tensor([ -89.3802, -212.1051]) # Adjust weights and reset gradients with(torch$no_grad(), { print(w); print(b) # requires_grad attribute remains w$data &lt;- torch$sub(w$data, torch$mul(w$grad$data, torch$scalar_tensor(1e-5))) b$data &lt;- torch$sub(b$data, torch$mul(b$grad$data, torch$scalar_tensor(1e-5))) print(w$grad$data$zero_()) print(b$grad$data$zero_()) }) #&gt; tensor([[ 1.5410, -0.2934, -2.1788], #&gt; [ 0.5684, -1.0845, -1.3986]], requires_grad=True) #&gt; tensor([0.4033, 0.8380], requires_grad=True) #&gt; tensor([[0., 0., 0.], #&gt; [0., 0., 0.]]) #&gt; tensor([0., 0.]) print(w) #&gt; tensor([[ 1.6104, -0.1967, -2.1213], #&gt; [ 0.7425, -0.8786, -1.2741]], requires_grad=True) print(b) #&gt; tensor([0.4042, 0.8401], requires_grad=True) With the new weights and biases, the model should have a lower loss. # Calculate loss preds = model(inputs) loss = mse(preds, targets) print(loss) #&gt; tensor(23432.4894, grad_fn=&lt;DivBackward0&gt;) 8.7 All together: train for multiple epochs To reduce the loss further, we repeat the process of adjusting the weights and biases using the gradients multiple times. Each iteration is called an epoch. # Running all together # Adjust weights and reset gradients num_epochs &lt;- 100 for (i in 1:num_epochs) { preds = model(inputs) loss = mse(preds, targets) loss$backward() with(torch$no_grad(), { w$data &lt;- torch$sub(w$data, torch$mul(w$grad, torch$scalar_tensor(1e-5))) b$data &lt;- torch$sub(b$data, torch$mul(b$grad, torch$scalar_tensor(1e-5))) w$grad$zero_() b$grad$zero_() }) } # Calculate loss preds = model(inputs) loss = mse(preds, targets) print(loss) #&gt; tensor(1258.0216, grad_fn=&lt;DivBackward0&gt;) # predictions preds #&gt; tensor([[ 69.2462, 80.2082], #&gt; [ 73.7183, 97.2052], #&gt; [118.5780, 124.9272], #&gt; [ 89.2282, 92.7052], #&gt; [ 47.4648, 80.7782]], grad_fn=&lt;AddBackward0&gt;) # Targets targets #&gt; tensor([[ 56., 70.], #&gt; [ 81., 101.], #&gt; [119., 133.], #&gt; [ 22., 37.], #&gt; [103., 119.]]) "],
["a-two-layer-neural-network.html", "Chapter 9 A two-layer neural network 9.1 Load the libraries 9.2 Dataset 9.3 Run the model for 50 iterations 9.4 Run it at 100 iterations 9.5 Original PyTorch code", " Chapter 9 A two-layer neural network The following example was converted from PyTorch to rTorch to show differences and similarities of both approaches. The original source can be found here: Source: https://github.com/jcjohnson/pytorch-examples#pytorch-tensors 9.1 Load the libraries library(rTorch) library(ggplot2) device = torch$device(&#39;cpu&#39;) # device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU torch$manual_seed(0) #&gt; &lt;torch._C.Generator&gt; N is batch size; D_in is input dimension; H is hidden dimension; D_out is output dimension. 9.2 Dataset We will create a random dataset for a two layer neural network. N &lt;- 64L; D_in &lt;- 1000L; H &lt;- 100L; D_out &lt;- 10L # Create random Tensors to hold inputs and outputs x &lt;- torch$randn(N, D_in, device=device) y &lt;- torch$randn(N, D_out, device=device) dim(x) #&gt; [1] 64 1000 dim(y) #&gt; [1] 64 10 9.3 Run the model for 50 iterations Let’s say that for the sake of time we select to run only 50 iterations of the loop doing the training. # Randomly initialize weights w1 &lt;- torch$randn(D_in, H, device=device) # layer 1 w2 &lt;- torch$randn(H, D_out, device=device) # layer 2 learning_rate = 1e-6 # loop for (t in 1:50) { # Forward pass: compute predicted y, y_pred h &lt;- x$mm(w1) # matrix multiplication, x*w1 h_relu &lt;- h$clamp(min=0) # make elements greater than zero y_pred &lt;- h_relu$mm(w2) # matrix multiplication, h_relu*w2 # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor # of shape (); we can get its value as a Python number with loss.item(). loss &lt;- (torch$sub(y_pred, y))$pow(2)$sum() # sum((y_pred-y)^2) # cat(t, &quot;\\t&quot;) # cat(loss$item(), &quot;\\n&quot;) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred &lt;- torch$mul(torch$scalar_tensor(2.0), torch$sub(y_pred, y)) grad_w2 &lt;- h_relu$t()$mm(grad_y_pred) # compute gradient of w2 grad_h_relu &lt;- grad_y_pred$mm(w2$t()) grad_h &lt;- grad_h_relu$clone() mask &lt;- grad_h$lt(0) # filter values lower than zero torch$masked_select(grad_h, mask)$fill_(0.0) # make them equal to zero grad_w1 &lt;- x$t()$mm(grad_h) # compute gradient of w1 # Update weights using gradient descent w1 &lt;- torch$sub(w1, torch$mul(learning_rate, grad_w1)) w2 &lt;- torch$sub(w2, torch$mul(learning_rate, grad_w2)) } df_50 &lt;- data.frame(y = y$flatten()$numpy(), y_pred = y_pred$flatten()$numpy(), iter = 50) ggplot(df_50, aes(x = y, y = y_pred)) + geom_point() We see a lot of dispersion between the predicted values, \\(y_{pred}\\) and the real values, \\(y\\). We are far from our goal. 9.4 Run it at 100 iterations Now, we convert the script above to a function, so we could reuse it several times. We want to study the effect of the iteration on the performance of rthe algorithm. train &lt;- function(iterations) { # Randomly initialize weights w1 &lt;- torch$randn(D_in, H, device=device) # layer 1 w2 &lt;- torch$randn(H, D_out, device=device) # layer 2 learning_rate = 1e-6 # loop for (t in 1:iterations) { # Forward pass: compute predicted y h &lt;- x$mm(w1) h_relu &lt;- h$clamp(min=0) y_pred &lt;- h_relu$mm(w2) # Compute and print loss; loss is a scalar stored in a PyTorch Tensor # of shape (); we can get its value as a Python number with loss.item(). loss &lt;- (torch$sub(y_pred, y))$pow(2)$sum() # cat(t, &quot;\\t&quot;); cat(loss$item(), &quot;\\n&quot;) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred &lt;- torch$mul(torch$scalar_tensor(2.0), torch$sub(y_pred, y)) grad_w2 &lt;- h_relu$t()$mm(grad_y_pred) grad_h_relu &lt;- grad_y_pred$mm(w2$t()) grad_h &lt;- grad_h_relu$clone() mask &lt;- grad_h$lt(0) torch$masked_select(grad_h, mask)$fill_(0.0) grad_w1 &lt;- x$t()$mm(grad_h) # Update weights using gradient descent w1 &lt;- torch$sub(w1, torch$mul(learning_rate, grad_w1)) w2 &lt;- torch$sub(w2, torch$mul(learning_rate, grad_w2)) } data.frame(y = y$flatten()$numpy(), y_pred = y_pred$flatten()$numpy(), iter = iterations) } df_100 &lt;- train(iterations = 100) ggplot(df_100, aes(x = y_pred, y = y)) + geom_point() Still there are differences between the value and the prediction. Let’s try with more iterations, like 250: df_250 &lt;- train(iterations = 200) ggplot(df_250, aes(x = y_pred, y = y)) + geom_point() We see the formation of a line between the values and prediction, which means we are getting closer at finding the right algorithm, in this particular case, weights and bias. Let’s try one more time with 500 iterations: df_500 &lt;- train(iterations = 500) ggplot(df_500, aes(x = y_pred, y = y)) + geom_point() 9.5 Original PyTorch code This code will not execute. It is shown here for reference. The running code will be written in rTorch. # Code in file tensor/two_layer_net_tensor.py import torch device = torch.device(&#39;cpu&#39;) # device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU # N is batch size; D_in is input dimension; # H is hidden dimension; D_out is output dimension. N, D_in, H, D_out = 64, 1000, 100, 10 # Create random input and output data x = torch.randn(N, D_in, device=device) y = torch.randn(N, D_out, device=device) # Randomly initialize weights w1 = torch.randn(D_in, H, device=device) w2 = torch.randn(H, D_out, device=device) learning_rate = 1e-6 for t in range(500): # Forward pass: compute predicted y h = x.mm(w1) h_relu = h.clamp(min=0) y_pred = h_relu.mm(w2) # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor # of shape (); we can get its value as a Python number with loss.item(). loss = (y_pred - y).pow(2).sum() print(t, loss.item()) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.t().mm(grad_y_pred) grad_h_relu = grad_y_pred.mm(w2.t()) grad_h = grad_h_relu.clone() grad_h[h &lt; 0] = 0 grad_w1 = x.t().mm(grad_h) # Update weights using gradient descent w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2____ "],
["a-very-simple-neural-network.html", "Chapter 10 A very simple neural network 10.1 Introduction 10.2 Select device 10.3 Create the dataset 10.4 Define the model 10.5 Loss function 10.6 Iterate through batches", " Chapter 10 A very simple neural network 10.1 Introduction Source: https://github.com/jcjohnson/pytorch-examples#pytorch-nn In this example we use the torch nn package to implement our two-layer network: 10.2 Select device library(rTorch) device = torch$device(&#39;cpu&#39;) # device = torch.device(&#39;cuda&#39;) # Uncomment this to run on GPU N is batch size; D_in is input dimension; H is hidden dimension; D_out is output dimension. 10.3 Create the dataset torch$manual_seed(0) #&gt; &lt;torch._C.Generator&gt; N &lt;- 64L; D_in &lt;- 1000L; H &lt;- 100L; D_out &lt;- 10L # Create random Tensors to hold inputs and outputs x = torch$randn(N, D_in, device=device) y = torch$randn(N, D_out, device=device) 10.4 Define the model Use the nn package to define our model as a sequence of layers. nn.Sequential is a Module which contains other Modules, and applies them in sequence to produce its output. Each Linear Module computes output from input using a linear function, and holds internal Tensors for its weight and bias. After constructing the model we use the .to() method to move it to the desired device. model &lt;- torch$nn$Sequential( torch$nn$Linear(D_in, H), # first layer torch$nn$ReLU(), torch$nn$Linear(H, D_out))$to(device) # output layer print(model) #&gt; Sequential( #&gt; (0): Linear(in_features=1000, out_features=100, bias=True) #&gt; (1): ReLU() #&gt; (2): Linear(in_features=100, out_features=10, bias=True) #&gt; ) 10.5 Loss function The nn package also contains definitions of popular loss functions; in this case we will use Mean Squared Error (MSE) as our loss function. Setting reduction='sum' means that we are computing the sum of squared errors rather than the mean; this is for consistency with the examples above where we manually compute the loss, but in practice it is more common to use mean squared error as a loss by setting reduction='elementwise_mean'. loss_fn = torch$nn$MSELoss(reduction = &#39;sum&#39;) 10.6 Iterate through batches learning_rate = 1e-4 for (t in 1:500) { # Forward pass: compute predicted y by passing x to the model. Module objects # override the __call__ operator so you can call them like functions. When # doing so you pass a Tensor of input data to the Module and it produces # a Tensor of output data. y_pred = model(x) # Compute and print loss. We pass Tensors containing the predicted and true # values of y, and the loss function returns a Tensor containing the loss. loss = loss_fn(y_pred, y) cat(t, &quot;\\t&quot;) cat(loss$item(), &quot;\\n&quot;) # Zero the gradients before running the backward pass. model$zero_grad() # Backward pass: compute gradient of the loss with respect to all the learnable # parameters of the model. Internally, the parameters of each Module are stored # in Tensors with requires_grad=True, so this call will compute gradients for # all learnable parameters in the model. loss$backward() # Update the weights using gradient descent. Each parameter is a Tensor, so # we can access its data and gradients like we did before. with(torch$no_grad(), { for (param in iterate(model$parameters())) { # in Python this code is much simpler. In R we have to do some conversions # param$data &lt;- torch$sub(param$data, # torch$mul(param$grad$float(), # torch$scalar_tensor(learning_rate))) param$data &lt;- param$data - param$grad * learning_rate } }) } #&gt; 1 628 #&gt; 2 585 #&gt; 3 547 #&gt; 4 513 #&gt; 5 482 #&gt; 6 455 #&gt; 7 430 #&gt; 8 406 #&gt; 9 385 #&gt; 10 364 #&gt; 11 345 #&gt; 12 328 #&gt; 13 311 #&gt; 14 295 #&gt; 15 280 #&gt; 16 265 #&gt; 17 252 #&gt; 18 239 #&gt; 19 226 #&gt; 20 214 #&gt; 21 203 #&gt; 22 192 #&gt; 23 181 #&gt; 24 172 #&gt; 25 162 #&gt; 26 153 #&gt; 27 145 #&gt; 28 137 #&gt; 29 129 #&gt; 30 122 #&gt; 31 115 #&gt; 32 109 #&gt; 33 103 #&gt; 34 96.9 #&gt; 35 91.5 #&gt; 36 86.3 #&gt; 37 81.5 #&gt; 38 76.9 #&gt; 39 72.6 #&gt; 40 68.5 #&gt; 41 64.6 #&gt; 42 61 #&gt; 43 57.6 #&gt; 44 54.3 #&gt; 45 51.3 #&gt; 46 48.5 #&gt; 47 45.8 #&gt; 48 43.2 #&gt; 49 40.9 #&gt; 50 38.6 #&gt; 51 36.5 #&gt; 52 34.5 #&gt; 53 32.7 #&gt; 54 30.9 #&gt; 55 29.3 #&gt; 56 27.8 #&gt; 57 26.3 #&gt; 58 24.9 #&gt; 59 23.7 #&gt; 60 22.4 #&gt; 61 21.3 #&gt; 62 20.2 #&gt; 63 19.2 #&gt; 64 18.2 #&gt; 65 17.3 #&gt; 66 16.5 #&gt; 67 15.7 #&gt; 68 14.9 #&gt; 69 14.2 #&gt; 70 13.5 #&gt; 71 12.9 #&gt; 72 12.3 #&gt; 73 11.7 #&gt; 74 11.1 #&gt; 75 10.6 #&gt; 76 10.1 #&gt; 77 9.67 #&gt; 78 9.24 #&gt; 79 8.82 #&gt; 80 8.42 #&gt; 81 8.05 #&gt; 82 7.69 #&gt; 83 7.35 #&gt; 84 7.03 #&gt; 85 6.72 #&gt; 86 6.43 #&gt; 87 6.16 #&gt; 88 5.9 #&gt; 89 5.65 #&gt; 90 5.41 #&gt; 91 5.18 #&gt; 92 4.97 #&gt; 93 4.76 #&gt; 94 4.57 #&gt; 95 4.38 #&gt; 96 4.2 #&gt; 97 4.03 #&gt; 98 3.87 #&gt; 99 3.72 #&gt; 100 3.57 #&gt; 101 3.43 #&gt; 102 3.29 #&gt; 103 3.17 #&gt; 104 3.04 #&gt; 105 2.92 #&gt; 106 2.81 #&gt; 107 2.7 #&gt; 108 2.6 #&gt; 109 2.5 #&gt; 110 2.41 #&gt; 111 2.31 #&gt; 112 2.23 #&gt; 113 2.14 #&gt; 114 2.06 #&gt; 115 1.99 #&gt; 116 1.91 #&gt; 117 1.84 #&gt; 118 1.77 #&gt; 119 1.71 #&gt; 120 1.65 #&gt; 121 1.59 #&gt; 122 1.53 #&gt; 123 1.47 #&gt; 124 1.42 #&gt; 125 1.37 #&gt; 126 1.32 #&gt; 127 1.27 #&gt; 128 1.23 #&gt; 129 1.18 #&gt; 130 1.14 #&gt; 131 1.1 #&gt; 132 1.06 #&gt; 133 1.02 #&gt; 134 0.989 #&gt; 135 0.954 #&gt; 136 0.921 #&gt; 137 0.889 #&gt; 138 0.858 #&gt; 139 0.828 #&gt; 140 0.799 #&gt; 141 0.772 #&gt; 142 0.745 #&gt; 143 0.719 #&gt; 144 0.695 #&gt; 145 0.671 #&gt; 146 0.648 #&gt; 147 0.626 #&gt; 148 0.605 #&gt; 149 0.584 #&gt; 150 0.564 #&gt; 151 0.545 #&gt; 152 0.527 #&gt; 153 0.509 #&gt; 154 0.492 #&gt; 155 0.476 #&gt; 156 0.46 #&gt; 157 0.444 #&gt; 158 0.43 #&gt; 159 0.415 #&gt; 160 0.402 #&gt; 161 0.388 #&gt; 162 0.375 #&gt; 163 0.363 #&gt; 164 0.351 #&gt; 165 0.339 #&gt; 166 0.328 #&gt; 167 0.318 #&gt; 168 0.307 #&gt; 169 0.297 #&gt; 170 0.287 #&gt; 171 0.278 #&gt; 172 0.269 #&gt; 173 0.26 #&gt; 174 0.252 #&gt; 175 0.244 #&gt; 176 0.236 #&gt; 177 0.228 #&gt; 178 0.221 #&gt; 179 0.214 #&gt; 180 0.207 #&gt; 181 0.2 #&gt; 182 0.194 #&gt; 183 0.187 #&gt; 184 0.181 #&gt; 185 0.176 #&gt; 186 0.17 #&gt; 187 0.165 #&gt; 188 0.159 #&gt; 189 0.154 #&gt; 190 0.149 #&gt; 191 0.145 #&gt; 192 0.14 #&gt; 193 0.136 #&gt; 194 0.131 #&gt; 195 0.127 #&gt; 196 0.123 #&gt; 197 0.119 #&gt; 198 0.115 #&gt; 199 0.112 #&gt; 200 0.108 #&gt; 201 0.105 #&gt; 202 0.102 #&gt; 203 0.0983 #&gt; 204 0.0952 #&gt; 205 0.0923 #&gt; 206 0.0894 #&gt; 207 0.0866 #&gt; 208 0.0838 #&gt; 209 0.0812 #&gt; 210 0.0787 #&gt; 211 0.0762 #&gt; 212 0.0739 #&gt; 213 0.0716 #&gt; 214 0.0693 #&gt; 215 0.0672 #&gt; 216 0.0651 #&gt; 217 0.0631 #&gt; 218 0.0611 #&gt; 219 0.0592 #&gt; 220 0.0574 #&gt; 221 0.0556 #&gt; 222 0.0539 #&gt; 223 0.0522 #&gt; 224 0.0506 #&gt; 225 0.0491 #&gt; 226 0.0476 #&gt; 227 0.0461 #&gt; 228 0.0447 #&gt; 229 0.0433 #&gt; 230 0.042 #&gt; 231 0.0407 #&gt; 232 0.0394 #&gt; 233 0.0382 #&gt; 234 0.0371 #&gt; 235 0.0359 #&gt; 236 0.0348 #&gt; 237 0.0338 #&gt; 238 0.0327 #&gt; 239 0.0317 #&gt; 240 0.0308 #&gt; 241 0.0298 #&gt; 242 0.0289 #&gt; 243 0.028 #&gt; 244 0.0272 #&gt; 245 0.0263 #&gt; 246 0.0255 #&gt; 247 0.0248 #&gt; 248 0.024 #&gt; 249 0.0233 #&gt; 250 0.0226 #&gt; 251 0.0219 #&gt; 252 0.0212 #&gt; 253 0.0206 #&gt; 254 0.02 #&gt; 255 0.0194 #&gt; 256 0.0188 #&gt; 257 0.0182 #&gt; 258 0.0177 #&gt; 259 0.0171 #&gt; 260 0.0166 #&gt; 261 0.0161 #&gt; 262 0.0156 #&gt; 263 0.0151 #&gt; 264 0.0147 #&gt; 265 0.0142 #&gt; 266 0.0138 #&gt; 267 0.0134 #&gt; 268 0.013 #&gt; 269 0.0126 #&gt; 270 0.0122 #&gt; 271 0.0119 #&gt; 272 0.0115 #&gt; 273 0.0112 #&gt; 274 0.0108 #&gt; 275 0.0105 #&gt; 276 0.0102 #&gt; 277 0.00988 #&gt; 278 0.00959 #&gt; 279 0.0093 #&gt; 280 0.00902 #&gt; 281 0.00875 #&gt; 282 0.00849 #&gt; 283 0.00824 #&gt; 284 0.00799 #&gt; 285 0.00775 #&gt; 286 0.00752 #&gt; 287 0.0073 #&gt; 288 0.00708 #&gt; 289 0.00687 #&gt; 290 0.00666 #&gt; 291 0.00647 #&gt; 292 0.00627 #&gt; 293 0.00609 #&gt; 294 0.00591 #&gt; 295 0.00573 #&gt; 296 0.00556 #&gt; 297 0.0054 #&gt; 298 0.00524 #&gt; 299 0.00508 #&gt; 300 0.00493 #&gt; 301 0.00478 #&gt; 302 0.00464 #&gt; 303 0.0045 #&gt; 304 0.00437 #&gt; 305 0.00424 #&gt; 306 0.00412 #&gt; 307 0.00399 #&gt; 308 0.00388 #&gt; 309 0.00376 #&gt; 310 0.00365 #&gt; 311 0.00354 #&gt; 312 0.00344 #&gt; 313 0.00334 #&gt; 314 0.00324 #&gt; 315 0.00314 #&gt; 316 0.00305 #&gt; 317 0.00296 #&gt; 318 0.00287 #&gt; 319 0.00279 #&gt; 320 0.00271 #&gt; 321 0.00263 #&gt; 322 0.00255 #&gt; 323 0.00248 #&gt; 324 0.0024 #&gt; 325 0.00233 #&gt; 326 0.00226 #&gt; 327 0.0022 #&gt; 328 0.00213 #&gt; 329 0.00207 #&gt; 330 0.00201 #&gt; 331 0.00195 #&gt; 332 0.00189 #&gt; 333 0.00184 #&gt; 334 0.00178 #&gt; 335 0.00173 #&gt; 336 0.00168 #&gt; 337 0.00163 #&gt; 338 0.00158 #&gt; 339 0.00154 #&gt; 340 0.00149 #&gt; 341 0.00145 #&gt; 342 0.00141 #&gt; 343 0.00137 #&gt; 344 0.00133 #&gt; 345 0.00129 #&gt; 346 0.00125 #&gt; 347 0.00121 #&gt; 348 0.00118 #&gt; 349 0.00114 #&gt; 350 0.00111 #&gt; 351 0.00108 #&gt; 352 0.00105 #&gt; 353 0.00102 #&gt; 354 0.000987 #&gt; 355 0.000958 #&gt; 356 0.000931 #&gt; 357 0.000904 #&gt; 358 0.000877 #&gt; 359 0.000852 #&gt; 360 0.000827 #&gt; 361 0.000803 #&gt; 362 0.00078 #&gt; 363 0.000757 #&gt; 364 0.000735 #&gt; 365 0.000714 #&gt; 366 0.000693 #&gt; 367 0.000673 #&gt; 368 0.000654 #&gt; 369 0.000635 #&gt; 370 0.000617 #&gt; 371 0.000599 #&gt; 372 0.000581 #&gt; 373 0.000565 #&gt; 374 0.000548 #&gt; 375 0.000532 #&gt; 376 0.000517 #&gt; 377 0.000502 #&gt; 378 0.000488 #&gt; 379 0.000474 #&gt; 380 0.00046 #&gt; 381 0.000447 #&gt; 382 0.000434 #&gt; 383 0.000421 #&gt; 384 0.000409 #&gt; 385 0.000397 #&gt; 386 0.000386 #&gt; 387 0.000375 #&gt; 388 0.000364 #&gt; 389 0.000354 #&gt; 390 0.000343 #&gt; 391 0.000334 #&gt; 392 0.000324 #&gt; 393 0.000315 #&gt; 394 0.000306 #&gt; 395 0.000297 #&gt; 396 0.000288 #&gt; 397 0.00028 #&gt; 398 0.000272 #&gt; 399 0.000264 #&gt; 400 0.000257 #&gt; 401 0.000249 #&gt; 402 0.000242 #&gt; 403 0.000235 #&gt; 404 0.000228 #&gt; 405 0.000222 #&gt; 406 0.000216 #&gt; 407 0.000209 #&gt; 408 0.000203 #&gt; 409 0.000198 #&gt; 410 0.000192 #&gt; 411 0.000186 #&gt; 412 0.000181 #&gt; 413 0.000176 #&gt; 414 0.000171 #&gt; 415 0.000166 #&gt; 416 0.000161 #&gt; 417 0.000157 #&gt; 418 0.000152 #&gt; 419 0.000148 #&gt; 420 0.000144 #&gt; 421 0.00014 #&gt; 422 0.000136 #&gt; 423 0.000132 #&gt; 424 0.000128 #&gt; 425 0.000124 #&gt; 426 0.000121 #&gt; 427 0.000117 #&gt; 428 0.000114 #&gt; 429 0.000111 #&gt; 430 0.000108 #&gt; 431 0.000105 #&gt; 432 0.000102 #&gt; 433 9.87e-05 #&gt; 434 9.59e-05 #&gt; 435 9.32e-05 #&gt; 436 9.06e-05 #&gt; 437 8.8e-05 #&gt; 438 8.55e-05 #&gt; 439 8.31e-05 #&gt; 440 8.07e-05 #&gt; 441 7.84e-05 #&gt; 442 7.62e-05 #&gt; 443 7.41e-05 #&gt; 444 7.2e-05 #&gt; 445 6.99e-05 #&gt; 446 6.79e-05 #&gt; 447 6.6e-05 #&gt; 448 6.41e-05 #&gt; 449 6.23e-05 #&gt; 450 6.06e-05 #&gt; 451 5.89e-05 #&gt; 452 5.72e-05 #&gt; 453 5.56e-05 #&gt; 454 5.4e-05 #&gt; 455 5.25e-05 #&gt; 456 5.1e-05 #&gt; 457 4.96e-05 #&gt; 458 4.82e-05 #&gt; 459 4.68e-05 #&gt; 460 4.55e-05 #&gt; 461 4.42e-05 #&gt; 462 4.3e-05 #&gt; 463 4.18e-05 #&gt; 464 4.06e-05 #&gt; 465 3.94e-05 #&gt; 466 3.83e-05 #&gt; 467 3.72e-05 #&gt; 468 3.62e-05 #&gt; 469 3.52e-05 #&gt; 470 3.42e-05 #&gt; 471 3.32e-05 #&gt; 472 3.23e-05 #&gt; 473 3.14e-05 #&gt; 474 3.05e-05 #&gt; 475 2.96e-05 #&gt; 476 2.88e-05 #&gt; 477 2.8e-05 #&gt; 478 2.72e-05 #&gt; 479 2.65e-05 #&gt; 480 2.57e-05 #&gt; 481 2.5e-05 #&gt; 482 2.43e-05 #&gt; 483 2.36e-05 #&gt; 484 2.29e-05 #&gt; 485 2.23e-05 #&gt; 486 2.17e-05 #&gt; 487 2.11e-05 #&gt; 488 2.05e-05 #&gt; 489 1.99e-05 #&gt; 490 1.94e-05 #&gt; 491 1.88e-05 #&gt; 492 1.83e-05 #&gt; 493 1.78e-05 #&gt; 494 1.73e-05 #&gt; 495 1.68e-05 #&gt; 496 1.63e-05 #&gt; 497 1.59e-05 #&gt; 498 1.54e-05 #&gt; 499 1.5e-05 #&gt; 500 1.46e-05 These two expression are equivalent, with the first being the long version natural way of doing it in PyTorch. The second is using the generics in R for subtraction, multiplication and scalar conversion. param$data &lt;- torch$sub(param$data, torch$mul(param$grad$float(), torch$scalar_tensor(learning_rate))) } param$data &lt;- param$data - param$grad * learning_rate "],
["neural-networks-2.html", "Chapter 11 Neural Networks 2 11.1 nn2 1 11.2 nn2 2", " Chapter 11 Neural Networks 2 Source: https://github.com/jcjohnson/pytorch-examples#pytorch-nn 11.1 nn2 1 11.2 nn2 2 "],
["working-with-data-frame.html", "Chapter 12 Working with data.frame 12.1 Load PyTorch libraries 12.2 Load dataset 12.3 Summary statistics for tensors", " Chapter 12 Working with data.frame 12.1 Load PyTorch libraries library(rTorch) torch &lt;- import(&quot;torch&quot;) torchvision &lt;- import(&quot;torchvision&quot;) nn &lt;- import(&quot;torch.nn&quot;) transforms &lt;- import(&quot;torchvision.transforms&quot;) dsets &lt;- import(&quot;torchvision.datasets&quot;) builtins &lt;- import_builtins() np &lt;- import(&quot;numpy&quot;) 12.2 Load dataset # folders where the images are located train_data_path = &#39;~/mnist_png_full/training/&#39; test_data_path = &#39;~/mnist_png_full/testing/&#39; # read the datasets without normalization train_dataset = torchvision$datasets$ImageFolder(root = train_data_path, transform = torchvision$transforms$ToTensor() ) print(train_dataset) #&gt; Dataset ImageFolder #&gt; Number of datapoints: 60000 #&gt; Root location: /home/msfz751/mnist_png_full/training/ 12.3 Summary statistics for tensors 12.3.1 using data.frame library(tictoc) tic() fun_list &lt;- list( size = c(&quot;size&quot;), numel = c(&quot;numel&quot;), sum = c(&quot;sum&quot;, &quot;item&quot;), mean = c(&quot;mean&quot;, &quot;item&quot;), std = c(&quot;std&quot;, &quot;item&quot;), med = c(&quot;median&quot;, &quot;item&quot;), max = c(&quot;max&quot;, &quot;item&quot;), min = c(&quot;min&quot;, &quot;item&quot;) ) idx &lt;- seq(0L, 599L) # how many samples fun_get_tensor &lt;- function(x) py_get_item(train_dataset, x)[[1]] stat_fun &lt;- function(x, str_fun) { fun_var &lt;- paste0(&quot;fun_get_tensor(x)&quot;, &quot;$&quot;, str_fun, &quot;()&quot;) sapply(idx, function(x) ifelse(is.numeric(eval(parse(text = fun_var))), # size return chracater eval(parse(text = fun_var)), # all else are numeric as.character(eval(parse(text = fun_var))))) } df &lt;- data.frame(ridx = idx+1, # index number for the sample do.call(data.frame, lapply( sapply(fun_list, function(x) paste(x, collapse = &quot;()$&quot;)), function(y) stat_fun(1, y) ) ) ) Summary statistics: head(df, 20) #&gt; ridx size numel sum mean std med max min #&gt; 1 1 torch.Size([3, 28, 28]) 2352 366 0.156 0.329 0 1.000 0 #&gt; 2 2 torch.Size([3, 28, 28]) 2352 284 0.121 0.297 0 1.000 0 #&gt; 3 3 torch.Size([3, 28, 28]) 2352 645 0.274 0.420 0 1.000 0 #&gt; 4 4 torch.Size([3, 28, 28]) 2352 410 0.174 0.355 0 1.000 0 #&gt; 5 5 torch.Size([3, 28, 28]) 2352 321 0.137 0.312 0 1.000 0 #&gt; 6 6 torch.Size([3, 28, 28]) 2352 654 0.278 0.421 0 1.000 0 #&gt; 7 7 torch.Size([3, 28, 28]) 2352 496 0.211 0.374 0 1.000 0 #&gt; 8 8 torch.Size([3, 28, 28]) 2352 549 0.233 0.399 0 1.000 0 #&gt; 9 9 torch.Size([3, 28, 28]) 2352 449 0.191 0.365 0 1.000 0 #&gt; 10 10 torch.Size([3, 28, 28]) 2352 465 0.198 0.367 0 1.000 0 #&gt; 11 11 torch.Size([3, 28, 28]) 2352 383 0.163 0.338 0 1.000 0 #&gt; 12 12 torch.Size([3, 28, 28]) 2352 499 0.212 0.378 0 1.000 0 #&gt; 13 13 torch.Size([3, 28, 28]) 2352 313 0.133 0.309 0 0.996 0 #&gt; 14 14 torch.Size([3, 28, 28]) 2352 360 0.153 0.325 0 1.000 0 #&gt; 15 15 torch.Size([3, 28, 28]) 2352 435 0.185 0.358 0 0.996 0 #&gt; 16 16 torch.Size([3, 28, 28]) 2352 429 0.182 0.358 0 1.000 0 #&gt; 17 17 torch.Size([3, 28, 28]) 2352 596 0.254 0.408 0 1.000 0 #&gt; 18 18 torch.Size([3, 28, 28]) 2352 527 0.224 0.392 0 1.000 0 #&gt; 19 19 torch.Size([3, 28, 28]) 2352 303 0.129 0.301 0 1.000 0 #&gt; 20 20 torch.Size([3, 28, 28]) 2352 458 0.195 0.364 0 1.000 0 Elapsed time per size of sample: toc() #&gt; 12.202 sec elapsed # 60 1.663s # 600 13.5s # 6000 54.321 sec; # 60000 553.489 sec elapsed "],
["working-with-data-table.html", "Chapter 13 Working with data.table 13.1 Load PyTorch libraries 13.2 Load dataset 13.3 Read the datasets without normalization 13.4 Using data.table", " Chapter 13 Working with data.table 13.1 Load PyTorch libraries library(rTorch) torch &lt;- import(&quot;torch&quot;) torchvision &lt;- import(&quot;torchvision&quot;) nn &lt;- import(&quot;torch.nn&quot;) transforms &lt;- import(&quot;torchvision.transforms&quot;) dsets &lt;- import(&quot;torchvision.datasets&quot;) builtins &lt;- import_builtins() np &lt;- import(&quot;numpy&quot;) 13.2 Load dataset ## Dataset iteration batch settings # folders where the images are located train_data_path = &#39;~/mnist_png_full/training/&#39; test_data_path = &#39;~/mnist_png_full/testing/&#39; 13.3 Read the datasets without normalization train_dataset = torchvision$datasets$ImageFolder(root = train_data_path, transform = torchvision$transforms$ToTensor() ) print(train_dataset) #&gt; Dataset ImageFolder #&gt; Number of datapoints: 60000 #&gt; Root location: /home/msfz751/mnist_png_full/training/ 13.4 Using data.table library(data.table) library(tictoc) tic() fun_list &lt;- list( numel = c(&quot;numel&quot;), sum = c(&quot;sum&quot;, &quot;item&quot;), mean = c(&quot;mean&quot;, &quot;item&quot;), std = c(&quot;std&quot;, &quot;item&quot;), med = c(&quot;median&quot;, &quot;item&quot;), max = c(&quot;max&quot;, &quot;item&quot;), min = c(&quot;min&quot;, &quot;item&quot;) ) idx &lt;- seq(0L, 5999L) fun_get_tensor &lt;- function(x) py_get_item(train_dataset, x)[[1]] stat_fun &lt;- function(x, str_fun) { fun_var &lt;- paste0(&quot;fun_get_tensor(x)&quot;, &quot;$&quot;, str_fun, &quot;()&quot;) sapply(idx, function(x) ifelse(is.numeric(eval(parse(text = fun_var))), # size return chracater eval(parse(text = fun_var)), # all else are numeric as.character(eval(parse(text = fun_var))))) } dt &lt;- data.table(ridx = idx+1, do.call(data.table, lapply( sapply(fun_list, function(x) paste(x, collapse = &quot;()$&quot;)), function(y) stat_fun(1, y) ) ) ) Summary statistics: head(dt) #&gt; ridx numel sum mean std med max min #&gt; 1: 1 2352 366 0.156 0.329 0 1 0 #&gt; 2: 2 2352 284 0.121 0.297 0 1 0 #&gt; 3: 3 2352 645 0.274 0.420 0 1 0 #&gt; 4: 4 2352 410 0.174 0.355 0 1 0 #&gt; 5: 5 2352 321 0.137 0.312 0 1 0 #&gt; 6: 6 2352 654 0.278 0.421 0 1 0 Elapsed time per size of sample: toc() #&gt; 113.922 sec elapsed # 60 1.266 sec elapsed # 600 11.798 sec elapsed; # 6000 119.256 sec elapsed; # 60000 1117.619 sec elapsed "],
["appendixA.html", "A Statistical Background A.1 Basic statistical terms", " A Statistical Background A.1 Basic statistical terms A.1.1 Five-number summary The five-number summary consists of five values: minimum, first quantile, second quantile, third quantile, and maximum. The quantiles are calculated as: first quantile (\\(Q_1\\)): the median of the first half of the sorted data third quantile (\\(Q_3\\)): the median of the second half of the sorted data First quantile: 25th percentile. Second quantile: 50th percentile. Third quantile: 75th percentile. The interquartile range or IQR is defined as \\(Q_3 - Q_1\\) and is a measure of how spread out the middle 50% of values is. "],
["appendixB.html", "B Activation Functions B.1 The Sigmoid function B.2 The ReLU function B.3 The tanh function B.4 The Softmax Activation function B.5 Coding your own activation functions in Python B.6 Softmax in Python", " B Activation Functions library(rTorch) library(ggplot2) B.1 The Sigmoid function Using the PyTorch sigmoid() function: x &lt;- torch$range(-5., 5., 0.1) y &lt;- torch$sigmoid(x) df &lt;- data.frame(x = x$numpy(), sx = y$numpy()) df #&gt; x sx #&gt; 1 -5.0 0.00669 #&gt; 2 -4.9 0.00739 #&gt; 3 -4.8 0.00816 #&gt; 4 -4.7 0.00901 #&gt; 5 -4.6 0.00995 #&gt; 6 -4.5 0.01099 #&gt; 7 -4.4 0.01213 #&gt; 8 -4.3 0.01339 #&gt; 9 -4.2 0.01477 #&gt; 10 -4.1 0.01630 #&gt; 11 -4.0 0.01799 #&gt; 12 -3.9 0.01984 #&gt; 13 -3.8 0.02188 #&gt; 14 -3.7 0.02413 #&gt; 15 -3.6 0.02660 #&gt; 16 -3.5 0.02931 #&gt; 17 -3.4 0.03230 #&gt; 18 -3.3 0.03557 #&gt; 19 -3.2 0.03917 #&gt; 20 -3.1 0.04311 #&gt; 21 -3.0 0.04743 #&gt; 22 -2.9 0.05215 #&gt; 23 -2.8 0.05732 #&gt; 24 -2.7 0.06297 #&gt; 25 -2.6 0.06914 #&gt; 26 -2.5 0.07586 #&gt; 27 -2.4 0.08317 #&gt; 28 -2.3 0.09112 #&gt; 29 -2.2 0.09975 #&gt; 30 -2.1 0.10910 #&gt; 31 -2.0 0.11920 #&gt; 32 -1.9 0.13011 #&gt; 33 -1.8 0.14185 #&gt; 34 -1.7 0.15447 #&gt; 35 -1.6 0.16798 #&gt; 36 -1.5 0.18243 #&gt; 37 -1.4 0.19782 #&gt; 38 -1.3 0.21417 #&gt; 39 -1.2 0.23148 #&gt; 40 -1.1 0.24974 #&gt; 41 -1.0 0.26894 #&gt; 42 -0.9 0.28905 #&gt; 43 -0.8 0.31003 #&gt; 44 -0.7 0.33181 #&gt; 45 -0.6 0.35434 #&gt; 46 -0.5 0.37754 #&gt; 47 -0.4 0.40131 #&gt; 48 -0.3 0.42556 #&gt; 49 -0.2 0.45017 #&gt; 50 -0.1 0.47502 #&gt; 51 0.0 0.50000 #&gt; 52 0.1 0.52498 #&gt; 53 0.2 0.54983 #&gt; 54 0.3 0.57444 #&gt; 55 0.4 0.59869 #&gt; 56 0.5 0.62246 #&gt; 57 0.6 0.64566 #&gt; 58 0.7 0.66819 #&gt; 59 0.8 0.68997 #&gt; 60 0.9 0.71095 #&gt; 61 1.0 0.73106 #&gt; 62 1.1 0.75026 #&gt; 63 1.2 0.76852 #&gt; 64 1.3 0.78584 #&gt; 65 1.4 0.80218 #&gt; 66 1.5 0.81757 #&gt; 67 1.6 0.83202 #&gt; 68 1.7 0.84553 #&gt; 69 1.8 0.85815 #&gt; 70 1.9 0.86989 #&gt; 71 2.0 0.88080 #&gt; 72 2.1 0.89090 #&gt; 73 2.2 0.90025 #&gt; 74 2.3 0.90888 #&gt; 75 2.4 0.91683 #&gt; 76 2.5 0.92414 #&gt; 77 2.6 0.93086 #&gt; 78 2.7 0.93703 #&gt; 79 2.8 0.94268 #&gt; 80 2.9 0.94785 #&gt; 81 3.0 0.95257 #&gt; 82 3.1 0.95689 #&gt; 83 3.2 0.96083 #&gt; 84 3.3 0.96443 #&gt; 85 3.4 0.96770 #&gt; 86 3.5 0.97069 #&gt; 87 3.6 0.97340 #&gt; 88 3.7 0.97587 #&gt; 89 3.8 0.97812 #&gt; 90 3.9 0.98016 #&gt; 91 4.0 0.98201 #&gt; 92 4.1 0.98370 #&gt; 93 4.2 0.98523 #&gt; 94 4.3 0.98661 #&gt; 95 4.4 0.98787 #&gt; 96 4.5 0.98901 #&gt; 97 4.6 0.99005 #&gt; 98 4.7 0.99099 #&gt; 99 4.8 0.99184 #&gt; 100 4.9 0.99261 #&gt; 101 5.0 0.99331 ggplot(df, aes(x = x, y = sx)) + geom_point() + ggtitle(&quot;Sigmoid&quot;) Plot the sigmoid function using an R custom-made function: sigmoid = function(x) { 1 / (1 + exp(-x)) } x &lt;- seq(-5, 5, 0.01) plot(x, sigmoid(x), col = &#39;blue&#39;, cex = 0.5, main = &quot;Sigmoid&quot;) B.2 The ReLU function Using the PyTorch relu() function: x &lt;- torch$range(-5., 5., 0.1) y &lt;- torch$relu(x) df &lt;- data.frame(x = x$numpy(), sx = y$numpy()) df ggplot(df, aes(x = x, y = sx)) + geom_point() + ggtitle(&quot;ReLU&quot;) B.3 The tanh function Using the PyTorch tanh() function: x &lt;- torch$range(-5., 5., 0.1) y &lt;- torch$tanh(x) df &lt;- data.frame(x = x$numpy(), sx = y$numpy()) df ggplot(df, aes(x = x, y = sx)) + geom_point() + ggtitle(&quot;tanh&quot;) B.4 The Softmax Activation function Using the PyTorch tanh() function: x &lt;- torch$range(-5.0, 5.0, 0.1) y &lt;- torch$softmax(x, dim=0L) df &lt;- data.frame(x = x$numpy(), sx = y$numpy()) ggplot(df, aes(x = x, y = sx)) + geom_point() + ggtitle(&quot;Softmax&quot;) B.5 Coding your own activation functions in Python library(rTorch) import numpy as np import matplotlib.pyplot as plt np.random.seed(42) Linear activation def Linear(x, derivative=False): &quot;&quot;&quot; Computes the Linear activation function for array x inputs: x: array derivative: if True, return the derivative else the forward pass &quot;&quot;&quot; if derivative: # Return derivative of the function at x return np.ones_like(x) else: # Return forward pass of the function at x return x Sigmoid activation def Sigmoid(x, derivative=False): &quot;&quot;&quot; Computes the Sigmoid activation function for array x inputs: x: array derivative: if True, return the derivative else the forward pass &quot;&quot;&quot; f = 1/(1+np.exp(-x)) if derivative: # Return derivative of the function at x return f*(1-f) else: # Return forward pass of the function at x return f Hyperbolic Tangent activation def Tanh(x, derivative=False): &quot;&quot;&quot; Computes the Hyperbolic Tangent activation function for array x inputs: x: array derivative: if True, return the derivative else the forward pass &quot;&quot;&quot; f = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) if derivative: # Return derivative of the function at x return 1-f**2 else: # Return the forward pass of the function at x return f Rectifier linear unit (ReLU) def ReLU(x, derivative=False): &quot;&quot;&quot; Computes the Rectifier Linear Unit activation function for array x inputs: x: array derivative: if True, return the derivative else the forward pass &quot;&quot;&quot; if derivative: # Return derivative of the function at x return (x&gt;0).astype(int) else: # Return forward pass of the function at x return np.maximum(x, 0) Visualization Plotting using matplotlib: x = np.linspace(-6, 6, 100) units = { &quot;Linear&quot;: lambda x: Linear(x), &quot;Sigmoid&quot;: lambda x: Sigmoid(x), &quot;ReLU&quot;: lambda x: ReLU(x), &quot;tanh&quot;: lambda x: Tanh(x) } plt.figure(figsize=(5, 5)) [plt.plot(x, unit(x), label=unit_name, lw=2) for unit_name, unit in units.items()] plt.legend(loc=2, fontsize=16) plt.title(&#39;Activation functions&#39;, fontsize=20) plt.ylim([-2, 5]) plt.xlim([-6, 6]) plt.show() B.6 Softmax in Python # Source: https://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/ import numpy as np import matplotlib.pyplot as plt def softmax(inputs): &quot;&quot;&quot; Calculate the softmax for the give inputs (array) :param inputs: :return: &quot;&quot;&quot; return np.exp(inputs) / float(sum(np.exp(inputs))) def line_graph(x, y, x_title, y_title): &quot;&quot;&quot; Draw line graph with x and y values :param x: :param y: :param x_title: :param y_title: :return: &quot;&quot;&quot; plt.plot(x, y) plt.xlabel(x_title) plt.ylabel(y_title) plt.show() graph_x = np.linspace(-6, 6, 100) graph_y = softmax(graph_x) print(&quot;Graph X readings: {}&quot;.format(graph_x)) print(&quot;Graph Y readings: {}&quot;.format(graph_y)) line_graph(graph_x, graph_y, &quot;Inputs&quot;, &quot;Softmax Scores&quot;) "],
["references.html", "References", " References "]
]
