```{r include=FALSE, cache=FALSE}
set.seed(1234)
options(digits = 3)

knitr::opts_chunk$set(
    comment = "#>",
    collapse = TRUE,
    cache = FALSE,
    out.width = "70%",
    fig.align = 'center',
    fig.width = 6,
    fig.asp = 0.618,  # 1 / phi
    fig.show = "hold"
)

# options(dplyr.print_min = 6, dplyr.print_max = 6)
```
# rTorch vs PyTorch: What's different
This chapter will explain the main differences between `PyTorch` and `rTorch`.
Most of the things work directly in `PyTorch` but we need to be aware of some minor differences when working with rTorch.

Here is a review of existing methods.

```{r}
library(rTorch)
```

## Calling objects from PyTorch
We use the dollar sign or `$` to call a class, function or method from the rTorch modules. In this case, from `torch` module:

```{r}
torch$tensor
```


## Call a module from PyTorch

```{r import-pytorch-modules}
# these are the equivalents of import module
nn          <- torch$nn
transforms  <- torchvision$transforms
dsets       <- torchvision$datasets
```

Then we can proceed to extract classes, methods and functions from the `nn`, `transforms`, and `dsets` objects.

## Show the attributes (methods) of a class or PyTorch object
Sometimes we are interested in knowing the internal components of a class. In that case, we use the reticulate function `py_list_attributes()`.

```{r mnist-digits-train-dataset}
local_folder <- './datasets/mnist_digits'
train_dataset = torchvision$datasets$MNIST(root = local_folder, 
                                           train = TRUE, 
                                           transform = transforms$ToTensor(),
                                           download = TRUE)

train_dataset
```

Show the attributes of `train_dataset`:
```{r list-attributes}
reticulate::py_list_attributes(train_dataset)
```

Knowing the internal methods of a class could be useful when we want to refer to a specific property of such class. For example, from the list above, we know that the object `train_dataset` has an attribute `__len__`. We can call it like this:

```{r len-dataset}
train_dataset$`__len__`()
```


## Enumeration

```{r create-r-array}

x_train = array(c(3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 
                  9.779, 6.182, 7.59, 2.167, 7.042,
                  10.791, 5.313, 7.997, 3.1), dim = c(15,1))

x_train <- r_to_py(x_train)
x_train <- torch$from_numpy(x_train)         # convert to tensor
x_train <- x_train$type(torch$FloatTensor)   # make it a a FloatTensor

x_train
```


```{r number-of-elements}
x_train$nelement()    # number of elements in the tensor
```


## How to iterate

### Using `enumerate` and `iterate`

```{r import-builtins}
py = import_builtins()

enum_x_train = py$enumerate(x_train)
enum_x_train

py$len(x_train)
```


```{r iterate-train}
xit = iterate(enum_x_train, simplify = TRUE)
xit
```


### Using a `for-loop` to iterate

```{r loop-iterator}
# reset the iterator
enum_x_train = py$enumerate(x_train)

for (i in 1:py$len(x_train)) {
    obj <- iter_next(enum_x_train)    # next item
    cat(obj[[1]], "\t")     # 1st part or index
    print(obj[[2]])         # 2nd part or tensor
}
```

> We will find very frequently this kind of iterators when we read a dataset using `torchvision`. There are different ways to iterate through these objects.

## Zero gradient
The zero gradient was one of the most difficult to implement in R if we don't pay attention to the content of the objects carrying the weights and biases. This happens when the algorithm written in __PyTorch__ is not immediately translatable to __rTorch__. This can be appreciated in this example.

> We are using the same seed in the PyTorch and rTorch versions, so, we could compare the results.

### Version in Python

```{python, python-rainfall-code}
import numpy as np
import torch

torch.manual_seed(0)  # reproducible

# Input (temp, rainfall, humidity)
inputs = np.array([[73, 67, 43],
                   [91, 88, 64],
                   [87, 134, 58],
                   [102, 43, 37],
                   [69, 96, 70]], dtype='float32')

# Targets (apples, oranges)
targets = np.array([[56, 70],
                    [81, 101],
                    [119, 133],
                    [22, 37],
                    [103, 119]], dtype='float32')
                    

# Convert inputs and targets to tensors
inputs = torch.from_numpy(inputs)
targets = torch.from_numpy(targets)

# random weights and biases
w = torch.randn(2, 3, requires_grad=True)
b = torch.randn(2, requires_grad=True)

# function for the model
def model(x):
  wt = w.t()
  mm = x @ w.t()
  return x @ w.t() + b       # @ represents matrix multiplication in PyTorch

# MSE loss function
def mse(t1, t2):
  diff = t1 - t2
  return torch.sum(diff * diff) / diff.numel()

# Running all together
# Train for 100 epochs
for i in range(100):
  preds = model(inputs)
  loss = mse(preds, targets)
  loss.backward()
  with torch.no_grad():
    w -= w.grad * 0.00001
    b -= b.grad * 0.00001
    w_gz = w.grad.zero_()
    b_gz = b.grad.zero_()
    
# Calculate loss
preds = model(inputs)
loss = mse(preds, targets)
print("Loss: ", loss)    

# predictions
print("\nPredictions:")
preds

# Targets
print("\nTargets:")
targets
```

### Version in R

```{r rtorch-rainfall-code}
library(rTorch)

torch$manual_seed(0)

device = torch$device('cpu')
# Input (temp, rainfall, humidity)
inputs = np$array(list(list(73, 67, 43),
                   list(91, 88, 64),
                   list(87, 134, 58),
                   list(102, 43, 37),
                   list(69, 96, 70)), dtype='float32')

# Targets (apples, oranges)
targets = np$array(list(list(56, 70), 
                    list(81, 101),
                    list(119, 133),
                    list(22, 37), 
                    list(103, 119)), dtype='float32')


# Convert inputs and targets to tensors
inputs = torch$from_numpy(inputs)
targets = torch$from_numpy(targets)

# random numbers for weights and biases. Then convert to double()
torch$set_default_dtype(torch$float64)

w = torch$randn(2L, 3L, requires_grad=TRUE) #$double()
b = torch$randn(2L, requires_grad=TRUE) #$double()

model <- function(x) {
  wt <- w$t()
  return(torch$add(torch$mm(x, wt), b))
}

# MSE loss
mse = function(t1, t2) {
  diff <- torch$sub(t1, t2)
  mul <- torch$sum(torch$mul(diff, diff))
  return(torch$div(mul, diff$numel()))
}

# Running all together
# Adjust weights and reset gradients
for (i in 1:100) {
  preds = model(inputs)
  loss = mse(preds, targets)
  loss$backward()
  with(torch$no_grad(), {
    w$data <- torch$sub(w$data, torch$mul(w$grad, torch$scalar_tensor(1e-5)))
    b$data <- torch$sub(b$data, torch$mul(b$grad, torch$scalar_tensor(1e-5)))
    
    w$grad$zero_()
    b$grad$zero_()
  })
}

# Calculate loss
preds = model(inputs)
loss = mse(preds, targets)
cat("Loss: "); print(loss)

# predictions
cat("\nPredictions:\n")
preds

# Targets
cat("\nTargets:\n")
targets
```

Notice that while in Python, the tensor operation, gradient of the weights times the **Learning Rate**, is:

$$w = -w + \nabla w \; \alpha$$

is a very straight forwward and clean code:

```{python py-calc-gradient, eval=FALSE}
w -= w.grad * 1e-5
```

In R shows a little bit more convoluted:

```{r r-calc-gradient1, eval=FALSE}
w$data <- torch$sub(w$data, torch$mul(w$grad, torch$scalar_tensor(1e-5)))
```

Which is simpliefied when we use the generic methods from rTorch as:


```{r r-calc-gradient2, eval=FALSE}
w$data <- w$data - w$grad * 1e-5
```


These two expression are equivalent, with the first being the long version natural way of doing it in __PyTorch__. The second is using the generics in R for subtraction, multiplication and scalar conversion.

```
param$data <- torch$sub(param$data,
                        torch$mul(param$grad$float(),
                          torch$scalar_tensor(learning_rate)))
}
```

```
param$data <- param$data - param$grad * learning_rate
```


## Transform a tensor
Explain how transform a tensor back and forth to `numpy`.
Why is this important?


## Build a model class
PyTorch classes cannot not directly be instantiated from `R`. We need an intermediate step to create a class. For this, we use `reticulate` functions that will read the class implementation in `Python` code.


### Example 1

```{r class-linreg, eval=FALSE}
py_run_string("import torch")
main = py_run_string(
"
import torch.nn as nn

class Net(nn.Module):
   def __init__(self):
       super(Net, self).__init__()
       self.layer = torch.nn.Linear(1, 1)

   def forward(self, x):
       x = self.layer(x)      
       return x
")


# build a Linear Rgression model
net <- main$Net()
```



### Example 2: Logistic Regression

```{r class-logreg, eval=FALSE}
main <- py_run_string(
"
import torch.nn as nn

class LogisticRegressionModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LogisticRegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        out = self.linear(x)
        return out
")

# build a Logistic Rgression model
LogisticRegressionModel <- main$LogisticRegressionModel
```



## Convert a tensor to `numpy` object
This is a frequent operation. I have found that this is necessary when 

* a numpy function is not implemented in PyTorch
* We need to convert a tensor to R
* Perform a Boolean operation that is not directly available in PyTorch.


## Convert a `numpy` object to an `R` object
This is mainly required for these reasons:

1. Create a data structure in R
2. Plot using r-base or ggplot2
3. Perform an analysis on parts of a tensor
4. Use R statistical functions that are not available in PyTorch.



